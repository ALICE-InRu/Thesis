\HeaderQuote{There's a large mustard-mine near here. And the moral of that is 
-- The more there is of mine, the less there is of yours.}{The Duchess} 


\chapter{Evolutionary Learning of CDRs}\label{ch:esmodels} 

\FirstSentence{G}{enetic algorithms (GA) are one of the} most widely used 
approaches in \JSP\ literature \citep{Pinedo08}. 
GA is search heuristic that is inspired by the process of natural selection, 
and is a subclass of evolutionary algorithms (EA), which generate solutions to 
optimisation problems using techniques based on natural evolution, such as 
inheritance, mutation, selection, and crossover.

When applying GAs to \JSP, an extensive number of schedules need to be 
evaluated, and even for low dimensional JSP, it can quickly become 
computationally infeasible. GAs can be used directly on schedules 
\citep{Cheng96,Cheng99,Tsai07,Qing-dao-er-ji12,Ak12}. 
However, then there are many concerns that need to be dealt with. 
To begin with there are nine encoding schemes for representing the schedules 
\citep{Cheng96}, in addition, special care must be taken when applying 
cross-over and mutation operators in order for the schedules (now in the role 
of `chromosomes') to still remain feasible. 
Moreover, in case of JSP, GAs are not adapt for fine-tuning around optima. 
Luckily a subsequent local search can mediate the optimisation 
\citep{Cheng99,Meeran12}.

The most predominant approach in hyper-heuristics, a framework of creating 
\emph{new} heuristics from a set of  predefined heuristics, is genetic 
programming \citep{Burke10}. Dispatching rules based genetic algorithms (DRGA) 
\citep{Vazquez-Rodriguez09,Dhingra10,Nguyen13} are a special case of genetic 
programming \citep{Koza05}, where GAs are applied indirectly to JSP via 
\dr s, i.e., where a solution is no longer a \emph{proper} schedule 
but a \emph{representation} of a schedule via applying certain DRs 
consecutively. 

As previously discussed in \cref{ch:introduction}, there are two main 
viewpoints on how to approach scheduling problems
\begin{enumerate*}
    \item tailored algorithms where schedules are built for one problem 
    instance at a time
    \item general algorithms where schedules are built for all problem 
    instances at once
\end{enumerate*}
For tailored algorithms a simple construction heuristic is applied. The 
schedule's features are collected at each dispatch iteration from which a 
learning model will inspect the feature set to discriminate which operations 
are preferred to others via ordinal regression. The focus is essentially on 
creating a meaningful preference set composed of features and their ranks as 
the learning algorithm is only run once to find suitable operators for the 
value function. This is the approach taken in \cref{InRu11a}. Expanding on 
that  work, this \lcnamecref{ch:esmodels} will explore a general algorithms 
construction viewpoint where there is no feature set collected beforehand since 
the learning model is optimised directly via evolutionary search. The framework 
was first reported in \cref{InRu14}, and later used to improve the preference 
models in \cref{InRu15a}. 

Evolutionary search requires numerous costly value function evaluations. 
In fact it involves an indirect method of evaluation whether one learning model 
is preferable to another, w.r.t. which one yields a better expected mean. For 
that reason, it can be mediated with the use of preference learning, as 
discussed in \cref{InRu11b}, albeit for traditional test functions suite (in 
particular Rosenbrock's function and Sphere model).

\section{Experimental setting}
A prevalent approach to solving \JSP\ is to combine several relatively simple 
dispatching rules such that they may benefit each other for a given problem 
space. Generally, this is done on an ad-hoc basis, requiring expert knowledge 
from heuristics designer, or extensive exploration of suitable combinations of 
heuristics. The approach in \cref{InRu14}, was to automate that selection 
similar to DRGA, by translating dispatching rules into measurable features and 
optimising what their contribution should be via evolutionary search, i.e., 
optimise the weights $\vec{w}$ in \cref{eq:CDR:feat} directly using
covariance matrix adaptation evolution strategy (CMA-ES) by \cite{Hansen01}, 
which has been proven to be a very efficient numerical optimisation technique. 
The framework is straight forward and easy to implement and shows promising 
results. 


Moreover, \cref{sec:es:measure} shows that the choice of objective function  
for evolutionary search is worth investigating. Since the optimisation is based 
on minimising the expected mean of the fitness function over a large set of 
problem instances, which can vary within. Then normalising the objective 
function can stabilise the optimisation process away from local minima. 

Preliminary experiments were first reported in \cref{InRu14,InRu15a} using 
\begin{enumerate*}
    \item standard set-up of parameters of the CMA-ES optimisation
    \item runtime was limited to 288 hours on a cluster for each 
    \Problem[6\times5]{\text{train}} problem set given in \cref{tbl:data}, 
    where in every case the optimisation reached its maximum walltime
\end{enumerate*}
\Cref{InRu14} had one model for all $K$ time steps, whereas \cref{InRu15a} used 
a different stepwise model at each dispatch iteration. 

Various data distributions from \cref{ch:genprobleminstances} are investigated. 
Due to computational cost, only $6\times5$ instances were initially 
considered. However, since then the scheduling subroutines have been made more 
time-efficient, making \Problem[10\times10]{\text{train}} applicable in a 
reasonable amount of time. 
CMA-ES models will be mostly trained on the lower dimension, $6\times5$, 
and only the general random $10\times10$ \JSP\ case is explored. 
Finally, to check robustness, models are validated on benchmark tests sets from 
the OR-Library (cf. \cref{sec:data:orlib}).


\section{Performance measures}\label{sec:es:measure}
Generally, evolutionary search only needs to minimise the expected fitness 
value. However, the  approach in \cref{InRu11a} was to use the known optimum to 
correctly label which operations' features were optimal when compared to other 
possible operations. Therefore, it would be of interest to inspect if there is 
any performance edge gained by incorporating optimal labelling in evolutionary 
search. Therefore, two objective functions will be considered, namely, 
\begin{subequations}\label{eq:cma:objfun}
\begin{equation}\label{eq:cma:makespan}
    \minCmax := \min \Exp{C_{\max}} 
\end{equation}
for optimising w.r.t. $C_{\max}$ directly, and on the other hand
\begin{equation}\label{eq:cma:rho}
    \minRho := \min \Exp{\rho} 
\end{equation} 
which optimises w.r.t. the resulting $C_{\max}$ scaled to its true optimum, 
i.e., \cref{eq:rho}.
\end{subequations}

\section{Experimental results}\label{sec:cma:expr}

Main statistics of the experimental run are given in \cref{tbl:cma:run} and 
depicted in \cref{fig:cma:fit} for both approaches. In addition, evolving 
decision variables, here weights $\vec{w}$ for \cref{eq:CDR:feat}, are 
depicted in \cref{fig:cma:wei}. 

The evolution of fitness per generation from the CMA-ES optimisation of 
\cref{eq:cma:objfun} is depicted in \cref{fig:cma:fit}. 
Note, most problem spaces reached their allotted maximum function evaluations 
(set as 50,000) without converging. 
In fact several problem spaces, e.g., \frnd{6}{5}, needed restarting during the 
optimisation process.
Notice \jrndJ{6}{5} especially, then $\minCmax$ needs more than twice the 
amount of function evaluations than using $\minRho$ as an objective.

Furthermore, the  evolution of the decision variables $\vec{w}$ are depicted in 
\cref{fig:cma:wei}. As one can see, the relative contribution for each weight 
clearly differs between problem spaces. Note, that in the case of 
\jrnd{6}{5} (cf. \cref{fig:cma:wei:1,fig:cma:fit}), CMA-ES restarts around 
generation 1,600 and quickly converges back to its previous fitness. 
However, lateral relation of weights has completely changed, implying that 
there are many optimal combinations of weights to be used. This can be expected 
due to the fact some features in \cref{tbl:features} are a linear combination 
of others, e.g. $\phi_3=\phi_1+\phi_2$.
Moreover, from \cref{fig:cma:wei:K} we see that the evolution of weights w.r.t. 
each step $k$ is quite erratic and \cref{eq:cma:makespan} is somewhat 
dissimilar to its \cref{eq:cma:rho} counterpart, yet their resulting  $\rho$ 
values are not significantly different. Most likely, this can be explained by 
feature equivalence. 

{\setlength{\tabcolsep}{3pt} \input{tables/esmodels-runs} }

\begin{figure}[t]\centering
    \includegraphics[width=\columnwidth]{{CMAES.generation.log.fitness}.pdf}
    \caption{Log fitness for optimising w.r.t. \cref{eq:cma:objfun}, per 
        generation of the CMA-ES optimisation.}\label{fig:cma:fit}
\end{figure}

\begin{figure} 
    \subcaptionbox{evolution of weights for \jrnd{n}{m} time independent models
        \label{fig:cma:wei:1}}{
        \includegraphics[width=\textwidth]{{CMAES.weights.timeindependent}.pdf}}\\
    \subcaptionbox{stepwise evolultion of final weights for \jrnd{6}{5} time 
        dependent models \label{fig:cma:wei:K}}{
        \includegraphics[width=\textwidth]{{CMAES.weights.timedependent}.pdf}}
    \caption{Evolution of weights for features (given in \cref{tbl:features}).
        Note, weights are normalised such that $\norm{\vec{w}}=1$.}
    \label{fig:cma:wei}
\end{figure}

In order to compare the two objective functions in \cref{eq:cma:objfun}, the 
best weights reported were used for \cref{eq:CDR:feat} on the corresponding 
training and test set. 
Its box-plot of \fullnamerho, is depicted in \cref{fig:cma:boxplot:set} and 
\cref{tbl:cma:boxplot:set} presents its main statistics: mean, median, standard 
deviation, minimum and maximum values. 

In most cases (except for \frndn{6}{5}, \fjc{6}{5}) there was a 
significant difference w.r.t. lower mean $\rho$, when using a separate model 
for each time step, as is to be expected as the optimal \dr s used in the 
beginning of the scheduling process may not necessarily be the same as in the 
middle, or end of the schedule. 
Alas, stepwise models aren't appropriate when inspecting robustness towards 
different problem spaces.\footnote{Note, time dependant 
    models are inapplicable for OR-Library, since their size $n\times m$ 
    doesn't match any size in the benchmark set. However, this is irrelevant 
    for time independent models.}
Hence, a single model for all iterations is preferred. 

Regarding the choice of objective function in \cref{eq:cma:objfun}, then there 
is no statistical difference between adopting either objective function with 
respect to training and test set, save for time independent \fmxc{6}{5}. 
Now, when applying the time independent models to the OR-Library benchmark 
data sets, depicted in \cref{fig:cma:boxplot:orlib}, then we see a clear 
performance boost when using \cref{eq:cma:rho} in 
\begin{enumerate*}
    \item \fjc{6}{5}, \fmc{6}{5} and \fmxc{6}{5} for \JSP\ 
    \item \fjc{6}{5} and \fmxc{6}{5} for \FSP\ 
\end{enumerate*}
Therefore, minimisation of expectation of $\rho$, is preferred over simply 
using the unscaled resulting makespan. 
Also it's noted that in \cref{InRu14}, then \fjc{6}{5} optimised w.r.t. 
\cref{eq:cma:makespan} gave a considerably worse results, since the 
optimisation got trapped in a local minima.

Furthermore, notice how \fmxc{6}{5} obtains a significantly better mean $\rho$ 
(from 52.8\% down to 24.46\%) for the \JSP\ (cf. \cref{fig:cma:boxplot:orlib}) 
then it did for it's corresponding problem space, which was the only setting 
where \cref{eq:cma:rho} was significantly different than \cref{eq:cma:makespan} 
(worsening by $\approx1\%$).

\begin{figure}[p]
    \subcaptionbox{Models applied to corresponding training and test set
        \label{fig:cma:boxplot:set}}{
        \includegraphics[width=\textwidth]{{boxplot.CMAES}.pdf}}
    \subcaptionbox{Models applied to ORLIB test set 
        \label{fig:cma:boxplot:orlib}}{
        \includegraphics[width=\textwidth]{{boxplot.CMAES.ORLIB}.pdf}}
	\caption{Box-plot for \namerho, when implementing the final weights 
	obtained from CMA-ES optimisation, using objective functions from 
	\cref{eq:cma:objfun}.}\label{fig:cma:boxplot}
\end{figure}

{\setlength{\tabcolsep}{3pt} \input{tables/stats.boxplot.CMAES.tex}}



\section{Discussion and conclusions}\label{sec:disc}
Data distributions considered in this study incorporated more problem spaces 
than in its initial reports in  \cref{InRu14}. Furthermore, both time dependent 
and independent models were optimised with CMA-ES. The former generally 
obtained lower mean $\rho$. However, the latter was often equally good, with 
the added benefit of being applicable for higher (or lower) dimensionality, 
which was then tested using the benchmark set from the OR-Library.

The study showed that the choice of objective function for evolutionary search 
is worth investigating. 
There was no statistical difference from minimising the fitness function 
directly and its normalisation w.r.t. true optimum (cf. \cref{eq:cma:objfun}), 
save for time independent \fmxc{6}{5}, when applying the models to their 
corresponding training and test set. However, preliminary experiments in 
\cref{InRu14} and application on unseen data sets from the OR-Library, showed 
great improvement when using \cref{eq:cma:rho} over \cref{eq:cma:makespan}.
Implying, even though CMA-ES doesn't rely on optimal solutions, there are some 
problem spaces where it can be of great benefit. This is due to the fact that 
the problem instances can vary greatly within the same problem space 
\cref{InRu12}. Thus normalising the objective function would help the 
evolutionary search to deviate from giving too much weight for problematic 
problem instances.

The main drawback of using evolutionary search for finding optimal weights for 
\cref{eq:CDR:feat} is how computationally expensive it is to evaluate the mean 
expected fitness. Even for a low problem dimension such as 6-job 5-machine 
\JSP, each optimisation run reached their maximum allotted function evaluations 
without converging. 
Now, $6\times5$ \JSP\ requires 30 sequential operations where at each time step 
there are up to $6$ jobs to choose from, in fact its complexity is  
$\bigOh{n!^m}$ \citep{Giffler60} making it computationally infeasible to apply 
this framework for higher dimensions as is. Especially, considering that it's 
preferred to run these experiments several times -- e.g. in \cref{InRu14} 
\fjc{6}{5} got stuck in local minima for $\minCmax$, which could have been 
avoided by restarting the optimisation.
However, evolutionary search only requires the rank of the candidates and 
therefore it is appropriate to retain a sufficiently accurate surrogate for the 
value function during evolution in order to reduce the number of costly true 
value function evaluations, such as the approach in \cref{InRu11b}. This could 
reduce the computational cost of the evolutionary search considerably, making 
it feasible to conduct the experiments from \cref{sec:cma:expr} for problems of 
higher dimensions, e.g., with these adjustments it is possible to train on 
$10\times10$ with greater ease, or even considering even higher dimensions, 
e.g. $14\times14$.