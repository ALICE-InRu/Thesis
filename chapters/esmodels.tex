\HeaderQuote{There's a large mustard-mine near here. And the moral of that is 
-- The more there is of mine, the less there is of yours.}{The Duchess} 


\chapter{Evolutionary Learning of CDRs}\label{ch:esmodels} 

\FirstSentence{G}{enetic algorithms (GA) are one of the} most widely used 
approaches in \JSP\ literature \citep{Pinedo08}. 
However, in that case an extensive number of schedules need to be evaluated, 
and even for low dimensional JSP, it can quickly become computationally 
infeasible. GAs can be used directly on schedules 
\cite{Cheng96,Cheng99,Tsai07,Qing-dao-er-ji12,Ak12}. 
However, then there are many concerns that need to be dealt with. 
To begin with there are nine encoding schemes for representing the schedules 
\cite{Cheng96}, in addition, special care must be taken when applying 
cross-over and mutation operators in order for the schedules, now in the role 
of `chromosomes,' to still remain feasible. 
Moreover, in case of JSP, GAs are not adapt for fine-tuning around optima. 
Luckily a subsequent local search can mediate the optimisation 
\citep{Cheng99,Meeran12}.

The most predominant approach in hyper-heuristics, a framework of creating 
\emph{new} heuristics from a set of  predefined heuristics, is genetic 
programming \cite{Burke10}. Dispatching rules based genetic algorithms (DRGA) 
\cite{Vazquez-Rodriguez09,Dhingra10,Nguyen13} are a special case of genetic 
programming \cite{Koza05}, where GAs are applied indirectly to JSP via 
\dr s, i.e., where a solution is no longer a \emph{proper} schedule 
but a \emph{representation} of a schedule via applying certain DRs 
consecutively. 

A prevalent approach to solving \JSP\ is to combine several relatively simple 
dispatching rules such that they may benefit each other for a given problem 
space. Generally, this is done on an ad-hoc basis, requiring expert knowledge 
from heuristics designer, or extensive exploration of suitable combinations of 
heuristics. The approach in this \namecref{ch:esmodels}, is to automate that 
selection, by translating dispatching rules into measurable features and 
optimising what their contribution should be via evolutionary search. The 
framework is straight forward and easy to implement and shows promising 
results. Various data distributions from \cref{ch:genprobleminstances} are 
investigated, however only trained on the lower dimension, $6\times5$, yet, 
validated on higher dimension, $10\times10$. 

Moreover, \cref{sec:es:measure} shows that the choice of objective function  
for evolutionary search is worth investigating. Since the optimisation is based 
on minimising the expected mean of the fitness function over a large set of 
problem instances, which can vary within. Then normalising the objective 
function can stabilise the optimisation process away from local minima. 

As previously discussed in \cref{ch:introduction}, there are two main 
viewpoints on how to approach scheduling problems
\begin{enumerate*}
	\item local level by building schedules for one problem instance at a time
    \item global level by building schedules for all problem instances at once
\end{enumerate*}
For local level construction a simple construction heuristic is applied. The 
schedule's features are collected at each dispatch iteration from which a 
learning model will inspect the feature set to discriminate which operations 
are preferred to others via ordinal regression. The focus is essentially on 
creating a meaningful preference set composed of features and their ranks as 
the learning algorithm is only run once to find suitable operators for the 
value function. This is the approach taken in \cref{InRu11a}. Expanding on 
that  work, this study will explore a global level construction viewpoint where 
there is no feature set collected beforehand since the learning model is 
optimised directly via evolutionary search. This involves numerous costly value 
function evaluations. In fact it involves an indirect method of evaluation 
whether one learning model is preferable to another, w.r.t. which one yields a 
better expected mean. 

\section{Experimental set-up}\label{sec:expr}
\todoWrite{Throughout a Kolmogorov-Smirnov test with $\alpha=0.05$ is applied 
to determine statistical significance between methodologies.}

Inspired by DRGA, the approach taken in this study is to optimise the weights 
$\vec{w}$ in \cref{eq:CDR:feat} directly via evolutionary search such as 
covariance matrix adaptation evolution strategy (CMA-ES) \cite{Hansen01}. This 
has been proven to be a very efficient numerical optimisation technique. 

Using standard set-up of parameters of the CMA-ES optimisation, the runtime was 
limited to 288 hours on a cluster for each training set given in 
\cref{tbl:data} and in every case the optimisation reached its maximum walltime.

\section{Performance measures}\label{sec:expr:measure}
Generally, evolutionary search only needs to minimise the expected fitness 
value. However, the  approach in \cref{InRu11a} was to use the known optimum to 
correctly label which operations' features were optimal when compared to other 
possible operations. Therefore, it would be of interest to inspect if there is 
any performance edge gained by incorporating optimal labelling in evolutionary 
search. Therefore, two objective functions will be considered, namely, 
\begin{equation}\label{eq:cma:makespan}
	ES_{C_{\max}} := \min \Exp{C_{\max}} 
\end{equation}
for optimising w.r.t. $C_{\max}$ directly, and on the other hand
\begin{equation}\label{eq:cma:rho}
	ES_{\rho} := \min \Exp{\rho} 
\end{equation} 
which optimises w.r.t. the resulting $C_{\max}$ scaled to its true optimum, 
i.e., \cref{eq:rho}.

Main statistics of the experimental run are given in \cref{cma:funeval} and 
depicted in \cref{fig:cma:fit} for both approaches. In addition, evolving 
decision variables, here weights $\vec{w}$ for \cref{eq:CDR:feat}, are 
depicted in \cref{fig:cma:wei}. 

In order to compare the two objective functions, the best weights reported were 
used for \cref{eq:CDR:feat} on the corresponding training data. Its 
box-plot of percentage relative deviation from optimality, defined by 
\cref{eq:rho}, is depicted in \cref{fig:cma:trainboxpl} and 
\cref{tbl:results:train} present its main statistics; mean, median, standard 
deviation, minimum and maximum values.

In the case of \frndn{6}{5}, \cref{eq:cma:rho} gave a considerably worse 
results, since the optimisation got trapped in a local minima, as the erratic 
evolution of the weights in \cref{fig:cma:wei:cmax} suggest.
For other problem spaces, \cref{eq:cma:makespan} gave slightly better results 
than \cref{eq:cma:rho}. However, there was no statistical difference between 
adopting either objective function. Therefore, minimisation of expectation of 
$\rho$, is preferred over simply using the unscaled resulting makespan. 

\begin{figure}[t]
	\includegraphics[width=\textwidth]{CMAboxplotEvoTrain}
	\caption{Box-plot of training data for percentage relative deviation from 
	optimality, defined by \cref{eq:rho}, when implementing the final weights 
	obtained from CMA-ES optimisation, using both objective functions from 
	\cref{eq:cma:makespan,eq:cma:rho}, left and right, 
	respectively.}\label{fig:cma:trainboxpl}
\end{figure}

\section{Problem difficulty}\label{sec:expr:data}
The evolution of fitness per generation from the CMA-ES optimisation of 
\cref{eq:cma:rho} is depicted in \cref{fig:cma:fit}. Note, all problem spaces 
reached their allotted computational time without converging. In fact 
\frnd{6}{5} and \jrndn{6}{5} needed restarting during the optimisation process.
Furthermore, the  evolution of the decision variables $\vec{w}$ are depicted in 
\cref{fig:cma:wei}. As one can see, the relative contribution for each weight 
clearly differs between problem spaces. Note, that in the case of 
\jrndn{6}{5} (cf. \cref{fig:cma:wei:rho}), CMA-ES restarts around generation 
1,000 and quickly converges back to its previous fitness. However, lateral 
relation of weights has completely changed, implying that there are many 
optimal combinations of weights to be used. This can be expected due to the 
fact some features in \cref{tbl:features} are a linear combination of others, 
e.g. $\phi_3=\phi_1+\phi_2$.

\begin{figure}[t]\centering
	\includegraphics[width=\columnwidth]{CMAfitnessEvo}
	\caption{Fitness for optimising (w.r.t. \cref{eq:cma:makespan,eq:cma:rho} 
	above and below, receptively), per generation of the CMA-ES 
	optimisation.}\label{fig:cma:fit}
\end{figure}

\input{tables/esmodels-runs}

\begin{figure} 
	\subcaptionbox{minimise w.r.t. \cref{eq:cma:makespan} 
	\label{fig:cma:wei:cmax}}{\includegraphics[width=\textwidth]{{CMAweightsEvo.ES_Cmax}.eps}}
	\\
	\subcaptionbox{minimise w.r.t. \cref{eq:cma:rho} 
	\label{fig:cma:wei:rho}}{\includegraphics[width=\textwidth]{{CMAweightsEvo.ES_rho}.eps}}
	\caption{Evolution of weights of features (given in \cref{tbl:features}) at 
	each generation of the CMA-ES optimisation. Note, weights are normalised 
	such that $\norm{\vec{w}}=1$.}\label{fig:cma:wei}
\end{figure}

\subsection{Scalability}\label{sec:expr:scalability} 
As a benchmark, the linear ordinal regression model (PREF) from \cref{InRu11a} 
was created.
Using the weights obtained from optimising \cref{eq:cma:rho} and applying them 
on their  $6\times5$ training data. Their main statistics of \cref{eq:rho} 
are reported in \cref{tbl:results:train} for all training sets described in 
\cref{tbl:data}. Moreover, the best SDR from which the features in 
\cref{tbl:features} were inspired by, are also reported for comparison, i.e., 
most work remaining (MWR) for all \JSP\ problem spaces, and least work 
remaining (LWR) for all \FSP\ problem spaces.

To explore the scalability of the learning models, a similar comparison to 
\cref{sec:expr:data} is made for applying the learning models on their 
corresponding $10\times10$ testing data. Results are reported in 
\cref{tbl:results:test}. Note, that only resulting $C_{\max}$ is reported as 
the optimum makespan is not known and \cref{eq:rho} is not applicable. 

{\setlength{\tabcolsep}{3pt}
    \input{tables/esmodels.6x5}
    \input{tables/esmodels.10x10}
}

\section{Discussion and conclusions}\label{sec:disc}
Data distributions considered in this study either varied 
w.r.t. the processing time distributions, continuing the preliminary 
experiments in  \cref{InRu11a}, or 
w.r.t. the job ordering permutations -- i.e., homogeneous machine order for 
\FSP\ versus heterogeneous machine order for JSP. 
From the results based on $6\times5$ training data given  in 
\cref{tbl:results:train}, it's obvious that CMA-ES optimisation substantially 
outperforms the previous PREF methods from \cref{InRu11a} for all problem 
spaces considered. Furthermore, the results hold when testing on $10\times10$ 
(cf. \cref{tbl:results:test}), suggesting the method is indeed scalable to 
higher dimensions. 

Moreover, the study showed that the choice of objective function  for 
evolutionary search is worth investigating. There was no statistical difference 
from minimising the fitness function directly and its normalisation w.r.t. true 
optimum (cf. \cref{eq:cma:makespan,eq:cma:rho}), save for \frndn{6}{5}. 
Implying, even though ES doesn't rely on optimal solutions, there are some 
problem spaces where it can be of great benefit. This is due to the fact that 
the problem instances can vary greatly within the same problem space 
\cref{InRu12}. Thus normalising the objective function would help the 
evolutionary search to deviate from giving too much weight for problematic 
problem instances.

The weights for \cref{eq:CDR:feat} in \cref{InRu11a} were found using 
supervised learning, where the training data was created from optimal solutions 
of randomly generated problem instances. As an alternative, this study showed  
that minimising the mean makespan directly using a brute force search via 
CMA-ES actually results in a better CDRs. The nature of CMA-ES is to explore 
suboptimal routes until it converges to an optimal one. Implying that the 
previous approach of only looking into one optimal route may not produce a 
sufficiently rich training set. That is, the training set should incorporate a 
more complete knowledge on \emph{all} possible preferences, i.e., make also the 
distinction between suboptimal and sub-suboptimal features, etc.  This would 
require a Pareto ranking of preferences which can be used to make the 
distinction to which feature sets are equivalent, better or worse -- and to 
what degree, i.e., by giving a weight to the preference. This would result in a 
very large training set, which of course could be re-sampled in order to make 
it computationally feasible to learn.

The main drawback of using evolutionary search for learning optimal weights for 
\cref{eq:CDR:feat} is how computationally expensive it is to evaluate the mean 
expected fitness. Even for a low problem dimension 6-job 5-machine JSP, each 
optimisation run reached their walltime of 288 hours without converging. Now, 
$6\times5$ JSP requires 30 sequential operations where at each time step there 
are up to $6$ jobs to choose from -- i.e., its complexity is 
$\mathcal{O}(n^{n\cdot m})$ making it computationally infeasible to apply this 
framework for higher dimensions as is. 
However, evolutionary search only requires the rank of the candidates and 
therefore it is appropriate to retain a sufficiently accurate surrogate for the 
value function during evolution in order to reduce the number of costly true 
value function evaluations, such as the approach in \cref{InRu11b}. This could 
reduce the computational cost of the evolutionary search considerably, making 
it feasible to conduct the experiments from \cref{sec:expr} for problems of 
higher dimensions, e.g., with these adjustments it is possible to train on 
$10\times10$ and test on for example $14\times14$ to verify whether scalability 
holds for even higher dimensions.  





\todoWrite{A prevalent approach to solving job shop scheduling problems is to 
    combine several relatively simple dispatching rules such that they may 
    benefit 
    each other for a given problem space. Generally, this is done in an ad-hoc 
    fashion, requiring expert knowledge from heuristics designers, or extensive 
    exploration of suitable combinations of heuristics. The approach here is to 
    automate that selection by translating dispatching rules into measurable 
    features and optimising what their contribution should be via evolutionary 
    search. The framework is straight forward and easy to implement and shows 
    promising results. Various data distributions are investigated for both job 
    shop and flow shop problems, as is scalability for higher dimensions. 
    Moreover, the study shows that the choice of objective function  for 
    evolutionary search is worth investigating. Since the optimisation is based 
    on minimising the expected mean of the fitness function over a large set of 
    problem instances which can vary within the set, then normalising the 
    objective function can stabilise the optimisation process away from local 
    minima.}
