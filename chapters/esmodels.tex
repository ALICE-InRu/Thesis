\HeaderQuote{There's a large mustard-mine near here. And the moral of that is 
-- The more there is of mine, the less there is of yours.}{The Duchess} 


\chapter{Evolutionary Learning of CDRs}\label{ch:esmodels} 

\FirstSentence{G}{enetic algorithms (GA) are one of the} most widely used 
approaches in \JSP\ literature \citep{Pinedo08}. 
GA is search heuristic that is inspired by the process of natural selection, 
and is a subclass of evolutionary algorithms (EA), which generate solutions to 
optimisation problems using techniques based on natural evolution, such as 
inheritance, mutation, selection, and crossover.

When applying GAs to \JSP, an extensive number of schedules need to be 
evaluated, and even for low dimensional JSP, it can quickly become 
computationally infeasible. GAs can be used directly on schedules 
\citep{Cheng96,Cheng99,Tsai07,Qing-dao-er-ji12,Ak12}. 
However, then there are many concerns that need to be dealt with. 
To begin with there are nine encoding schemes for representing the schedules 
\citep{Cheng96}, in addition, special care must be taken when applying 
cross-over and mutation operators in order for the schedules (now in the role 
of `chromosomes') to still remain feasible. 
Moreover, in case of JSP, GAs are not adapt for fine-tuning around optima. 
Luckily a subsequent local search can mediate the optimisation 
\citep{Cheng99,Meeran12}.

The most predominant approach in hyper-heuristics, a framework of creating 
\emph{new} heuristics from a set of  predefined heuristics, is genetic 
programming \citep{Burke10}. Dispatching rules based genetic algorithms (DRGA) 
\citep{Vazquez-Rodriguez09,Dhingra10,Nguyen13} are a special case of genetic 
programming \citep{Koza05}, where GAs are applied indirectly to JSP via 
\dr s, i.e., where a solution is no longer a \emph{proper} schedule 
but a \emph{representation} of a schedule via applying certain DRs 
consecutively. 

As previously discussed in \cref{ch:introduction}, there are two main 
viewpoints on how to approach scheduling problems
\begin{enumerate*}
    \item tailored algorithms where schedules are built for one problem 
    instance at a time
    \item general algorithms where schedules are built for all problem 
    instances at once
\end{enumerate*}
For tailored algorithms a simple construction heuristic is applied. The 
schedule's features are collected at each dispatch iteration from which a 
learning model will inspect the feature set to discriminate which operations 
are preferred to others via ordinal regression. The focus is essentially on 
creating a meaningful preference set composed of features and their ranks as 
the learning algorithm is only run once to find suitable operators for the 
value function. This is the approach taken in \cref{InRu11a}. Expanding on 
that  work, this \lcnamecref{ch:esmodels} will explore a general algorithms 
construction viewpoint where there is no feature set collected beforehand since 
the learning model is optimised directly via evolutionary search. The framework 
was first reported in \cref{InRu14}, and later used to improve the preference 
models in \cref{InRu15a}. 

Evolutionary search requires numerous costly value function evaluations. 
In fact it involves an indirect method of evaluation whether one learning model 
is preferable to another, w.r.t. which one yields a better expected mean. For 
that reason, it can be mediated with the use of preference learning, as 
discussed in \cref{InRu11b}, albeit for traditional test functions suite (in 
particular Rosenbrock's function and Sphere model).

\section{Experimental setting}
A prevalent approach to solving \JSP\ is to combine several relatively simple 
dispatching rules such that they may benefit each other for a given problem 
space. Generally, this is done on an ad-hoc basis, requiring expert knowledge 
from heuristics designer, or extensive exploration of suitable combinations of 
heuristics. The approach in \cref{InRu14}, was to automate that selection 
similar to DRGA, by translating dispatching rules into measurable features and 
optimising what their contribution should be via evolutionary search, i.e., 
optimise the weights $\vec{w}$ in \cref{eq:CDR:feat} directly using
covariance matrix adaptation evolution strategy (CMA-ES) by \cite{Hansen01}, 
which has been proven to be a very efficient numerical optimisation technique. 
The framework is straight forward and easy to implement and shows promising 
results. 


Moreover, \cref{sec:es:measure} shows that the choice of objective function  
for evolutionary search is worth investigating. Since the optimisation is based 
on minimising the expected mean of the fitness function over a large set of 
problem instances, which can vary within. Then normalising the objective 
function can stabilise the optimisation process away from local minima. 

Preliminary experiments were first reported in \cref{InRu14,InRu15a} using 
\begin{enumerate*}
    \item standard set-up of parameters of the CMA-ES optimisation
    \item runtime was limited to 288 hours on a cluster for each 
    \Problem[6\times5]{\text{train}} problem set given in \cref{tbl:data}, 
    where in every case the optimisation reached its maximum walltime
\end{enumerate*}
\Cref{InRu14} had one model for all $K$ time steps, whereas \cref{InRu15a} used 
a different stepwise model at each dispatch iteration. 

Various data distributions from \cref{ch:genprobleminstances} are investigated. 
Due to computational cost, only $6\times5$ instances were initially 
considered. However, since then the scheduling subroutines have been made more 
time-efficient, making \Problem[10\times10]{\text{train}} applicable in a 
reasonable amount of time. 
CMA-ES models will be mostly trained on the lower dimension, $6\times5$, 
and only the general random $10\times10$ \JSP\ case is explored. 
Finally, to check robustness, models are validated on benchmark tests sets from 
the OR-Library (cf. \cref{sec:data:orlib}).


\section{Performance measures}\label{sec:es:measure}
Generally, evolutionary search only needs to minimise the expected fitness 
value. However, the  approach in \cref{InRu11a} was to use the known optimum to 
correctly label which operations' features were optimal when compared to other 
possible operations. Therefore, it would be of interest to inspect if there is 
any performance edge gained by incorporating optimal labelling in evolutionary 
search. Therefore, two objective functions will be considered, namely, 
\begin{subequations}\label{eq:cma:objfun}
\begin{equation}\label{eq:cma:makespan}
    \minCmax := \min \Exp{C_{\max}} 
\end{equation}
for optimising w.r.t. $C_{\max}$ directly, and on the other hand
\begin{equation}\label{eq:cma:rho}
    \minRho := \min \Exp{\rho} 
\end{equation} 
which optimises w.r.t. the resulting $C_{\max}$ scaled to its true optimum, 
i.e., \cref{eq:rho}.
\end{subequations}

\section{Experimental results}

Main statistics of the experimental run are given in \cref{tbl:cma:run} and 
depicted in \cref{fig:cma:fit} for both approaches. In addition, evolving 
decision variables, here weights $\vec{w}$ for \cref{eq:CDR:feat}, are 
depicted in \cref{fig:cma:wei}. 

{\setlength{\tabcolsep}{3pt} \input{tables/esmodels-runs} }

In order to compare the two objective functions, the best weights reported were 
used for \cref{eq:CDR:feat} on the corresponding training and test set. 
Its box-plot of \fullnamerho, is depicted in \cref{fig:cma:boxplot:set} and 
\cref{tbl:cma:boxplot:set} present its main statistics: mean, median, standard 
deviation, minimum and maximum values. 
Furthermore, \cref{fig:cma:boxplot:orlib} depicts the box-plot when applying 
the time independent\footnote{Note, time dependant models are inapplicable
    for OR-Library, since they're size $n\times m$ doesn't match any in the 
    benchmark set. However, this is irrelevant for time independent models.} 
models (for all training data) on the benchmark problem 
instances from the OR-Library.

In the case of \frndn{6}{5}, \cref{eq:cma:rho} gave a considerably worse 
results, since the optimisation got trapped in a local minima, as the erratic 
evolution of the weights in \cref{fig:cma:wei:cmax} suggest.
For other problem spaces, \cref{eq:cma:makespan} gave slightly better results 
than \cref{eq:cma:rho}. However, there was no statistical difference between 
adopting either objective function. Therefore, minimisation of expectation of 
$\rho$, is preferred over simply using the unscaled resulting makespan. 

\begin{figure}[p]
    \subcaptionbox{Models applied to corresponding training and test set
        \label{fig:cma:boxplot:set}}{
        \includegraphics[width=\textwidth]{{boxplot.CMAES}.pdf}}
    \subcaptionbox{Models applied to ORLIB test set 
        \label{fig:cma:boxplot:orlib}}{
        \includegraphics[width=\textwidth]{{boxplot.CMAES.ORLIB}.pdf}}
	\caption{Box-plot for \namerho, when implementing the final weights 
	obtained from CMA-ES optimisation, using objective functions from 
	\cref{eq:cma:objfun}.}\label{fig:cma:boxplot}
\end{figure}


{\setlength{\tabcolsep}{3pt} \input{tables/stats.boxplot.CMAES.tex}}

\section{Problem difficulty}\label{sec:expr:data}
The evolution of fitness per generation from the CMA-ES optimisation of 
\cref{eq:cma:rho} is depicted in \cref{fig:cma:fit}. Note, most problem spaces 
reached their allotted maximum function evaluations without converging. In fact 
several problem spaces, e.g., \frnd{6}{5}, needed restarting during the 
optimisation process.
Notice \jrndJ{6}{5} especially, then $\minCmax$ needs more than twice the 
amount of function evaluations than using $\minRho$ as an objective.

Furthermore, the  evolution of the decision variables $\vec{w}$ are depicted in 
\cref{fig:cma:wei}. As one can see, the relative contribution for each weight 
clearly differs between problem spaces. Note, that in the case of 
\jrnd{6}{5} (cf. \cref{fig:cma:wei:1,fig:cma:fit}), CMA-ES restarts around 
generation 1,600 and quickly converges back to its previous fitness. 
However, lateral relation of weights has completely changed, implying that 
there are many optimal combinations of weights to be used. This can be expected 
due to the fact some features in \cref{tbl:features} are a linear combination 
of others, e.g. $\phi_3=\phi_1+\phi_2$.

\begin{figure}[t]\centering
    \includegraphics[width=\columnwidth]{{CMAES.generation.log.fitness}.pdf}
    \caption{Log fitness for optimising w.r.t. \cref{eq:cma:objfun}, per 
    generation of the CMA-ES optimisation.}\label{fig:cma:fit}
\end{figure}


\begin{figure} 
\subcaptionbox{evolution of weights for \jrnd{n}{m} time independent models
     \label{fig:cma:wei:1}}{
\includegraphics[width=\textwidth]{{CMAES.weights.timeindependent}.pdf}}\\
\subcaptionbox{stepwise evolultion of final weights for \jrnd{6}{5} time 
    dependent models \label{fig:cma:wei:K}}{
    \includegraphics[width=\textwidth]{{CMAES.weights.timedependent}.pdf}}
\caption{Evolution of weights for features (given in \cref{tbl:features}).
    Note, weights are normalised such that $\norm{\vec{w}}=1$.}
\label{fig:cma:wei}
\end{figure}

\section{Discussion and conclusions}\label{sec:disc}
Data distributions considered in this study either varied 
w.r.t. the processing time distributions, continuing the preliminary 
experiments in  \cref{InRu11a}, or 
w.r.t. the job ordering permutations -- i.e., homogeneous machine order for 
\FSP\ versus heterogeneous machine order for JSP. 
From the results based on $6\times5$ training data given  in 
\cref{tbl:results:train}, it's obvious that CMA-ES optimisation substantially 
outperforms the previous PREF methods from \cref{InRu11a} for all problem 
spaces considered. Furthermore, the results hold when testing on $10\times10$ 
(cf. \cref{tbl:results:test}), suggesting the method is indeed scalable to 
higher dimensions. 

Moreover, the study showed that the choice of objective function  for 
evolutionary search is worth investigating. There was no statistical difference 
from minimising the fitness function directly and its normalisation w.r.t. true 
optimum (cf. \cref{eq:cma:makespan,eq:cma:rho}), save for \frndn{6}{5}. 
Implying, even though ES doesn't rely on optimal solutions, there are some 
problem spaces where it can be of great benefit. This is due to the fact that 
the problem instances can vary greatly within the same problem space 
\cref{InRu12}. Thus normalising the objective function would help the 
evolutionary search to deviate from giving too much weight for problematic 
problem instances.

The weights for \cref{eq:CDR:feat} in \cref{InRu11a} were found using 
supervised learning, where the training data was created from optimal solutions 
of randomly generated problem instances. As an alternative, this study showed  
that minimising the mean makespan directly using a brute force search via 
CMA-ES actually results in a better CDRs. The nature of CMA-ES is to explore 
suboptimal routes until it converges to an optimal one. Implying that the 
previous approach of only looking into one optimal route may not produce a 
sufficiently rich training set. That is, the training set should incorporate a 
more complete knowledge on \emph{all} possible preferences, i.e., make also the 
distinction between suboptimal and sub-suboptimal features, etc.  This would 
require a Pareto ranking of preferences which can be used to make the 
distinction to which feature sets are equivalent, better or worse -- and to 
what degree, i.e., by giving a weight to the preference. This would result in a 
very large training set, which of course could be re-sampled in order to make 
it computationally feasible to learn.

The main drawback of using evolutionary search for learning optimal weights for 
\cref{eq:CDR:feat} is how computationally expensive it is to evaluate the mean 
expected fitness. Even for a low problem dimension such  as 6-job 5-machine 
JSP, each optimisation run reached their maximum allotted function evaluations 
without converging. 
Now, $6\times5$ \JSP\ requires 30 sequential operations where at each time step 
there are up to $6$ jobs to choose from, in fact its complexity is  
$\bigOh{n!^m}$ \citep{Giffler60} making it computationally infeasible to apply 
this framework for higher dimensions as is. Especially, considering that it's 
preferred to run these experiments several times -- e.g. in \cref{InRu14} 
\fjc{6}{5} got stuck in local minima for $\minCmax$, which could have been 
avoided by restarting the optimisation.
However, evolutionary search only requires the rank of the candidates and 
therefore it is appropriate to retain a sufficiently accurate surrogate for the 
value function during evolution in order to reduce the number of costly true 
value function evaluations, such as the approach in \cref{InRu11b}. This could 
reduce the computational cost of the evolutionary search considerably, making 
it feasible to conduct the experiments from \cref{sec:expr} for problems of 
higher dimensions, e.g., with these adjustments it is possible to train on 
$10\times10$ with greater ease, or even considering even  higher dimensions, 
e.g. $14\times14$.
