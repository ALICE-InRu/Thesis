\HeaderQuote{It was much pleasanter at home, when one wasn't always growing 
larger and smaller, and being ordered about by mice and rabbits.}{Alice} 

\chapter{Feature Selection}\label{ch:featselect}

\FirstSentence{F}{rom \cref{ch:esmodels} there exists linear} weights $\vec{w}$ 
for \cref{eq:CDR:feat} found with evolutionary optimisation that achieve a 
lower \namerho, than any of the preference models from \cref{ch:prefmodels}
has been able to outperform.\footnote{Although \PsiSet[\minCmax]{p} can be 
  statistically insignificant to its original trajectory iff stepwise 
  sampling is adjusted w.r.t. its preference set (cf. \cref{sec:pref:bias}).} 
This goes to show that the $d=\NrFeatLocal$ features are `enough' -- 
meaning there is not a need for defining new ones just yet. 
However, the optimal weights for \cref{eq:cma:objfun} were quite erratic (cf. 
\cref{fig:cma:wei}). 
Perhaps the features from \PhiSet{\CMAES} are contradictory, and therefore not 
suitable for preference learning. 

Furthermore, the SDRs we've inspected so-far are based on two job-attributes 
from \cref{tbl:features}, namely
\begin{enumerate*}[after={{,}}]
  \item \phiproc\ for SPT and LPT 
  \item \phijobWrm\ for LWR and MWR 
\end{enumerate*}
by choosing the lowest value for SPT and LWR, and highest value for LPT and 
MWR, i.e., the extremal values for those attributes. 
These SDRs achieve a remarkably low $\rho$, suggesting maybe not that many 
additional features are needed to achieve a competitive result.

For this study we will consider all combinations of feature attributes using 
either one, two, three or all $d=\NrFeatLocal$ of them, for a grand total of:
\begin{equation}\label{eq:697}
{d \choose 1}+{d \choose 2}+{d \choose 3}+{d \choose d} = 697
\end{equation}
The reason for such a limiting number of active features, are due to the fact 
we want to keep the models simple enough for improved model interpretability.
Furthermore, we will continue to use our baseline preference set from 
\cref{InRu11a,InRu14,InRu15a,InRu15b}, namely \PsiSet[\OPT]{p}.

For each feature combination, a linear preference model is created, where 
\PsiSet{p} is limited to the predetermined feature combination. 
This was done for all \Problem[10\times10]{\train} in \cref{tbl:data} (save for 
job and machine variation), 
each consisting of $N_{\train}=300$ problem instances.
Moreover, in order to report the validation accuracy, 20\% 
(i.e. $N_{\text{val}}=60$) of the training set was set aside for 
reporting the accuracy.

\section{Validation accuracy}\label{sec:CDR:acc}
As the preference set \PsiSet{p} has both preference pairs belonging to optimal
ranking, and subsequent rankings, it is not of primary importance to classify
\emph{all} rankings correctly, just the optimal ones. Therefore, instead of
reporting the validation accuracy based on the classification problem of the
correctly labelling the entire problem set \PsiSet{p}, it's opted that the 
validation accuracy is obtained using \cref{eq:tracc:opt}, namely the 
probability of choosing an optimal decision given the resulting linear 
weights.\footnote{Due to superabundant number of models then calculating 
    the \emph{preferable} $\xi_{\pi}$ from \cref{eq:tracc:track} is not viable.}
However, in this context, the mean throughout the dispatching process is
reported, i.e., 
\begin{equation}\label{eq:meanstepwiseopt}
\overline{\xi^{\star}_{\pi}} = \frac{1}{K}\sum_{k=1}^K \xi^{\star}_{\pi}(k)
\end{equation}
\Cref{fig:stepwise_vs_classification} shows the difference between
the two measures of reporting validation accuracy. 

Validation accuracy based on \cref{eq:meanstepwiseopt} only takes into 
consideration the likelihood of choosing the optimal move at each time step. 
However, the classification accuracy is also trying to correctly distinguish 
all subsequent rankings in addition of choosing the optimal move, as expected 
that measure is considerably lower. 

\begin{figure}[p]
  \centering
  \includegraphics[width=\linewidth]{figures/ALL/{training.accuracy}.pdf}
  \vspace*{-24pt}
  \caption{Various methods of reporting validation accuracy for preference 
    learning}
  \label{fig:stepwise_vs_classification}
\end{figure}

\section{Pareto front}\label{sec:CDR:pareto}
When training the learning model one wants to keep the validation accuracy 
high, as that would imply a higher likelihood of making optimal decisions, 
which would in turn translate into a low final makespan. To test the validity 
of this assumptions, each of the 697 models is run on the preference set, and 
its mean $\rho$ is reported against its corresponding validation accuracy 
\cref{eq:meanstepwiseopt} in \cref{fig:CDR:scatter}. 
The models are colour-coded w.r.t. the number of active features, and a line is 
drawn through its Pareto front. 
Those solutions are labelled with their corresponding model ID. 
Moreover, the Pareto front over all 697 models, irrespective of active feature 
count, is denoted with triangles. 
Their values are reported in \cref{tbl:CDR:pareto}, where the best objective is 
given in boldface. 

\begin{figure}[p]
    \centering
    \includegraphics[width=\linewidth]{figures/ALL/{pareto.rho.vs.acc}.pdf}
    \vspace*{-24pt}
    \caption{Scatter plot for validation accuracy  (\%) against its 
        corresponding mean expected $\rho$ (\%) for all 697 linear models from 
        \cref{eq:697}.
        Pareto fronts are for each active feature count, and labelled with 
        their model ID. Moreover, actual Pareto front over all models is marked 
        with triangles.} \label{fig:CDR:scatter}
\end{figure}

\begin{table}[p]
    \caption{Mean validation accuracy and mean expected deviation from 
        optimality, $\rho$, for all CDR models on the Pareto front from 
        \cref{fig:CDR:scatter}.}\label{tbl:CDR:pareto}
    \input{tables/PREF-equal}
\end{table}

\section{Inspecting weight contribution to end-result}
\Cref{fig:CDR:weights} depicts $\vec{w}$ for all of the learned CDR models 
reported in \cref{tbl:CDR:pareto}. 
The weights have been normalised for clarity purposes, such that it is scaled 
to $\norm{\vec{w}}=1$, thereby giving each feature their proportional 
contribution to the preference $I_j^{\pi}$ defined by \cref{eq:CDR}. 
These weights will now be explored further, along with testing whether models 
are statistically significant to one another, using a 
Kolmogorov-Smirnov test with $\alpha=0.05$.

For \jrnd{10}{10}  there is no statistical difference between models (2.69, 
3.355, 3.358, 3.524), w.r.t. $\rho$ and the latter three w.r.t. 
accuracy. These models are therefore equivalently `best' for the problem space.
As \cref{fig:CDR:weights} shows, \phiendTime, \phijobWrm\ and \phimacWrm\ are 
similar in value, and in the case of 3.358, then \phimacFree\ has similar 
contribution as \phiendTime\ for the other models. 
Which, as standalone models are 1.6 and 1.3, respectively, and yield 
equivalent mean $\rho$ and accuracy.
As these features often coincide in \jsp\, it is justifiable to use only 
either one, as they contains the same information as its 
counterpart.\footnote{Note, \phiendTime$~\leq~$\phimacFree, where
  \phiendTime$~=~$\phimacFree\ when $J_j$ is the latest job on $M_a$, 
  otherwise $J_j$ is placed in a previously created slot on $M_a$.}
Most likely, the equivalence of these features is indicating that the 
schedules are rarely able to dispatch in earlier slots, i.e., 
\phiendTime$~\approx~$\phimacFree. 

In addition, (2.111, 3.300) and (16.1, 3.355) are statistically insignificant 
w.r.t. validation accuracy for \jrnd{10}{10}. However, they have considerable 
performance difference w.r.t. $\rho$ ($\Delta\rho \approx 18\%$). 
So even looking at mean stepwise optimality from \cref{eq:meanstepwiseopt} by 
itself is very fickle, because slight variations can be quite dramatic to the 
end result. 

The solutions on the Pareto front for \jrndn{10}{10} are quite a few models
with no (or minimal) statistical difference w.r.t. $\rho$, and 
considerably more w.r.t. validation accuracy. 
Most notably are (3.281, 2.73, 2.75, 1.13)
(note, first one has the lowest mean $\rho$), which are all statistically 
insignificant w.r.t. validation accuracy yet none w.r.t. $\rho$, with 
difference up to $\Delta\rho=6.32\%$.

For \frnd{10}{10} almost all models are statistically different w.r.t. $\rho$, 
only exception is (1.6, 1.7).
Although, w.r.t. validation accuracy, there are a few equivalent models, 
namely, (3.151, 2.51), (2.94, 1.6) and (3.216, 3.213, 16.1), with $1.2\%$, 
$2.3\%$ and $5.75\%$ difference in mean $\rho$, respectively. 

It's interesting to inspect the full model for \frnd{10}{10}, 16.1. 
Despite having similar contributions, yet statistically significantly 
different, as all the active features as (3.213, 3.216), then the (slight) 
interference from of other features, hinders the full model from achieving a 
low $\rho$. 
Only considering \phijobOps\ and \phimacOps\ with either \phiendTime\ and 
\phimacFree, boosts performance by 5.28\% and 5.72\%, respectively. 
Thereby stressing the importance of feature selection, to steer clear of 
over-fitting. Note, unlike \jrnd{10}{10}, now \phiendTime\ differs from 
\phimacFree, indicating that there are some slots created, which could be 
better utilised.
Moreover, looking at model 2.111 for \frnd{10}{10}, which has similar 
contributions as the best model, 3.539. Then introducing a third feature, 
\phimacWrm, is the key to the success of the CDR, with a boost of $\rho$ 
performance by 1.33\%. 

For both \jrnd{10}{10} and \jrndn{10}{10}, model 1.13 is on the Pareto 
front. The model corresponds to feature \phijobWrm, and in both cases has a 
weight strictly greater than zero (cf. \cref{fig:CDR:weights}). Revisiting 
\cref{eq:CDR:SDR}, we observe that this implies the learning 
model was able to discover MWR as one of the Pareto solutions, and as is 
expected, there is no statistical difference to between 1.13 and MWR.

As one can see from \cref{fig:CDR:scatter}, adding additional features to 
express the linear model boosts performance in both validation accuracy and 
expected mean for $\rho$, i.e., the Pareto fronts are cascading towards more 
desirable outcome with higher number  of active features. However, there is a 
cut-off point for such improvement, as using all features is generally 
considerably worse off due to overfitting of classifying the preference set.

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{figures/{j.rnd}/{pareto.10x10.phi}.pdf}\\
  \includegraphics[width=\textwidth]{figures/{j.rndn}/{pareto.10x10.phi}.pdf}\\
  \includegraphics[width=\textwidth]{figures/{f.rnd}/{pareto.10x10.phi}.pdf}
  \vspace*{-24pt}
  \caption{Normalised weights for CDR models from \cref{tbl:CDR:pareto}, 
    models are grouped w.r.t. its dimensionality, $d$. Note, a triangle 
    indicates a solution on the Pareto front.
    Furthermore, bottom annotation indicates their objectives
    $\{\overline{\rho},\overline{\xi_{\text{CDR}}^{\star}}\}$.}
  \label{fig:CDR:weights}
\end{figure}

\section{Evolution of validation accuracy}\label{sec:CDR:pareto:tradeoff}
Let's inspect the models corresponding to the minimum mean $\rho$ and 
highest mean validation accuracy, highlighted in \cref{tbl:CDR:pareto} and 
inspect the evolution of $\xi_{\pi}^\star(k)$ for those models in 
\cref{fig:CDR:opt}, again using probability of randomly guessing an optimal 
move (i.e. $\xi^{\star}_{\RND}$ from \cref{fig:diff:opt:rnd}) as a benchmark.
As one can see for both \jrnd{10}{10} and \jrndn{10}{10}, despite having a 
higher mean validation accuracy overall, the probabilities vary significantly. 
A lower mean $\rho$ is obtained when the validation accuracy is gradually 
increasing over time, and especially during the last phase of the 
scheduling.\footnote{It's almost illegible to notice this shift directly 
  from \cref{fig:CDR:opt}, as the difference between the two best models is 
  oscillating up to only 3\% at any given step. In fact \jrndn{10}{10} has 
  the most clear difference w.r.t. classification accuracy of indicating when 
  a minimum $\rho$ model excels at choosing the preferred move.} 
Revisiting \cref{fig:diff:case:OPT:10x10}, this trend indicates that it's 
likelier for the resulting makespan to be considerably worse off if suboptimal 
moves are made at later stages, than at earlier stages. 
Therefore, it's imperative to make the `best' decision at the `right' moment, 
not just look at the overall mean performance. 
Hence, the measure of validation accuracy as discussed in \cref{sec:CDR:acc} 
should take into consideration the impact a suboptimal move yields on a 
step-by-step basis, e.g., \cref{eq:tracc:opt} should be weighted w.r.t. a curve 
such as \cref{eq:bwc:opt}.

\begin{figure}[p]
  \centering
  \includegraphics[width=\linewidth]{figures/ALL/{stepwise.10x10.FeatSelect}.pdf}
  \vspace*{-30pt}
  \caption{Probability of choosing optimal move for models corresponding to 
    highest mean validation accuracy and lowest mean deviation from 
    optimality, $\rho$, compared to the baseline of probability of 
    choosing an optimal move at random (dashed)}
  \label{fig:CDR:opt}
\end{figure}

\input{tables/stats.exhaust.10x10}

\section{Comparison to other approaches}
Main statistics for the best models from \cref{eq:697} are reported in 
\cref{tbl:best.exhaust:stats}. 
Going back to the original SDRs discussed in \cref{sec:SDR} along with the 
CMA-ES obtained weights for \cref{eq:cma:makespan} and compare then against the 
best CDR models, a box-plot for $\rho$ is depicted in \cref{fig:boxplot:CDR}. 
Firstly, there is a statistical difference between all models, and the CDR 
model corresponding to minimum mean $\rho$ value, is the clear winner for 
\jrndn{10}{10} and \frnd{10}{10}. On the other hand, for \jrnd{10}{10} it loses 
by $\Delta\rho\approx-2.6\%$ to $\minCmax$ optimisation.
In all cases the there is substantial performance boost w.r.t. SDRs
\begin{enumerate*}
    \item 8.2\% from \jrnd{10}{10}'s MWR
    \item 5.9\% from \jrndn{10}{10}'s MWR 
    \item 6.5\% from \frnd{10}{10}'s LWR)
\end{enumerate*}  
However, the best model w.r.t. maximum validation accuracy, then the CDR model 
shows a lacklustre performance. In some cases it's better off, e.g., compared 
to \frnd{10}{10}'s LWR ($\Delta\rho\approx1\%$), yet for \jsp\ it doesn't 
surpass the performance of MWR. 
This implies, the learning model is over-fitting the training data. 
Results hold for the test set. 

\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/{boxplotRho.FeatSelect.10x10}.pdf}
  \caption{Box-plot for \namerho, for the best CDR preference models (cf. 
  \cref{tbl:CDR:pareto}) and compared against their best reference model.} 
  \label{fig:boxplot:CDR}
\end{figure}

\clearpage
\section{Conclusions}

When training the learning model, it's not sufficient to only optimise w.r.t. 
highest mean validation accuracy defined in \cref{eq:meanstepwiseopt}. 
As \cref{sec:CDR:pareto:tradeoff} showed, there is a trade-off between making 
the over-all best decisions versus making the right decision on crucial time 
points in the scheduling process, as \cref{fig:diff:case:OPT} clearly 
illustrated. 
It is for this reason, traditional feature selection such as \emph{add1} and 
\emph{drop1} were unsuccessful in preliminary experiments, 
and thus resorting to exhaustively searching all feature combinations.
This also opens of the question of how should validation accuracy be measured? 
Since the model is based on learning preferences, both based on optimal versus 
suboptimal, and then varying degrees of suboptimality. As we are only looking 
at the ranks in a `black and white' fashion, such that the makespans need to be 
strictly greater to belong to a higher rank, then it can be argued that some 
ranks should be grouped together if their makespans are sufficiently close. 
This would simplify the training set, making it (presumably) less of 
contradictions and more appropriate for preference learning. Or simply the 
validation accuracy in \cref{eq:meanstepwiseopt} could be weighted w.r.t. the  
difference in makespan.
During the dispatching process, there are some pivotal times which need to be 
especially taken care off. \Cref{fig:diff:case:OPT} showed how making 
suboptimal decisions were more of a factor during the later stages, whereas for 
flow-shop the case was exact opposite. 

Note, from \cref{sec:pref:bias} it's possible to sample w.r.t. stepwise bias 
such that it gives preference pairs that are more relevant to its 
end-performance. In other words, weighing the measure from 
\cref{eq:meanstepwiseopt} via the sampling strategy. 
Presumably, if such adjusted bias were applied to this study, then greater 
performance boost could be achieved.