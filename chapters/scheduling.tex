\HeaderQuote{Read the directions and directly you will be directed in the right direction.}{Doorknob} 

\chapter{Scheduling}\label{ch:scheduling}
\FirstSentence{S}{cheduling problems } are a type of combinatorial optimisation problems that occur frequently in practice. A subclass of scheduling problems is the \jsp\ scheduling problem (\JSP), which is widely studied in operations research. \JSP\ deals with the allocation of tasks of competing resources where its goal is to optimise a single or multiple objectives. The analogy is from manufacturing industry where a set of jobs are broken down into tasks that must be processed on several machines in a workshop. 
Furthermore, its formulation can be used on a wide variety of practical problems in real-life applications which involve decision making, therefore its problem-solving capabilities has a high impact on many manufacturing organisation. % Gefa dæmi?

Deterministic \JSP\ is the most \emph{general} case for classical scheduling problems \citep{Jain99}. Many other scheduling problems can be reformulated as \JSP. For instance the widely studied Travelling Salesman Problem can be contrived as \JSP\ with the salesman as a single machine in use and the cities to be visited are the jobs to be processed.
Moreover, the general form of \JSP\ assumes that each job can have its own distinctive flow pattern through the machines which is independent of the other jobs. In the case where all jobs share the same permutation route, \JSP\ is reduced to a permutation \fsp\ scheduling problem (\FSP) \citep{Guinet1998,Tay08}. Therefore, without loss of generality, this dissertation is structured around \JSP. 

\section{\Jsp\ scheduling problem}
\JSP\ considered for this dissertation is where $n$ jobs, $\mathcal{J}=\{J_j\}_{j=1}^n$, are scheduled on a finite set, $\mathcal{M}=\{M_a\}_{a=1}^m$, of $m$ machines, subject to the constraint that each job $J_j$ must follow a predefined machine order (a chain or sequence of $m$ operations, $\vsigma_j=[\sigma_{j1},\sigma_{j2},\dotsc,\sigma_{jm}]$) and that a machine can handle at most one job at a time. 
The objective is to schedule the jobs so as to minimise the maximum completion times for all tasks, also known as the makespan, $C_{\max}$. A common notion for this family of scheduling problems, i.e., a $m$ machine \JSP\ w.r.t. minimising makespan, is $Jm||C_{\max}$ \citep[cf.][]{Pinedo08}. In addition, for \FSP\ w.r.t. minimising makespan the notation is $Fm||C_{\max}$. 
An additional constraint commonly considered are job release-dates and due-dates, and then the objective is generally minimising the maximum lateness, denoted $Jm||L_{\max}$, however, those  constraints will not be considered here. 

Henceforth the index $j$ refers to a job $J_j\in\mathcal{J}$ while the index $a$ refers to a machine $M_a\in\mathcal{M}$. If a job requires a number of processing steps or operations, then the pair $(j,a)$ refers to the operation, i.e., processing the task of job $J_j$ on machine $M_a$. Moreover, index $k$ will denote the time step of the operation. Note that once an operation is started, it must be completed uninterrupted, i.e., pre-emption is not allowed. Moreover, there are no sequence dependent setup times.

\section{Mathematical formulation}
For any given \JSP, consisting of $n$ jobs for $m$ machines each job $J_j$ has an indivisible operation time (or cost) on machine $M_a$, $p_{ja}$, which is assumed to be integral and finite. %, where $J_j \in\mathcal{J}$ and $M_a\in\mathcal{M}$. 
Starting time of job $J_j$ on machine $M_a$ is denoted $x_s(j,a)$ and its completion time is denoted $x_f(j,a)$ where, 
\begin{equation}  x_f(j,a):=x_s(j,a)+p_{ja} \end{equation} 
Each job $J_j$ has a specified processing order through the machines, it is a permutation vector, $\vsigma_j$, of $\{1,..,m\}$, representing a job $J_j$ can be processed on $M_{\vsigma_j(a)}$ only after it has been completely processed on $M_{\vsigma_j(a-1)}$, i.e.,
\begin{equation}\label{eq:permutation}
   x_s(j,\vsigma_j(a)) \geq x_f(j,\vsigma_j(a-1)) 
\end{equation}
for all $J_j\in\mathcal{J}$ and $a\in\{2,..,m\}$. 
Note, that each job can have its own distinctive flow pattern through the machines which is independent of the other jobs. However, in the case that all jobs share the same permutation route \JSP\ is reduced to a \FSP\ \citep{Guinet1998,Tay08}.

The disjunctive condition that each machine can handle at most one job at a time is the following,
\begin{equation}\label{eq:oneJobPerMac}
   x_s(j,a) \geq x_f(j',a) \quad\textrm{or}\quad x_s(j',a) \geq x_f(j,a)  
\end{equation}
for all $J_j,J_{j'}\in\mathcal{J},\; J_j\neq J_{j'}$ and $M_a\in\mathcal{M}$. 

The objective function is to minimise the maximum completion times for all tasks, commonly referred to as the makespan, $C_{\max}$, which is defined as follows,
\begin{equation}
  C_{\max} := \max\{x_f(j,\vsigma_j(m))\;|\;J_j\in\mathcal{J}\}.\label{eq:makespan}
\end{equation} 
Clearly, w.r.t. minimum makespan, it is preferred that schedules are non-delay, i.e., the machines are not kept idle. The time in which machine $M_a$ is idle between consecutive jobs $J_j$ and $J_{j'}$ is called idle time, or flow, 
\begin{equation} s(a,j):=x_s(j,a)-x_f(j',a) \label{eq:slack}\end{equation}
where $J_j$ is the immediate successor of $J_{j'}$ on $M_a$. Although this is not a variable directly needed to construct a schedule for \JSP, it is a key feature in order to measure the quality of the schedule. 

\section{Construction heuristics}\label{sec:CH}
Construction heuristics are designed in such a way that it limits the search space in a logical manner, as to not to exclude the optimum. The construction heuristic here is to schedule the dispatches as closely together as possible, i.e., minimise the schedule's flow. 
More specifically, once an operation $(j,a)$ has been chosen from the ready-list, $\mathcal{R}$, by some dispatching rule, it can placed immediately after (but not prior) $x_f(j,\vsigma_j(a-1))$ on machine $M_a$ due to constraint \cref{eq:permutation}. 
However to guarantee that constraint \cref{eq:oneJobPerMac} is not violated, idle times $M_a$ are inspected, as they create flow time  which $J_j$ can occupy. Bearing in mind that $J_j$ release time is $x_f(j,\vsigma_j(a-1))$ one cannot implement \cref{eq:slack} directly, instead it has to be updated as follows,
\begin{eqnarray}
\tilde{s}(a,j')&:=& x_s(j'',a)-\max\{x_f(j',a),x_f(j,\vsigma_j(a-1))\} % \textrm{ if } x_f(j,\vsigma_j(a-1)\geq x_f(j',a) 
\end{eqnarray}
for all already dispatched jobs $J_{j'},J_{j''}\in \mathcal{J}_a$ where $J_{j''}$ is $J_{j'}$ successor on $M_a$. Since pre-emption is not allowed, the only applicable slots are whose idle time can process the entire operation, i.e.,
\begin{eqnarray}
\tilde{S}_{ja}&:=&\{J_{j'}\in \mathcal{J}_a\;|\;\tilde{s}(a,j')\geq p_{ja} \}\label{eq:slots}.
\end{eqnarray} 
Now, there are several heuristics for selecting a slot from \cref{eq:slots}, e.g., if the main concern were to utilise the slot space, then choosing the slot with the smallest idle time would yield a snugger fitted schedule and leaving larger idle times undiminished for subsequent dispatches on $M_a$. However dispatching $J_j$ in the first slot would result in its earliest possible release time, which would be beneficial for subsequent dispatches for $J_j$. Preliminary experiments favoured dispatching in the first (earliest) slot,\footnote{Preliminary experiments of 500 \JSP\ instances where inspected: First slot chosen could always achieve its known optimum by implementing the pseudo code in \cref{fig:pseudocode}, however only $97\%$ of the instances when choosing the smallest slot.} and henceforth will be used throughout the dissertation.

\begin{comment} % virðist að þetta vandamál sé leyst, sbr. "scriptCompareSlotChoice.m"
Preliminary experiments which at each time step all jobs on the ready-list are explored and dispatched according to a (fixed) construction heuristic. The job corresponding to the best resulting makespan (found via analytical methods) is chosen to have the highest priority. The resulting makespan is divided by its theoretical optimal makespan, i.e., the deviation from optimality defined by \cref{eq:ratio}. A histogram for 6-job 5-machine \jsp\ problem instances ($N=500$) is depicted in \cref{fig:slot:smallestvsfirst} using both the intrinsic heuristics: (\subref{fig:slot:first}) first slot chosen and (\subref{fig:slot:small}) slot corresponding to the smallest slot size chosen. Using always the first slot, roughly 26\% of the instances were able to achieve the optimum makespan, however mere 16\% using the smallest slot size. Hence dispatching in the first slot was favoured and will be used throughout the study.
\begin{figure}
\subfloat[First slot]{\includegraphics[width=\textwidth]{slotsize_first.eps}} %\label{fig:slot:first}}
\subfloat[Smallest slot size]{\includegraphics[width=\textwidth]{slotsize_small.eps}} %\label{fig:slot:small}}
\caption{Histogram of deviation from optimality by sequentially dispatching optimal jobs using a fixed construction heuristic.}
\label{fig:slot:smallestvsfirst}
\end{figure}
\end{comment}

Note that the choice of slot is an intrinsic heuristic within the construction heuristic. As has been previously mentioned, construction heuristics are designed in such a manner that they limit the search space. Preferably without excluding the true optimum. The focus of this dissertation, however, is on learning the priority of the jobs on the ready-list, for a fixed construction heuristic. Hence there are some problem instances in which the optimum makespan cannot be achieved due to the limitations of the schedule's construction heuristic of not being properly able to differentiate between which slot from $\tilde{S}_{ja}$ is the most effective. Instead, hopefully, the learning algorithm will be able to spot these problematic situations, should they arise, by inspecting the schedule's features and translate that into the jobs' priorities.

\section{Example}\label{sec:jsp:example}
Let's define a six-job and five-machine \jsp\ problem, with the following $\vec{p}\sim\mathcal{U}(1,99)$ and $\vsigma$ matrices,
\begin{eqnarray}
\vec{p}=
\left[\begin{array}{rrrrr}
 \tcr{91} & \tcr{53} & \textbf{31} & 59 & 84 \\
 \tcr{15} & \textbf{22} & 23 & 13 & 92 \\
 \tcr{54} & \tcr{33} & \tcr{15} & \tcr{62} & \tcr{83} \\
 \tcr{83} & \tcr{51} & \tcr{80} & \tcr{97} & \textbf{40} \\
 \tcr{51} & \tcr{27} & \textbf{74} & 85 & 70 \\
 \tcr{59} & \textbf{69} & 66 & 46 & 20 
\end{array}\right], 
\quad \label{eq:psigma}
\vsigma=
\left[\begin{array}{r}
\vsigma_1 \\ \vsigma_2  \\ \vsigma_3 \\ \vsigma_4 \\ \vsigma_5 \\ \vsigma_6
\end{array}\right] =
\left[\begin{array}{rrrrr} 
 \tcr{4} & \tcr{5} & \textbf{3} & 2 & 1 \\
 \tcr{1} & \textbf{3} & 2 & 4 & 5 \\
 \tcr{3} & \tcr{1} & \tcr{2} & \tcr{4} & \tcr{5} \\
 \tcr{2} & \tcr{3} & \tcr{5} & \tcr{1} & \textbf{4} \\
 \tcr{2} & \tcr{5} & \textbf{4} & 3 & 1 \\
 \tcr{2} & \textbf{3} & 5 & 1 & 4 
\end{array}\right].
\end{eqnarray}
Now assume 15 operations have already dispatched been made, i.e., the red entries, by using the following sequence of jobs,
\begin{eqnarray}
\vchi=\left[J_3,J_3,J_3,J_3,J_4,J_4,J_5,J_1,J_1,J_2,J_4,J_6,J_4,J_5,J_3\right]
\end{eqnarray}
hence the ready-list is $\mathcal{R}=\{J_1,J_2,J_4,J_5,J_6\}$ (note that $J_3$ has traversed through all of its machines) indicating the 5 potential jobs to be dispatched at step $k=16$, denoted in bold. \Cref{fig:example} illustrates the temporal partial schedule of the dispatching process.
Numbers in the boxes represent the job identification $j$. The width of the box illustrates the processing times for a given job for a particular machine $M_a$ (on the vertical axis). The dashed boxes represent the resulting partial schedule for when a particular job is scheduled next. Moreover, the current $C_{\max}$ is denoted with a dotted line.

If the job with the shortest processing time were to be scheduled next, i.e., implementing the SPT heuristic, then $J_2$ would be dispatched. Similarly, for the LPT (largest processing time) heuristic then $J_5$ would be dispatched. 
Other DRs use features not directly observable from looking at the current partial schedule (but easy to keep record of), for example by assigning jobs with most or least total processing time remaining, i.e., MWR and LWR heuristics, who would yield $J_5$ and $J_4$, respectively.

To summarise, in order to create a schedule for \JSP, a construction heuristic is chosen with some DR to determine the priority of the jobs on the ready-list, $\mathcal{R}$. \Cref{fig:pseudocode} outlines the pseudo code for the dispatching process of a \JSP\ problem instance.



\begin{figure}[p]
\includegraphics[width=\textwidth]{jssp_example}
\caption[Gantt chart of a partial \JSP\ schedule]{Gantt chart of a partial \JSP\ schedule after 15 dispatches: Solid and dashed boxes represent $\vchi$ and $\mathcal{R}^{(16)}$, respectively. Current $C_{\max}$ denoted as dotted line.}
\label{fig:example}
\end{figure}

\begin{figure}[p]\centering 
\noindent{\footnotesize\begin{tabbing}
\quad \quad \= 0\;\; \= \emph{Initialization}: Let $\vchi=\emptyset$ denote \\
\>  \> the current dispatching sequence. \\
\>1 \> {\bf for} \= $k := 1$ to $\ell=n\cdot m$ {\bf do} \emph{(at each dispatch iteration)}\\ 
\>2 \>\> {\bf for} \= $J_j\in \mathcal{R}^{(k)}\subset \mathcal{J}$ {\bf do} \emph{(inspect ready-list)} \\
\>3 \>\>\>  $I_j^{DR}\leftarrow \text{DR}\left([\vchi,j],Y\right)$ \emph{(priority $J_j$)}\\
\>4 \>\> {\bf od} \\
\>5 \>\> $j^*\leftarrow \argmax_{j\in \mathcal{R}^{(k)}}\{I_j^{DR}\}$ \emph{(choose highest priority)} \\
\>6 \>\> $\chi_k \leftarrow j^*$ \emph{(dispatch $j^*$)}\\
\>7 \> {\bf od} \\
\>8 \> $C_{max} \leftarrow Y(\vchi)$ \emph{(makespan)}
\end{tabbing}}
\caption{Pseudo code for constructing a \JSP\ sequence using a dispatching rule (DR) for a fixed construction heuristic (CH).}\label{fig:pseudocode}
\end{figure}


Henceforth, a \emph{sequence} will refer to the sequential ordering of the dispatches of tasks to machines, i.e., $(j,a)$; the collective set of allocated tasks to machines, which is interpreted by its sequence, is referred to as a \emph{schedule}; a \emph{scheduling policy} will pertain to the manner in which the sequence is manufactured for an (near) optimal schedule: be it a SDR such as MWR or via evolutionary search, etc. 

\section{Single-based priority dispatching rules}\label{sec:SDR}
\emph{Dispatching rules} (DR) are of a construction heuristics, where one starts with an empty schedule and adds sequentially on one operation (or task) at a time. Namely, at each time step $k$, an operation is dispatched which has the highest priority of the ready-list, \mbox{$\mathcal{R}^{(k)}\subset\mathcal{J}$}, i.e., the jobs who still have operations unassigned. If there is a tie, some other priority measure is used. 

A \emph{single-based priority dispatching rule}(SDR), or simple priority dispatching rule, is a function of features of the jobs and/or machines of the schedule. The features can be constant or vary throughout the scheduling process. For instance, the priority may depend on job processing attributes, such as which job has, 
\begin{description}
\item[Shortest immediate processing time (SPT)] \hfill \\
greedy approach to finish shortest tasks first,  
\item[Longest immediate processing time (LPT)] \hfill \\
greedy approach to finish largest tasks first, 
\item[Least work remaining (LWR)] \hfill \\
whose intention is to complete jobs advanced in their pro\-gress, i.e., minimising the ready-list $\mathcal{R}$,
\item[Most work remaining (MWR)] \hfill \\
whose intention is to accelerate the processing of jobs that require a great deal of work, yielding a balanced progress for all jobs during dispatching, however in-process inventory can be high.
\end{description}
These rules are the ones most commonly applied in the literature due to their simplicity and effectiveness, %\citep[cf.][]{Haupt89,Panwalkar77}
therefore they will be referenced throughout the dissertation. 
However there are many more available, e.g., randomly selecting an operation with equal possibility (RND); minimum slack time (MST); smallest slack per operation (S/OP); and using the aforementioned dispatching rules with predetermined weights. A survey of more than 100 of such rules are presented in \citet{Panwalkar77}, however the reader is referred to an in-depth survey for SDRs by \citet{Haupt89}. 

%Haupt89:
%Among the rules with job processing information, SPT is the most known, the most applied, and yet one of the most efficient rules. In line with LPT, it requires the lowest information amount, since only operation data (not job data) from the local queue (not from other queues) are needed.
%LWR give preference to jobs the work completed of which is rather advanced. Thus, they can be regarded as value-oriented rules selecting jobs with a high fraction of their value added or cumulative value to their total value.
% The intent of MWR is to speed up jobs with large processing work resulting in a well-balanced work progress of all jobs, at the expense of a high volume of in-process inventory, while LWR tend to reduce the number of jobs in the shop.

To summarise, SDRs assign an index to each job of the ready-list waiting to be scheduled, and are generally only based on few features and simple mathematical operations. 

%Haupt89:
%Among the rules with job processing information, SPT is the most known, the most applied, and yet one of the most efficient rules. In line with LPT, it requires the lowest information amount, since only operation data (not job data) from the local queue (not from other queues) are needed.
%LWR give preference to jobs the work completed of which is rather advanced. Thus, they can be regarded as value-oriented rules selecting jobs with a high fraction of their value added or cumulative value to their total value.
% The intent of MWR is to speed up jobs with large processing work resulting in a well-balanced work progress of all jobs, at the expense of a high volume of in-process inventory, while LWR tend to reduce the number of jobs in the shop.

\section{Features for \jsp}\label{sec:features}
A DR may need to perform a one-step look-ahead and observes features of the partial schedule to make a decision, for example by observing the resulting temporal makespan. These emanated observed features are sometimes referred to as an \emph{after-state} or \emph{post-decision state}. 

Features are used to grasp the essence of the current state of the schedule. Temporal scheduling features applied in this dissertation for a job $J_j$ to be dispatched on machine $M_a$ are given in \cref{tbl:features}. 
Note, from a job-oriented viewpoint, for a job already dispatched $J_j\in\mathcal{J}$ the corresponding set of machines now processed is $\mathcal{M}_j\subset\mathcal{M}$. Similarly from the machine-oriented viewpoint, $M_a\in\mathcal{M}$ with corresponding $\mathcal{J}_a\subset\mathcal{J}$. 

The features of particular interest were obtained from inspecting the aforementioned SDRs from \cref{sec:SDR}:  
\phiJobRelated\ and \phiMacRelated\ are job-related and machine-related attributes of the current schedule, respectively. 

Some features are directly observed from the partial schedule, such as the job- and machine-related features. 
In addition there are flow-related, \phiFlowRelated, which measure the influence of idle time on the schedule, 
and current makespan-related, \phiScheduleRelated.

Note that \phiLocalRelated\ are only based on the current step of the schedule, i.e., schedule's \emph{local features}, and might not give an accurate indication of how it will effect the schedule in the long run. Therefore, a set of features are needed to estimate the schedule's overall performance, referred to as its \emph{global features}. The approach here is to use well known SDRs, \phiSDRRelated, as a benchmark by retrieving what would the resulting $C_{\max}$ would be given if that SDR would be implemented from that point forward. Moreover, random completion of the partial schedule are implemented, here \phiRND\ corresponds to 100 random rollouts, which can be used to identify which features $\vphi$ are promising on a long-term basis.  

All of the features vary throughout the scheduling process, w.r.t. operation belonging to the same time step $k$, save for \phitotalProc\ which varies between jobs; \phistep\ to keep track of features' evolution w.r.t. the scheduling process; and \phiwrmTotal\ which is static for a given problem instance, but used for normalising other features, such as work-remaining based (\phiwrmJob\ and \phiwrmMac) or makespan-based (\phiGlobalRelated) ones.
In addition, \phimac, is reported in order to distinguish which features are in conflict with each other.

\begin{table} \centering 
  \caption[Feature space $\mathcal{F}$ for \JSP]{Feature space $\mathcal{F}$ for \JSP\ where job $J_j$ on machine $M_a$ given the resulting temporal schedule after dispatching $(j,a)$.}
  \label{tbl:features}
  \input{tables/features-description}
\end{table}



\section{Composite dispatching rules}\label{sec:CDR}
\citet{Jayamohan04} showed that a careful combination of dispatching rules can perform significantly better. These are referred to as \emph{composite dispatching rules} (CDR), where the priority ranking is an expression of several SDRs. For instance, optimising \JSP\ w.r.t. $L_{max}$ for one machine\ \cite[see. chapter 14.2]{Pinedo08}, one can combine SDRs that are optimal for a different criteria of problem instances, which complement each other as a CDR, e.g., combining the SDRs 
WSPT (SPT weighted w.r.t. $\mathcal{J}$), ``which is optimal when all release dates and due dates are zero,'' 
and minimum slack first (MS), ``which is optimal when all due dates are sufficiently loose and spread out,'' 
one gets the CDR apparent tardiness cost (ATC) which can work well on a broader set of problem instances than the original SDRs by themselves.

CDRs can deal with greater number of features and more complicated form, in short, CDR are a combination of several SDRs. For instance let CDR be comprised of $d$ SDRs, then the index $I$ for job $J_j$ using CDR is, 
\begin{equation}
I_j^{CDR} = \sum_{i=1}^d w_i \cdot \text{DR}^i(\vphi_j)
\end{equation}
where $w_i>0$ and $\sum_{i=0}^d w_i = 1$, then $w_i$ gives the \emph{weight} of the influence of $\text{DR}^i$ (which could be SDR or another CDR) to CDR. Note, each $\text{DR}^i$ is a function of the job $J_j$'s feature state $\vphi_j$.

\subsection{Blended dispatching rules}
Since each DR yield a priority index $I^{DR}$ then it is easy to translate its index as a  performance measure $a$, i.e., $a:~I^{DR}\mapsto \mathcal{Y}$. Then it is possible to combine several performance measures into a single DR, these are referred to as \emph{blended dispatching rules} (BDR), where an overall blended priority index $P$ is defined as 
\begin{equation}
P_j = \sum_{i=1}^d w_i \cdot a_i 
\end{equation}
where $w_i>0$ and $\sum_{i=0}^d w_i = 1$, then $w_i$ gives the weight of the proportional influence of performance measure $a_i$ (based on some SDR or CDR) to the overall priority. 

Generally the weights $\vec{w}$ chosen by the algorithm designer a priori. 
A more sophisticated approach would to learn have the algorithm discover these weights autonomously, for instance via evolutionary search or ordinal regression, to be discussed in \cref{ch:esmodels} and \cref{ch:prefmodels}, respectively.

\subsection{Automated discovery of dispatching rules}
\citet{Monch13} stress the importance of automated discovery of DRs and named several of successful implementations in the field of semiconductor wafer fabrication facilities, however this sort of investigation is still in its infancy and subject for future research.

A recent editorial of the state-of-the-art approaches in advanced dispatching rules for large-scale manufacturing systems by \citet{Chen13} points out that:
\begin{quote}
[..] most traditional dispatching rules are based on historical data. With the emergence of data mining and on-line analytic processing, dispatching rules can now take predictive information into account.
\end{quote}
implying that there has not been much automation in the process of discovering new dispatching rules, which is the ultimate goal of this dissertation, i.e., automate creating optimisation heuristics for scheduling. 

With meta heuristics one can use existing DRs and use for example portfolio-based algorithm selection \citep{Rice76,Gomes01}, either based on a single instance or class of instances \citep{Xu07} to determine which DR to choose from. 
Instead of optimising which algorithm to use under what data distributions, such as the case of portfolio algorithms, the approach taken in this dissertation is more similar to that of \emph{meta learning} \citep{Vilalta02} which is the study of how learning algorithms can be improved, i.e., exploiting their strengths and remedy their failings, in order for a better algorithm design. Thus creating an adaptable learning algorithm that dynamically finds the appropriate dispatching rule  to the data distribution at hand. 

\citet{Kalyanakrishnan11} point out that meta learning can be very fruitful in reinforcement learning, and in their experiments they discovered some key discriminants between competing algorithms for their particular problem instances, which provided them with a hybrid algorithm which combines the strengths of the algorithms.

\citet{Nguyen13} proposed a novel iterative dispatching rules (IDR) for \JSP\ which learns from completed schedules in order to iteratively improve new ones. At each dispatching step, the method can utilise the current feature space to \emph{correctify} some possible \emph{bad} dispatch made previously (sort of reverse lookahead).
Their method is straightforward, and thus easy to implement and more importantly, computationally inexpensive, although \citeauthor{Nguyen13} stress that there still remains room for improvement. 

\citet{Korytkowski13} implement ant colony optimisation to select the best DR from a selection of nine DRs for \JSP\ and their experiments showed that the choice of DR do affect the results and that for all performance measures considered it was better to have all of the DRs to choose from rather than just a single DR at a time. 
Similarly, \citet{Lu13} investigate eleven SDRs for \JSP\ to create a pool of thirty three CDRs that strongly outperformed the ones they were based on, which is intuitive since where one SDR might be failing, another could be excelling, hence combining them should yield a better CDR. \citeauthor{Lu13} create their CDRs with multi-contextual functions (MCF) based either on machine idle time or job waiting time, so one can say that the CDRs are a combination of those two key features of the schedule and then the basic DRs. However, there are no combinations of the basic DR explored, only machine idle time and job waiting time.  
\citet{Yu13} used priority rules to combine twelve existing DRs from the literature, in their approach they had forty eight priority rules combinations, yielding forty eight different models to implement and test. This is a fairly ad hoc solution and there is no guarantee the optimal combination of DRs is found. 

\clearpage
\section{Rice's framework for \jsp}\label{sec:rice:jsp}
\citeauthor{Rice76}'s framework for algorithm selection (discussed in \cref{sec:rice}) has already been formulated for \jsp\ \citep[cf.][]{SmithMilesLion3,SmithMilesLion5,InRu12}, as follows, 
\begin{description} 
\item[Problem space] $\mathcal{P}$ is defined as the union of $N$ problem instances consisting of processing time and ordering matrices,
\begin{equation} 
\mathcal{P}=\left\{(p_{ja}^{(i)},\vsigma_j^{(i)})\;\big|\;J_j\in\mathcal{J},\;M_a\in\mathcal{M}\right\}_{i=1}^{N}
\end{equation}
Problem generators for $\mathcal{P}$ are given in \cref{ch:genprobleminstances}.
\item[Feature space] $\mathcal{F}$ which is outlined in \cref{sec:features}. Note, these are not the only possible set of features, however, the local feature, \phiLocalRelated, are built on the work by \cite{InRu11a,SmithMilesLion3} and deemed successful in capturing the essence of a \jsp\ data structure;
\item[Algorithm space] $\mathcal{A}$ is simply the scheduling policies under consideration, e.g., SDRs from \cref{sec:SDR},
\begin{equation}
\mathcal{A}=\left\{\text{SPT,~LPT,~LWR,~MWR}\dotsc\right\}
\end{equation} 
\item[Performance space] $\mathcal{Y}$ is based on the resulting $C_{\max}$, defined by \cref{eq:makespan}. The optimum makespan is denoted $C_{\max}^{\text{opt}}$, and the makespan obtained from the scheduling policy $A\in\mathcal{A}$ under inspection by $C_{\max}^{A}$. Since the optimal makespan varies between problem instances the performance measure is the following, 
\begin{equation}\label{eq:rho}
\rho=\frac{C_{\max}^{A}-C_{\max}^{opt}}{C_{\max}^{\text{opt}}}\cdot 100\%
\end{equation}
which indicates the \namerho. Thus $\mathcal{Y}$ is the following, 
\begin{equation}
\mathcal{Y}=\left\{\rho_i\right\}_{i=1}^{N}
\end{equation}
\end{description}
The mapping $Y:\;\mathcal{A}\times\mathcal{F} \mapsto \mathcal{Y}$ is the step-by-step construction heuristic introduced in \cref{sec:CH}.