\HeaderQuote{Read the directions and directly you will be directed in the right direction.}{Doorknob} 

\chapter{\Jsp\ Scheduling Problem}\label{ch:scheduling}
\FirstSentence{S}{cheduling problems}, which occur frequently in practice, are 
a category within combinatorial optimisation problems. 
A subclass of scheduling problems is \jsp\ (\JSP), which 
is widely studied in operations research. 
\JSP\ deals with the allocation of tasks of competing resources where its goal 
is to optimise a single or multiple objectives. 
\Jsp's analogy is from manufacturing industry where a set of jobs are broken down into tasks that must be processed on several machines in a workshop. 
Furthermore, its formulation can be applied on a wide variety of practical 
problems in real-life applications which involve decision making. Therefore, 
its problem-solving capabilities has a high impact on many manufacturing 
organisations. % Gefa d√¶mi?

Deterministic \JSP\ is the most \emph{general} case for classical scheduling problems \citep{Jain99}. 
Many other scheduling problems can be reformulated as \JSP. 
For instance, the \textit{travelling salesman problem}\footnote{
    The travelling salesman problem (TSP) was formulated in the 1800s by the 
    mathematicians W.R. Hamilton and Thomas Kirkman \citep{graphtheory}. 
    The salesman has to visit a set of cities exactly once (i.e. Hamiltonian 
    path), with the objective of minimising the route, in terms of distance, 
    between them.}
 can be contrived as \JSP
\begin{enumerate*}[label={{}}]
    \item with the salesman as a single machine in use
    \item the cities to be visited are the jobs to be processed
    \item distance is sequence dependent set-up time
\end{enumerate*}
The general form of~~\JSP\ assumes that each job can have its own 
distinctive flow pattern through the machines, which is independent of the 
other jobs. 
In the case where all jobs share the same permutation route, \jsp\ is reduced 
to a \fsp\ scheduling problem (\FSP) \citep{Guinet1998,Tay08}. 
Therefore, without loss of generality, this dissertation is structured around 
\JSP. 

\emph{Remark:} Throughout the dissertation the \FSP\ variation will \emph{not} 
be a commonly used permutation \fsp\ (P\FSP) from the literature, which has 
the added constraints of not allowing any jobs to pass one another.
Here, the jobs have to be processed in the same machine order. However, 
machines do not necessarily need to process jobs in the same order, as is 
implied in P\FSP.

\section{Mathematical formulation}
\Jsp\ considered for this dissertation is when $n$ jobs, 
$\mathcal{J}=\{J_j\}_{j=1}^n$, are scheduled on a finite set, 
$\mathcal{M}=\{M_a\}_{a=1}^m$, of $m$ machines, subject to the constraint that 
each job $J_j$ must follow a predefined machine order (a chain of 
$m$ operations, $\vsigma_j=[\sigma_{j1},\sigma_{j2},\dotsc,\sigma_{jm}]$) and 
that a machine can handle at most one job at a time. 
The objective is to schedule jobs in such a manner as to minimise the maximum 
completion times for all tasks, which is also known as the makespan, 
$C_{\max}$. 

A common notation for scheduling problems \citep[cf. Chapter 2 in ][]{Pinedo08} 
is given by a triplet $\alpha|\beta|\gamma$, where 
\begin{enumerate*}[itemjoin*={, and finally}, label={{}}]
  \item $\alpha$ describes the machine environment
  \item $\beta$ details any additional processing characteristics and/or 
  constraints
  \item $\gamma$ lists the problem's objective
\end{enumerate*}
Hence our family of scheduling problems, i.e., a $m$ machine \JSP\ and \FSP\ 
w.r.t. minimising makespan, is $Jm||C_{\max}$ and $Fm||C_{\max}$, respectively. 
An additional constraint commonly considered are job release-dates and 
due-dates, and then the objective is generally minimising the maximum lateness, 
denoted $Jm|r_j,d_j|L_{\max}$. 
However, those shop-requirements will not be considered here. 

Henceforth, the index $j$ refers to a job $J_j\in\mathcal{J}$, while the index 
$a$ refers to a machine $M_a\in\mathcal{M}$. If a job requires a number of 
processing steps or operations, then the pair $(j,a)$ refers to the operation, 
i.e., processing the task of job $J_j$ on machine $M_a$. Moreover, index $k$ 
will denote the time step of the operation. Note that once an operation is 
started, it must be completed uninterrupted, i.e., pre-emption is not allowed. 
Moreover, there are no sequence dependent set-up times.

For any given \JSP, then each job $J_j$ has an indivisible processing time (or 
cost) on machine $M_a$, $p_{ja}$, which is assumed to be integral and finite. 

Starting time of job $J_j$ on machine $M_a$ is denoted $x_s(j,a)$ and its 
completion or end time is denoted $x_e(j,a)$ where, 
\begin{equation}  x_e(j,a):=x_s(j,a)+p_{ja} \end{equation} 

\clearpage
Each job $J_j$ has a specified processing order through the machines, it is a 
permutation vector, $\vsigma_j$, of $\{1,..,m\}$, representing a job $J_j$ can 
be processed on $M_{\vsigma_j(a)}$ only after it has been completely processed 
on $M_{\vsigma_j(a-1)}$, i.e.,
\begin{equation}\label[ineq]{eq:permutation}
	x_s(j,\vsigma_j(a)) \geq x_e(j,\vsigma_j(a-1)) 
\end{equation}
for all $J_j\in\mathcal{J}$ and $a\in\{2,..,m\}$. 
Note, that each job can have its own distinctive flow pattern through the 
machines, which is independent of the other jobs. However, in the case that all 
jobs share the same permutation route, \JSP\ is reduced to a \FSP.

The disjunctive condition that each machine can handle at most one job at a time is the following,
\begin{equation}\label[ineq]{eq:oneJobPerMac}
	x_s(j,a) \geq x_e(j',a) \quad\textrm{or}\quad x_s(j',a) \geq x_e(j,a)  
\end{equation}
for all $J_j,J_{j'}\in\mathcal{J},\; J_j\neq J_{j'}$ and $M_a\in\mathcal{M}$. 

The objective function is to minimise its maximum completion times for all tasks, commonly referred to as the makespan, $C_{\max}$, which is defined as follows,
\begin{equation}
	C_{\max} := 
	\max\condset{x_e(j,\vsigma_j(m))}{J_j\in\mathcal{J}}.\label{eq:makespan}
\end{equation} 
Clearly, w.r.t. minimum makespan, it is preferred that schedules are non-delay, 
i.e., the machines are not kept idle. The time in which machine $M_a$ is idle 
between consecutive jobs $J_j$ and $J_{j'}$ is called idle time, or slack,
\begin{equation} 
  s(a,j):=x_s(j,a)-x_e(j',a) \label{eq:slack}
\end{equation}
where $J_j$ is the immediate successor of $J_{j'}$ on $M_a$. Although this is 
not a variable directly needed to construct a schedule for \JSP, it is a key 
attribute in order to measure the quality of the schedule. 

\section{Construction heuristics}\label{sec:CH}
Construction heuristics are designed in such a way that it limits the search 
space in a logical manner. Preferably without excluding the true optimum. 
Here, the construction heuristic, $\Upsilon$, is to schedule the dispatches as 
closely together as possible, i.e., minimise the schedule's idle times. 
More specifically, once an operation $(j,a)$ has been chosen from the job-list, 
$\mathcal{L}$, by some dispatching rule, it can placed immediately after (but 
not prior) $x_e(j,\vsigma_j(a-1))$ on machine $M_a$ due to 
\cref{eq:permutation}.
However, to guarantee that \cref{eq:oneJobPerMac} is not violated, 
idle times $M_a$ are inspected, as they create a slot which in $J_j$ can 
occupy. Bearing in mind that $J_j$ release time is $x_e(j,\vsigma_j(a-1))$ one 
cannot implement \cref{eq:slack} directly, instead it has to be updated as 
follows,
\begin{equation}
	\tilde{s}(a,j') := x_s(j'',a)-\max\{x_e(j',a),\;x_e(j,\vsigma_j(a-1))\} % 
	%\textrm{ if } x_e(j,\vsigma_j(a-1)\geq x_e(j',a) 
\end{equation}
for all already dispatched jobs $J_{j'},J_{j''}\in \mathcal{J}_a$ where $J_{j''}$ is $J_{j'}$ successor on $M_a$. Since pre-emption is not allowed, the only applicable slots are whose idle time can process the entire operation, i.e.,
\begin{equation}
	\tilde{S}_{ja} := \condset{J_{j'}\in \mathcal{J}_a}{\tilde{s}(a,j')\geq 
	p_{ja} }\label{eq:slots}.
\end{equation} 
There are several heuristic methods for selecting a slot from 
\cref{eq:slots}, e.g., if the main concern were to utilise the slot space, then 
choosing the slot with the smallest idle time would yield a closer-fitted 
schedule and leaving greater idle times undiminished for subsequent dispatches 
on $M_a$. However, dispatching $J_j$ in the first slot would result in its 
earliest possible release time, which would be beneficial for subsequent 
dispatches for $J_j$. Experiments favoured dispatching in the 
earliest slot,\footnote{Preliminary experiments of 500 \JSP\ instances where 
    inspected: First slot chosen could always achieve its known optimum by 
    implementing \cref{pseudo:constructJSP}, however, only $97\%$ of
    instances when choosing the smallest slot.} 
thus used throughout.

Note that the choice of slot is an intrinsic heuristic within $\Upsilon$.
The focus of this dissertation, however, is on learning the priority of the 
jobs on the job-list, for a fixed construction heuristic. 
Hence, there could be some problem instances in which the optimum makespan 
cannot be achieved, due to the limitations of $\Upsilon$ of not being properly 
able to differentiate between which slot from \cref{eq:slots} is the most 
effective.
Instead, hopefully, the learning algorithm will be able to spot these 
problematic situations, should they arise, by inspecting the schedule's 
features and translate that into the jobs' priorities.

\emph{Dispatching rules} (DR) are an integral part of a construction 
heuristics, as it determines the priorities of the job-list, i.e., 
the jobs who still have operations unassigned. 
Starting with an empty schedule, and sequentially adding one operation (or 
task) at a time. Then, for each time step $k$, an operation is dispatched which 
has the highest priority of the 
job-list, \mbox{$\mathcal{L}^{(k)}\subset\mathcal{J}$}.
If there is a tie, some other priority measure is used. 
However, let's assume that ties are broken randomly. 
\Cref{pseudo:constructJSP} outlines the pseudo code for the entire dispatching 
process of a \JSP\ problem instance.

\input{pseudocode/constructJSP.tex}

Henceforth, we will adopt the following terminology: a \emph{sequence} will 
refer to the sequential ordering of the dispatches\footnote{Note, 
only a sequence of $J_j$ is needed, since the corresponding $M_a$ can be 
    obtained by reading $\vsigma$.} 
of tasks to machines, namely, 
\begin{equation}
  \vchi = \big\{\chi_k\big\}_{k=1}^K
        = \condset{(j,a)}{J_j\in\mathcal{L}^{(k)}}_{k=1}^K
\end{equation}
The collective set of allocated tasks to machines, which is interpreted by its 
sequence, is referred to as a \emph{schedule};
and a \emph{scheduling policy} (or \dr) $\pi$ will pertain to the manner in 
which the sequence is manufactured: be it a 
SDR such as SPT or some other heuristic. 
Sequence and schedule are often used interchangeably, as they are closely 
related. A complete schedule is also known as $K$-solution\footnote{A partial 
schedule, at step $k$, is called $k$-solution.} \citep{Bertsekas97}.

\section{Example}\label{sec:jsp:example}
There are many examples of \jsp\ for real-world application. 
For demonstration purposes, let's examine a hypothetical problem from 
the 18th century. Assume we are invited to the Mad Hatter's Tea Party in 
Wonderland, illustrated in \cref{fig:teaparty}. 
There are four guests attending
\begin{enumerate*}[label={$J_\arabic*$)}, ref={{$J_\arabic*$}}, 
    itemjoin*={{, and of course our host }}]
    \item Alice\label{guest:alice}
    \item March Hare\label{guest:marchhare}
    \item Dormouse\label{guest:dormouse}
    \item Mad Hatter\label{guest:madhatter}
\end{enumerate*}
During these festivities, there are several things each member of the party 
has to perform. They all have to
\begin{enumerate*}[label={$M_\arabic*$)}]
    \item have wine or pour tea
    \item spread butter
    \item get a haircut
    \item check the time of the broken watch for themselves
    \item say what they mean, e.g., asking a riddle  or reciting a poem to the 
    group
\end{enumerate*}
The guests are very particular creatures, and would like to do these task in a 
very specific order, e.g., March Hare insists on doing 
them alphabetically. Each would rather wait than breaking their habit.
They tend to be absent-minded, so each task takes them a different amount of 
time. 
Let's assume their processing times and ordering are given in 
\cref{tbl:example}. 
\clearpage

\begin{figure}[b!]\centering 
    \includegraphics[width=0.8\textwidth]{alice-mad-tea-party}
    \caption{The Mad Hatter's Tea Party, from Alice's Adventures in Wonderland 
        by \citet{alice}. Illustration by John Tenniel 
        (1820-1914).}\label{fig:teaparty}
\end{figure}

\input{tables/example}

Unfortunately, Alice can't stay long. She must leave as soon as possible to 
play croquet with the Red Queen, 
and she mustn't be late for that very important date. Otherwise, it's off with 
someone's head! However, Alice, had a proper upbringing and won't leave the 
table until everyone has finished their tasks. 
How should the guests go about their tea-party, in order for Alice to be 
on-time?

The problem faced by Alice and her new friends is in what order should they 
rotate their tasks between themselves so that they all finish as soon as 
possible? This can be considered as is a typical four-job and five-machine 
\jsp, where
\begin{enumerate*}[label={{}}]
    \item our guests are the jobs
    \item their tasks are the machines
    \item our objective is to minimise $C_{\max}$, i.e., when Alice can 
    leave
\end{enumerate*}

Let's assume we've come to the party, after 10 operations have already been 
made (i.e. strikeout entries in \cref{tbl:example}), by using the following 
job sequence,\footnote{In fact this is the sequence resulting from 10 
dispatches following the SPT-rule, to be defined shortly.}
\begin{equation}
  \vchi = \left\{ \chi_i \right\}_{i=1}^{k-1} \label{eq:vchi:example}
        = \left\{ J_4,J_2,J_3,J_3,J_1,J_1,J_1,J_1,J_1,J_4 \right\}
\end{equation}
hence currently, at step $k=11$, the job-list is 
$\mathcal{L}^{(k)}=\{J_2,J_3,J_4\}$ indicating the 3 potential\footnote{
  Alice is quite anxious to leave, so she has already completed everything, and 
  therefore \ref{guest:alice}$\notin\mathcal{L}^{(11)}$.} 
jobs (i.e. denoted in bold in \cref{tbl:example}) to be dispatched, i.e., 
$\chi_k\in\mathcal{L}^{(k)}$.

\input{figures/example.graph.tex}

This is a very compact form for the current partial solution, it's easiest to 
comprehend it via disjunctive graph \citep{roy64} to model the work-flow of 
tasks to be scheduled.
Let's encode
\begin{enumerate*}
    \item the operations as vertices
    \item horizontally aligning them w.r.t. each job $J_j$
    \item connect vertices with directed edges according the 
        \cref{eq:permutation}
    \item by introducing dummy vertices before and after, then the goal is to 
    visit each vertex exactly once, or \emph{Hamiltonian} path: starting at the 
    `source'  (i.e. empty schedule), and finishing at the sink (i.e. complete 
    schedule)
\end{enumerate*}
The path gives the prescription of the order in which the jobs rotate 
between machines. 
\Cref{fig:example:graph} depicts the path generation at the beginning, midway, 
and final stages for our Tea Party example. 
Gray vertices are operations that haven't yet been dispatched, and the pink 
vertices are the ones that correspond to $\vchi$, 
and the pink directed edges indicate the current partial Hamiltonian path.
Note, only \cref{fig:example:graph:midway} corresponds to 
\cref{eq:vchi:example}.

\begin{figure}\centering
    \includegraphics[width=0.9\textwidth]{{example.gantt}.pdf}
    \caption[Gantt chart of a partial \JSP\ schedule]{Gantt chart of a partial 
        \JSP\ schedule after 10 dispatches: Solid and dashed boxes represent 
        $\vchi$ and $\mathcal{L}^{(11)}$, respectively. Current $C_{\max}$ 
        denoted 
        as dotted line.}
    \label{fig:example:midway}
\end{figure}

However, this hasn't told us when each guest should start their task, 
i.e., the project schedule. For that, \cref{fig:example:midway} illustrates the 
temporal partial schedule (or $k$-solution) of \cref{eq:vchi:example} as a 
Gantt-chart
\begin{enumerate*}
    \item numbers in the boxes represent the job identification $j$
    \item the width of the box illustrates the processing times for a given job 
    for a particular machine $M_a$ (on the vertical axis)
    \item the dashed boxes represent the resulting $(k+1)$-solution for when a 
    particular job is scheduled next 
    \item the current $C_{\max}$ is denoted with a dotted line    
\end{enumerate*}
Note, the disjunctive graph from \cref{fig:example:graph:midway} gives the 
schedule in \cref{fig:example:midway}.

If the job with the shortest processing time were to be scheduled next, i.e., 
implementing the SPT-rule, then $J_4$ would be dispatched. Similarly, for 
the LPT (largest processing time) then $J_2$ would be dispatched. 
Other DRs use features not directly observable from looking at the $k$-solution 
(but easy to keep record of), for example by assigning jobs 
with most or least total processing time remaining, i.e., MWR and LWR 
heuristics, who would yield $J_2$ and $J_4$, respectively.

\section{Single priority based \dr s}\label{sec:SDR}

A \emph{\sdr} (SDR) is a function of attributes, or features, of the jobs 
and/or machines of the schedule. The features can be constant or vary 
throughout the scheduling process. 
For instance, priority may depend on job processing attributes, such as 
which job has, 
\begin{description}
  \item[Shortest immediate processing time (SPT)] \hfill \\
  greedy approach to finish shortest tasks first,  
  \item[Longest immediate processing time (LPT)] \hfill \\
  greedy approach to finish largest tasks first, 
  \item[Least work remaining (LWR)] \hfill \\
  whose intention is to complete jobs advanced in their pro\-gress, i.e., 
  minimising $\mathcal{L}$,
  \item[Most work remaining (MWR)] \hfill \\
  whose intention is to accelerate the processing of jobs that require a 
  great deal of work, yielding a balanced progress for all jobs during 
  dispatching. However, in-process inventory can be high.
\end{description}
These rules are the ones most commonly applied in the literature due to their 
simplicity and surprising efficiency. %\citep[cf.][]{Haupt89,Panwalkar77}
Therefore, they will be referenced throughout the dissertation. 
However, there are many more available, e.g., randomly selecting an operation 
with equal possibility (RND); minimum slack time (MST); smallest slack per 
operation (S/OP); and using the aforementioned dispatching rules with 
predetermined weights. 
A survey of more than 100 of such rules are presented in \citet{Panwalkar77}. 
However, the reader is referred to an in-depth survey for SDRs by 
\citet{Haupt89}. 


%Haupt89:
%Among the rules with job processing information, SPT is the most known, the most applied, and yet one of the most efficient rules. In line with LPT, it requires the lowest information amount, since only operation data (not job data) from the local queue (not from other queues) are needed.
%LWR give preference to jobs the work completed of which is rather advanced. Thus, they can be regarded as value-oriented rules selecting jobs with a high fraction of their value added or cumulative value to their total value.
% The intent of MWR is to speed up jobs with large processing work resulting in a well-balanced work progress of all jobs, at the expense of a high volume of in-process inventory, while LWR tend to reduce the number of jobs in the shop.

To summarise, SDRs assign an index to each job of the job-list waiting to be 
scheduled, and are generally only based on few features and simple mathematical 
operations. 
Continuing with the example from \cref{sec:jsp:example}, the final schedules 
for these main SDRs (and a possible optimal schedule for reference) are 
depicted in \cref{fig:example:SDRs}. As we can see, MWR would have  been 
the best strategy for Alice and company, since it has the makespan closest to 
the optimum. 

\begin{figure}[t]
    \includegraphics[width=\textwidth]{{example.gantt.SDRs}.pdf}
    \caption{SDRs applied to the Tea-party example in \cref{sec:jsp:example}. 
        A possible optimal solution is shown in the lower right 
        corner as a reference.}
    \label{fig:example:SDRs}
\end{figure}

\section{Features for \jsp}\label{sec:features}
A DR may need to perform a one-step look-ahead, and observe features of the 
partial schedule to make a decision. 
For example by observing the resulting temporal makespan. 
These emanated observed features are sometimes referred to as an 
\emph{after-state} or \emph{post-decision state}. 
A $k$-solution is denoted $\vchi^j$ where $J_j$ is the latest dispatch, i.e., 
$\chi_k=J_j$, and its resulting features is denoted, 
\begin{equation}\label{eq:postdecision}
  \vphi^j:=\vphi(\vchi^j).
\end{equation}

Features are used to grasp the essence of the current state of the schedule. 
Temporal scheduling features applied in this dissertation are given in 
\cref{tbl:features}. 

Note, from a job-oriented viewpoint, for a job already dispatched 
$J_j\in\mathcal{J}$ the corresponding set of machines already processed is 
$\mathcal{M}_j\subset\mathcal{M}$. Similarly from the machine-oriented 
viewpoint, $M_a\in\mathcal{M}$ with corresponding 
$\mathcal{J}_a\subset\mathcal{J}$. 

The features of particular interest were obtained from inspecting the 
aforementioned SDRs from \cref{sec:SDR}: namely \phiproc\ and \phijobWrm. 
Moreover, \phiJobRelated\ and \phiMacRelated\ are job-related and 
machine-related attributes of the current schedule, respectively. 

Some features are directly observed from the $k$-solution, such as the job- 
and machine-related features, namely, \phiLocalRelated\ and they are only based 
on the current step of the schedule, 
i.e., schedule's \emph{local features}, and might not give an accurate 
indication of how it will effect the schedule in the long run. Therefore, a set 
of features are needed to estimate the schedule's overall performance, referred 
to as its \emph{global features}. 

The approach here is to use well known SDRs, \phiSDRRelated, as a benchmark by 
retrieving what would the resulting $C_{\max}$ be given if that SDR would 
be implemented from that point forward. 
Moreover, random completion of the $k$-solution are implemented, 
here \phiRNDRelated\ corresponds to statistics from 100 random roll-outs, which 
can be used to identify which features $\vphi$ are promising on a long-term 
basis. 
\emph{Roll-out algorithms} \citep{Bertsekas97}, also known as \emph{Pilot 
Method} \citep{Duin99}, for combinatorial optimisation aim to improve 
performance by sequential application of a pilot heuristic, which completes the 
remaining $K-k$ steps. Roll-outs for \JSP\ have been conducted by 
\citet{Ru12}. Continuing with that work, \citet{Geirsson2012} compares several 
pilot heuristics, e.g., \emph{Randomly Chosen Dispatch Rules} which is similar 
to \phiSDRRelated\ (but here one roll-out per fixed SDR). 
The motivation being, that a SDR-based roll-out are of higher quality than 
random ones, requiring less computational budget. 
However, \citeauthor{Geirsson2012} notes that performance w.r.t. traditional 
random roll-outs is statistically insignificant, and not worth the overhead of 
implementing various SDRs beforehand. 

\citeauthor{Geirsson2012} reworks the roll-out algorithm as an 
$\abs{\mathcal{L}}$-armed bandit,\footnote{In probability theory, the
    multi-armed bandit problem \citep{badit:book} describes a gambler at a row 
    of slot machines, who has to decide which machines to play, i.e., pull its 
    lever, in  order to maximise his rewards, that are specific to each 
    machine. The gambler also has to decide how many times to play each machine 
    and in which order to play them. The gambler's actions are referred to as 
    \emph{pilot-heuristic}.} 
i.e., each job of the job-list are the levers. Since the best job, $j^*$, to 
dispatch at step $k$, is not known beforehand, therefore all available jobs are 
evaluated, using roll-outs. 
As a result, using the features \phiRNDRelated, the weights $\vec{w}$ yield the 
deterministic pilot heuristic. Although in \citeauthor{Geirsson2012}'s work, 
other statistics are used for guidance, e.g., quartile and octile. 

It's noted, that the roll-outs considered in this dissertation, are with a very 
frugal budget, only 100 roll-outs per lever is considered -- all evenly 
distributed between levers. But using the multi-armed bandit paradigm, it's 
possible to allocate roll-outs originating from the job-list with bias towards 
more promising levers.

\begin{table} \centering 
	\caption[Feature space $\mathcal{F}$ for \JSP]{Feature space $\mathcal{F}$ 
	for \JSP\ where job $J_j$ on machine $M_a$ given the resulting temporal 
	schedule after dispatching $(j,a)$.}
	\label{tbl:features}
	\input{tables/features-description}
\end{table}

\section{Composite dispatching rules}\label{sec:CDR}

Priority \dr s were originally introduced in \citet{Giffler60} to resolve 
conflicts of the job-list, and have made great headway since. 
They are especially attractive since they are relatively simple to 
implement, fast and find good schedules. In addition, they are easy 
to interpret, which makes them desirable for the end-user (i.e. shop floor 
operators). 
However, they can also fail unpredictably. 
\citet{Jayamohan04} showed that a careful combination of \dr s can 
perform significantly better. These are referred to as \emph{composite 
dispatching rules} (CDR), where the priority ranking is an expression of 
several DRs. 

For instance, optimising $J1||L_{\max}$ \cite[see. chapter 14.2]{Pinedo08}, one 
can combine SDRs that are optimal for a different criteria of problem 
instances, which complement each other as a CDR, e.g., combining the SDRs 
\begin{enumerate*}[after={{}}]
    \item WSPT \footnote{WSPT is optimal when all release dates and due dates 
    are zero.} (SPT weighted w.r.t. $\mathcal{J}$)
    \item minimum slack first (MS),\footnote{MS is optimal when all due dates 
    are sufficiently loose and spread out.} 
\end{enumerate*}
yields the CDR \emph{Apparent Tardiness Cost}, which can work well on a 
broader set of problem instances than the original SDRs by themselves.

%\subsection*{Mathematical formulation of CDRs}

CDRs can deal with a greater number of more complicated functions constructed 
from the schedules attributes. In short, a CDR is a combination of several DRs. 
For instance let $\pi$ be a CDR comprised of $d$ DRs, then the index $I$ for 
$J_j\in\mathcal{L}^{(k)}$ using $\pi$ is, 
\begin{equation}
  I_j^{\pi} = \sum_{i=1}^d w_i \pi_i(\vchi^j) \label{eq:CDR}
\end{equation}
where $w_i>0$ and $\sum_{i=0}^d w_i = 1$, then $w_i$ gives the \emph{weight} of 
the influence of $\pi_i$ (which could be a SDR or another CDR) to $\pi$. Note, 
each $\pi_i$ is function of $J_j$'s attributes from the $k$-solution $\vchi^j$.

The \cdr\ presented in \cref{eq:CDR} can be considered as a special case of a 
the following general linear value function,
\begin{equation}\label{eq:CDR:feat}
  \pi(\vchi^j) = \sum_{i=1}^d w_i \phi_i(\vchi^j)
              \stackrel{\eqref{eq:postdecision}}{=} \inner{\vec{w}}{\vphi^j}.
\end{equation}
when $\pi_i(\cdot)=\phi_i(\cdot)$, i.e., a composite function of the features 
from \cref{tbl:features}. 

Finally, the job to be dispatched, $J_{j^*}$, corresponds to the one with the 
highest value, i.e.,
\begin{equation}
  J_{j^*}=\argmax_{J_j\in \mathcal{L}}\; \pi(\vphi^j)
\end{equation}

Since we're using a feature space based on job-attributes, then it's trivial to 
interpret \cref{eq:CDR:feat} as the SDRs from \cref{sec:SDR}. 
Then for $i\in\{1,\ldots,d\}$, they're simply,

\begin{subequations}
\begin{IEEEeqnarray}{s"rCl} 
    SPT:& w_i&=&\bigg\{ \begin{array}{rl}-1&\text{if 
        }i=1\\0&\text{otherwise}\end{array}  \\
    LPT:& w_i&=&\bigg\{ \begin{array}{rl} 1&\text{if 
        }i=1\\0&\text{otherwise}\end{array}  \\
    MWR:& w_i&=&\bigg\{ \begin{array}{rl} 1&\text{if 
        }i=7\\0&\text{otherwise}\end{array}  \\
    LWR:& w_i&=&\bigg\{ \begin{array}{rl}-1&\text{if 
        }i=7\\0&\text{otherwise}\end{array} \label{pref:SDR}
\end{IEEEeqnarray} 
\end{subequations}


\subsection*{Automated discovery of CDRs}
Generally the weights $\vec{w}$ in \cref{eq:CDR:feat} are chosen by the 
algorithm designer a priori. 
A more sophisticated approach would have the algorithm discover these 
weights autonomously. For instance via preference-based 
imitation learning or evolutionary search, to be discussed in 
\cref{ch:prefmodels} and \cref{ch:esmodels}, respectively.

\citet{Monch13} stress the importance of automated discovery of DRs and named 
several successful such implementations in the field of semiconductor wafer 
fabrication facilities. 
However, \citeauthor{Monch13} note that this sort of investigation is still in its infancy and subject for future research.

A recent editorial of the state-of-the-art approaches in advanced dispatching 
rules for large-scale manufacturing systems by \citet{Chen13} points out that:
\begin{quote}
	[..] most traditional dispatching rules are based on historical data. With the emergence of data mining and on-line analytic processing, dispatching rules can now take predictive information into account.
\end{quote}
implying that there has not been much automation in the process of discovering 
new dispatching rules, which is the ultimate goal of this dissertation, i.e., 
automate creation of optimisation heuristics for scheduling. 

With meta heuristics one can use existing DRs and use for example 
{portfolio-based algorithm selection} either based 
on a single instance \citep{Rice76,Gomes01} or class of instances \citep{Xu07} 
to determine which DR to choose from. 
Instead of optimising which algorithm to use under what data distributions, 
such as the case of portfolio algorithms, the approach taken in this 
dissertation is more similar to that of \emph{meta learning} \citep{Vilalta02}, 
which is the study of how learning algorithms can be improved, i.e., exploiting 
their strengths and remedy their failings, in order for a better algorithm 
design. Thus, creating an adaptable learning algorithm that dynamically finds 
the appropriate dispatching rule  to the data distribution at hand. 

\citet{Kalyanakrishnan11} point out that meta learning can be very fruitful in 
reinforcement learning, and in their experiments they discovered some key 
discriminants between competing algorithms for their particular problem 
instances, which provided them with a hybrid algorithm which combines the 
strengths of the algorithms.

\citet{Nguyen13} proposed a novel {iterative dispatching rules} for \JSP\ 
which learns from completed schedules in order to iteratively improve new ones. 
At each dispatching step, the method can utilise the current feature space to 
`correctify' some possible `bad' dispatch made previously (sort of reverse 
lookahead). Their method is straightforward, and thus easy to implement and 
more importantly, computationally inexpensive, although \citeauthor{Nguyen13} 
stress that there still remains room for improvement. 

\citet{Korytkowski13} implemented {ant colony optimisation} to select the 
best DR from a selection of 9 DRs for \JSP\ and their experiments showed 
that the choice of DR do affect the results and that for all performance 
measures considered it was better to have all of the DRs to choose from rather 
than just a single DR at a time. 

Similarly, \citet{Lu13} investigate 11 SDRs for \JSP\ to create a pool of 
33 CDRs that strongly outperformed the ones they were based on. The CDRs were 
created with {multi-contextual functions} based either on machine idle time 
or job waiting time (similar to \phiwait\ and \phimacSlack\ in 
\cref{tbl:features}), creating CDRs thate are a combination of those 
two key features of the schedule and then the basic DRs. However, there are no 
combinations of the basic DR explored, only machine idle time and job waiting 
time.  

\citet{Yu13} used priority rules to combine 12 existing DRs from the 
literature, in their approach they had 48 priority rules combinations, 
yielding 48 different models to implement and test. This is a fairly 
ad-hoc solution and there is no guarantee the optimal combination of DRs is 
found. 

It is intuitive to get a boost in performance by introducing new CDRs, since 
where one DR might be failing, another could be excelling so combining them 
together should yield a better CDR. However, these aforementioned approaches 
introduce fairly ad-hoc solutions and there is no guarantee the optimal 
combination of \dr s were found.

\section{Rice's framework for \jsp}\label{sec:rice:jsp}
\citeauthor{Rice76}'s framework for algorithm selection (discussed in 
\cref{sec:rice}) has already been formulated for \jsp\ (cf. 
\citet{SmithMilesLion3,SmithMilesLion5} and \cref{InRu12}), as follows, 
\begin{description} 
	\item[Problem space] $\mathcal{P}$ is defined as the union of $N$ problem 
	instances consisting of processing time and ordering matrices, 
	$\vec{x}=(\vec{p},\vsigma)$, for $n$-jobs and $m$-machines, 
	\begin{equation} 
		\mathcal{P}=\condset{\vec{x}_i}{n\times m~}_{i=1}^{N}
	\end{equation}
	Problem generators for $\mathcal{P}$ are given in \cref{ch:genprobleminstances}.
	\item[Feature space] $\mathcal{F}$ which was outlined in 
	\cref{sec:features}. Note, these are not the only possible set of features. 
	However, the local feature, \phiLocalRelated, are built on the work by 
	\cite{SmithMilesLion3} and \cref{InRu11a} and deemed successful in 
	capturing the essence of a \jsp\ data structure;
	\item[Algorithm space] $\mathcal{A}$ is simply the scheduling policies under consideration, e.g., SDRs from \cref{sec:SDR},
	\begin{equation}
		\mathcal{A}=\left\{\text{SPT,~LPT,~LWR,~MWR,~RND,~}\dotsc\right\}.
	\end{equation} 
	\item[Performance space] $\mathcal{Y}$ is based on the resulting 
	$C_{\max}$, defined by \cref{eq:makespan}. The optimum makespan is denoted 
	$C_{\max}^{\pi_\star}$, i.e., following the expert policy $\pi_\star$, and 
	the makespan obtained from the scheduling policy $\pi\in\mathcal{A}$ under 
	inspection by $C_{\max}^{\pi}$. 
    Since the optimal makespan varies between problem instances the performance 
    measure is the following, 
	\begin{equation}\label{eq:rho}
		\rho=\frac{C_{\max}^{\pi}-C_{\max}^{\pi_\star}}{C_{\max}^{\pi_\star}}\cdot
		 100\%
	\end{equation}
	which indicates the \namerho. Thus $\mathcal{Y}$ is given as, 
	\begin{equation}
		\mathcal{Y}=\left\{\rho_i\right\}_{i=1}^{N}
	\end{equation}
\end{description}
The mapping $\Upsilon:\;\mathcal{A}\times\mathcal{F} \mapsto \mathcal{Y}$ is 
the step-by-step construction heuristic in \cref{pseudo:constructJSP}.