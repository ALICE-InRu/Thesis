\HeaderQuote{Well! I've often seen a cat without a grin; but a grin without a 
cat! It's the most curious thing I ever say in my life!}{Alice} 

\chapter{Generating Training Data}\label{ch:gentrdat} 

\FirstSentence{W}{hen building a complete} \jsp\ schedule, $K=n\cdot m$ 
dispatches must be made sequentially.
A job is placed at the earliest available time slot for its next machine, 
whilst still fulfilling constraints \cref{eq:permutation,eq:oneJobPerMac}.
%that each machine can handle at most one job at each time, and jobs need to 
%have finished their previous machines according to its machine order. 
Unfinished jobs are dispatched one at a time according to some heuristic, or 
policy $\pi$. 
After each dispatch\footnote{The terms dispatch (iteration) and time step are 
    used interchangeably.} the schedule's current features (cf. 
\cref{tbl:features}) are updated based on the half-finished schedule. Namely, 
when implementing \cref{pseudo:constructJSP}, a training set will consist of 
all features from \cref{tbl:features} at every post-decision state visited in 
line \ref{pseudo:constructJSP:phi}. 
These collected features are denoted $\Phi$, where, 
\begin{equation}\label{eq:Phi}
\Phi:= \bigcup_{i=1}^{N_{\text{train}}} 
\bigcup_{k=1}^K\bigcup_{J_j\in\mathcal{L}^{(k)}} 
\condset{\vphi_j}{\vec{x}_i\in\mathcal{P}^{n\times m}_{\text{train}} }.
\end{equation}

\begin{figure}[p]
    \input{figures/gametree.tex}
    \vspace{-27pt}
    \caption[Partial Game Tree for \jsp]{Partial Game Tree for \jsp\ for the 
        first two dispatches. 
        Top layer depicts all possible dispatches (dashed) for an empty 
        schedule. 
        Middle layer depicts all possible dispatches given that one of the 
        dispatches from the layer above has been executed (solid). 
        Bottom layer depicts when job $J_4$ on machine $M_4$ has been chosen to 
        be dispatched from the previous layer, moreover it depicts all possible 
        next dispatches from that scenario.}
    \label{fig:example:gametree}
\end{figure}

\vspace*{-24pt}
\section{\Jsp\ tree representation}\label{sec:gen:gametree}
Continuing with the example from \cref{sec:jsp:example}, 
\cref{fig:example:gametree} shows how the first two dispatches could be 
executed for a $4\times5$ \jsp\ from \cref{sec:jsp:example}.
In the top layer one can see an empty schedule.
In the middle layer one of the possible dispatches from the layer above is 
fixed, and one can see the resulting schedule, i.e., what are the next possible 
dispatches given this scenario? Assuming $J_4$ would be dispatched first, the 
bottom layer depicts all the next possible partial schedules.

This sort of tree representation is similar to \emph{game trees} 
\citep[cf.][]{Rosen03} where the root node denotes the initial (i.e. empty) 
schedule and the leaf nodes denote the complete schedule (resulting after 
$n\cdot m$ dispatches, thus height of the tree is $K$), therefore the 
distance $k$ from an internal node to the root yields the number of operations 
already dispatched. Traversing from root to leaf node one can obtain a sequence 
of dispatches that yielded the resulting schedule, i.e., the sequence indicates 
in which order the tasks should be dispatched for that particular schedule. 

\section{Labelling schedules w.r.t. optimal decisions} 
%\label{sec:trdat:label}
One can easily see that sequence $\vchi$ from \cref{eq:vchi} for task 
assignments is by no means unique. 
Inspecting a partial schedule further along in the dispatching process such as 
in \cref{fig:example:midway}, then let's say $J_2$ would be dispatched next, 
and in the next iteration $J_4$. 
Now this sequence would yield the same schedule as if $J_4$ would have been 
dispatched first and then $J_2$ in the next iteration. 
This is due to the fact they have non-conflicting machines, which indicates 
that some of the nodes in game tree can merge. 
Meanwhile, the states of the schedule are different and thus their 
features, although they manage to yield with the same (partial) schedule at a 
later date.  % ATHUGASEMD 1 -- SEQ. REP NON-UNIQUE
In this particular instance one can not infer that choosing $J_2$ is better and 
$J_4$ is worse (or vice versa) since they can both yield the same solution.

Furthermore, in some cases there can be multiple optimal solutions to the same 
problem instance. 
Hence not only is the sequence representation `flawed' in the sense that slight 
permutations on the sequence are in fact equivalent w.r.t. the end-result.
In addition, varying permutations of the dispatching sequence (however given 
the same partial initial sequence) can result in very different complete 
schedules but can still achieve the same makespan, and thus same \fullnamerho\ 
(which is the measure under consideration). 
Care must be taken in this case that neither resulting features are labelled 
\footnote{Here the tasks labelled `optimal' do not necessarily yield the
  optimum makespan (except in the case of following expert policy $\pi_\star$), 
  instead these are the optimal dispatches for the given partial schedule.} 
as undesirable. 
Only the features from a dispatch yielding a truly suboptimal solution should 
be labelled undesirable. 

\section{Computational growth}
The creation of the game tree for \JSP\ can be done recursively for all 
possible permutations of dispatches, resulting in a full \mbox{$n$-ary} tree 
(since $|\mathcal{L}|\leq n$) of height $K$. 
Such an exhaustive search would yield at the most $n^K$ leaf nodes. Worst case 
scenario being no sub-trees merge.

\noindent Since the internal vertices (i.e. partial schedules) are only of 
interest to learn,\footnote{The root is the empty initial schedule and for the
  last dispatch there is only one option left to choose from, so there is no 
  preferred `choice' to learn.} 
the number of those can be at the most \mbox{${}^{n^K-1}/_{n-1}$}.
Even for small dimensions of $n$ and $m$ the number of internal vertices are 
quite substantial and thus too computationally expensive to investigate them 
all. Not to mention that this is done iteratively for all $N_{\text{train}}$ 
problem instances.

Since we know that once a job is processed on all of its machines, then it 
stops being a contender for future dispatches, therefore the all possible 
assignments of operations for an $n\times m$~~\JSP\ would require an 
examination of $(n!)^m$ \citep{Giffler60}, thus a $6\times5$ problem may have 
at most $1.93\cdot10^{14}$ possible solutions, and for $10\times10$ problem 
then it's $3.96\cdot10^{65}$ solutions! Thus the factorial growth makes it 
infeasible for exploring all nodes to completion. However, our training data 
consist of relatively large $N_{\text{train}}$, so even though we will only 
pursue one trajectory per instance, then the aggregated training data will give 
it variety.

\section{Trajectory sampling strategies}\label{sec:trdat:tracks}
For each feature in \cref{eq:Phi} we need to keep track of the resulting 
makespan for its dispatched job. 
As a result, we obtain the meta-data from 
\cref{fig:rice} as follows, 
\begin{equation}\label{eq:trdat:metadata}
\{\Phi^\pi,\mathcal{Y}^\pi\} := 
\condset{\{ \vphi_j, C_{\max}^{\pi_\star(\vchi^j)}
    \}}{J_j\in\mathcal{L}^{(k)}}_{k=1}^K \in \mathcal{F} \times \mathcal{Y}
\end{equation}
for a single problem instance $\vec{x}\in\mathcal{P}_{\text{train}}$, and where 
$C_{\max}^{\pi_\star(\vchi^j)}$ denotes the optimal makespan (i.e. following 
the expert policy $\pi_\star$) from the resulting post-decision state $\vchi^j$.

Due to superabundant possible solutions for a single problem instance, 
there needs to be some logic based on how to sample the state-space for a 
valuable outcome. Especially considering the cost of correctly labelling 
\footnote{Optimal solutions can be obtained by using a commercial software 
    package by \citet{gurobi}, which has a free academic licence. However, GLPK 
    by \citet{glpk} has a free licence. Alas, GLPK has a lacklustre performance 
    w.r.t. speed for solving $10\times10$ \JSP.}
each dispatch that is encountered.\footnote{Generally it takes 
    only several hours to collect $N_{\text{train}}^{6\times5}=500$. Alas, when 
    going to higher dimension, $N_{\text{train}}^{10\times10}=300$ 
    really becomes an issue, as \jrnd{10}{10} needs a few days, and 
    \jrndn{10}{10} or \frnd{10}{10} require several weeks!}
Obviously we'd like to inspect optimal solutions as they are what we'd like to 
mimic. Moreover, since we'd like to infer the footprints in instance space for 
the SDRs we started doing in \cref{ch:defdifficulty}, then we will consider 
them also.
Similarly, the weights for \cref{eq:CDR:feat} that were optimised directly 
using from evolutionary search (cf. \cref{ch:esmodels}) will also be used.

\clearpage
To clarify, the trajectory sampling strategies for collecting a feature set and 
its corresponding labelling for \cref{eq:trdat:metadata} are the following:
\begin{description}
    \item[Optimum trajectory, \PhiSet{OPT} or $\Phi^{\pi_\star}$,] at each 
    dispatch some (random) optimal task is dispatched. This is also referred to 
    following the expert policy, $\pi_\star$.
    \item[SPT trajectory, \PhiSet{SPT},] at each dispatch the task 
    corresponding to shortest processing time is dispatched, i.e., following 
    \sdr~SPT.
    \item[LPT trajectory, \PhiSet{LPT},] at each dispatch the task 
    corresponding to largest processing time is dispatched, i.e., following 
    \sdr~LPT.
    \item[LWR trajectory, \PhiSet{LWR},] at each dispatch the task 
    corresponding to least work remaining is dispatched, i.e., following 
    \sdr~LWR.
    \item[MWR trajectory, \PhiSet{MWR},] at each dispatch the task 
    corresponding to most work remaining is dispatched, i.e., following 
    \sdr~MWR.
    \item[Random trajectory, \PhiSet{RND},] at each dispatch some random task 
    is dispatched.
    \item[CMA-ES trajectories, $\Phi^{\minRho}$ and $\Phi^{\minCmax}$,] at each 
    dispatch the task  corresponding to highest priority, computed with fixed 
    weights $\vec{w}$, which were obtained by optimising the mean for 
    \fullnamerho, with CMA-ES optimisation from \cref{ch:esmodels}. 
    \item[All trajectories, \PhiSet{ALL},] denotes all aforementioned 
    trajectories were explored, i.e., \vspace*{-18pt}
    \begin{equation} \nonumber
    \Phi^{\text{ALL}}:=\condset{\Phi^{A}}{\forall A \in 
    \left\{\pi_\star,\text{~SPT,~LPT,~LWR,~MWR,~RND,~}\minRho,\minCmax\right\}}
    \end{equation}
\end{description}

When following optimal trajectory, then due to the nature of the sequence 
representation (i.e. $\vchi$), the earlier stages for \Problem{j.rnd} of the 
dispatching are more or less equivalent and thus irrelevant (cf. 
\cref{fig:diff:case:OPT}). 
Hence it is appropriate to follow some random optimal path to begin with and 
then go after some (if not all possible) optimal paths until completion at step 
$K$. 

In the case of the $\Phi^{\langle \text{SDR} \rangle}$ and 
$\Phi^{\langle \text{CMA-ES} \rangle}$ 
trajectories it is sufficient to explore each trajectory exactly once for each 
problem instance. 
Whereas, for \PhiSet{OPT} and \PhiSet{RND} there can be several 
trajectories worth exploring, however, only one is chosen (at random). It is 
noted that since the number of problem instances, $N_{\text{train}}$, is 
relatively large, it is deemed sufficient to explore one trajectory for each 
instance, in those cases as well.

These trajectory strategies were initially introduced in \cref{InRu15a}. 
However, more SDR-based trajectories are now addressed since, e.g., LWR is 
considered more favourable for \fsp\ rather than MWR (cf. 
\cref{ch:defdifficulty}). 

The number of features that were collected on a step-by-step basis for 
\Problem[6\times5]{\text{train}} in \cref{tbl:data} is illustrated in 
\cref{fig:size:Phi:K}. 
There is an apparent stair-like structure for LWR, in accordance with its 
motivation (cf. \cref{sec:SDR}), which is completing jobs advanced in their 
progress, that is to say minimising $\mathcal{L}$ and from \cref{eq:Phi} we 
have $\abs{\Phi}\propto\abs{\mathcal{L}}$.
Whereas MWR tries to keep the jobs more balanced, hence more steady 
$\abs{\mathcal{L}}$, until at $k>(K-n)$ then \mbox{$\abs{\mathcal{L}} \lneq 
(K-k)$}, which explains the sharp decent near the end for MWR.
\Cref{tbl:size:Phi:K} gives the total size for $\abs{\Phi}$, indicating the
number of optimisations needed for obtaining $\mathcal{Y}$.

\begin{figure}[p]
    \includegraphics[width=\textwidth]{{trdat.size.6x5}.pdf}
    \vspace{-24pt}
    \caption[Size of training set, $\abs{\Phi}$]{Size of $6\times5$ training 
    set, $\abs{\Phi}$, over different trajectory strategies}
    \label{fig:size:Phi:K}
\end{figure}

\input{tables/size.trdat.tex}
