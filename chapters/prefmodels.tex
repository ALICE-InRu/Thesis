\HeaderQuote{Oh my ears and whiskers, how late it's getting!}{Rabbit} 

\chapter{Preference Learning}\label{ch:prefmodels} 
\FirstSentence{L}{earning models considered in this } dissertation are based on ordinal regression in which the learning task is formulated as learning preferences. In the case of scheduling, learning which operations are preferred to others. Ordinal regression has been previously presented in \cite{Ru06:PPSN}, and given in \cref{ch:ordinal} for completeness. 

\section{Ordinal regression for \jsp}
Using the training set $\{\Phi^\pi,\mathcal{Y}^\pi\}$, given in 
\cref{eq:trdat:metadata} by following some policy $\pi$, 
let $\vphi_{o}\in\Phi^\pi$ denote the post-decision state when dispatching 
job $J_o$ corresponds to an optimal schedule being built. All post-decisions 
states corresponding to suboptimal dispatches, $J_s$, are denoted by 
$\vphi_{s}\in\Phi^\pi$. 

Let's label feature sets which were considered optimal, 
$\vec{z}_{o}=\vphi_{o}-\vphi_{s}$, and suboptimal, 
$\vec{z}_{s}=\vphi_{s}-\vphi_{o}$ by $y_o=+1$ and $y_s=-1$ respectively. 
The preference learning problem is specified by a set of preference pairs,
\begin{equation}\label{eq:Psi:jsp}
	\Psi := \bigcup_{\{\vec{x}_i\}_{i=1}^{N_{\train}}}
    \condset{
        \left\{\vec{z}_o,+1\right\},\left\{\vec{z}_s,-1\right\}}{\forall 
        (J_o,J_s) 
        \in \mathcal{O}^{(k)} \times \mathcal{S}^{(k)}}_{k=1}^K 
    \subset \Phi\times Y 
\end{equation}
where 
\begin{enumerate*}
  \item $\Phi\subset\mathcal{F}$ is the training set of $d=\NrFeatLocal$ 
  features (cf. the local features from \cref{tbl:features}) 
  \item $Y=\{-1,+1\}$ is the outcome space
  \item at each dispatch $k\in\{1,\ldots,K\}$
  \item $J_o\in\mathcal{O}^{(k)},~J_s\in \mathcal{S}^{(k)}$
  are optimal and suboptimal dispatches, respectively
\end{enumerate*}

A negative example is only created as long as $J_s$ actually 
results in a worse makespan, i.e., $C_{\max}^{\pi_\star(\vchi^s)} \gneq 
C_{\max}^{\pi_\star(\vchi^o)}$, since there can exist situations in which more 
than one operation can be considered optimal. 
Hence, $\mathcal{O}^{(k)}\cup\mathcal{S}^{(k)}=\mathcal{L}^{(k)}$, and 
$\mathcal{O}^{(k)}\cap\mathcal{S}^{(k)}=\emptyset$.
If the makespan would be unaltered, the pair is omitted from $\Psi$, since they 
give the same optimal makespan. 
This way, only features from a dispatch resulting in a suboptimal solution is 
labelled undesirable.
The approach taken here is to verify analytically, at each time step, by 
retaining the current temporal schedule as an initial state, whether it can 
indeed \emph{somehow} yield an optimal schedule by manipulating the remainder 
of the sequence, i.e., $C_{\max}^{\pi_\star(\vchi^j)}$ for all 
$J_j\in\mathcal{L}^{(k)}$. 
This also takes care of the scenario that having dispatched a job resulting in 
a different temporal makespan would have resulted in the same final makespan if 
another optimal dispatching sequence would have been chosen. 
That is to say the data generation takes into consideration when there are 
multiple optimal solutions to the same problem instance. 

Since $Y=\{+1,-1\}$, we can use logistic regression, which makes decisions 
regarding optimal dispatches and at the same time efficiently estimates a 
posteriori probabilities. 
When using linear classification model (cf. \cref{sec:ord:linpref}) for 
\cref{eq:CDR:feat},then the optimal $\vec{w}^*$ obtained from the preference 
set can be used on any new data point (i.e. partial schedule), $\vchi$, and 
their inner product is proportional to probability estimate \cref{eq:prob}. 
%Similarly for non-linear classification models. 
Hence, for each job on the job-list, $J_j\in\mathcal{L}$, let $\vphi_j$ denote 
its corresponding post-decision state. Then the job chosen to be dispatched, 
$J_{j^*}$, is the one corresponding to the highest preference estimate
from \cref{eq:CDR:feat} where $\pi(\cdot)$ is the classification model obtained 
by the preference set, $\Psi$, defined by \cref{eq:Psi:jsp}. 

\section{Selecting preference pairs}\label{sec:trdat:param}
Defining the size of the preference set as $l=\abs{\Psi}$, then 
\cref{eq:Psi:jsp} gives the size of the feature training set as 
$\abs{\Phi}=\frac{1}{2}l$, which is given in 
\cref{fig:size:Phi:K,tbl:size:Phi:K}.
If $l$ is too large, than sampling needs to be done in order for the ordinal 
regression to be computationally feasible.

The strategy approached in  \cref{InRu11a} was to follow a \emph{single} 
optimal job $J_j\in\mathcal{O}^{(k)}$ (chosen at random), thus creating 
$\abs{\mathcal{O}^{(k)}}\cdot\abs{\mathcal{S}^{(k)}}$ feature pairs at each 
dispatch $k$, resulting in a preference set of size,
\begin{equation}\label{eq:sizePsi_b}
l =  \sum_{i=1}^{N_{\train}} \left(2 \abs{\mathcal{O}^{(k)}_i}\cdot 
\abs{\mathcal{S}^{(k)}_i} \right)
\end{equation}
For the problem spaces considered in \cref{InRu11a}, that sort of simple 
sampling of the state space was sufficient for a favourable outcome. 
However, for a considerably harder problem spaces (cf. \cref{ch:defdifficulty}) 
and not to mention increased number of jobs and machines, preliminary 
experiments were not satisfactory. 

A brute force approach was adopted to investigate the feasibility of finding 
optimal weights $\vec{w}$ for \cref{eq:CDR:feat}. 
By applying CMA-ES (discussed thoroughly in \cref{ch:esmodels}) to directly 
minimize the mean $C_{\max}$  w.r.t. the weights $\vec{w}$, gave a considerably 
more favourable result in predicting optimal versus suboptimal dispatching 
paths. 
So the question put forth is, why was the ordinal regression not able to detect 
it?
The nature of the CMA-ES is to explore suboptimal routes until it converges to 
an optimal one. 
Implying that the previous approach of only looking into one optimal route is 
not sufficient information. 
Suggesting that the preference set should incorporate a more complete knowledge 
about \emph{all} possible preferences, i.e., make also the distinction between 
suboptimal and sub-suboptimal features, etc.  
This would require a Pareto ranking for the job-list, $\mathcal{L}$, which can 
be used to make the distinction to which feature sets are equivalent, better or 
worse, and to what degree (i.e. giving a weight to the preference)? 
By doing so, the preference set becomes much greater, which of course would 
again need to be sampled in order to be computationally feasible to learn. 

For instance \cite{Siggi05} used decision trees to `rediscover' LPT by using 
the dispatching rule to create its training data. The limitations of using 
heuristics to label the training data is that the learning algorithm will mimic 
the original heuristic (both when it works poorly and well on the problem 
instances) and does not consider the real optimum. In order to learn new 
heuristics that can outperform existing heuristics then the training data needs 
to be correctly labelled. This drawback is confronted in 
\citep{Malik08,Russell09,Siggi10} by using an optimal scheduler, computed 
off-line. 

All problem instances are correctly labelled w.r.t. their optimum makespan, 
found with analytical means.\footnote{Optimal solution were found using 
  \cite{gurobi}, a commercial software package for solving large-scale linear 
  optimisation and a state-of-the-art solver for mixed integer programming.} 
The main motivation for the data generation of $\Psi$ that will be used in 
preference learning, will now need to consider the following main aspects
\begin{enumerate}[after={{}}, leftmargin=*, label={\textbf{PREF.\arabic*}}, 
ref={{PREF.\arabic*}}]
    \item Which path(s) should be investigated to collect training instances, 
    i.e., $\Phi$. Should they be features gathered resulting in
    \label{PREF:param:tracks}
    \begin{enumerate*}[label=\textit{\roman*)},before=\unskip{: }, itemjoin={{? 
    }}, itemjoin*={{, or }},after={{? }}]
      \item optimal solutions (querying expert policy $\pi_\star$)
      \item suboptimal solutions when a DR is implemented (following a fixed 
      policy $\pi$)
      \item combination of both
    \end{enumerate*}
    \item What sort of rankings should be compared during each step?
    \label{PREF:param:ranks}
    \item What sort of stepwise sampling strategy is needed for a good
    \emph{single} time independent model?
    \label{PREF:param:bias}
\end{enumerate}
The collection of the training set $\Phi$ in \ref{PREF:param:tracks} (which is 
described in \cref{ch:gentrdat}) is of paramount of importance, as the 
subsequent preference pairs in $\Psi$ are highly dependent on the quality of 
$\Phi$. 
Since the labelling of $\Phi$ is quite computationally intensive, its 
collection should be done parsimoniously in order to not waste valuable time 
and resources. 
On the other hand, \ref{PREF:param:ranks} and \ref{PREF:param:bias} are 
easy to inspect, once $\Phi$ has been chosen.
The followingÂ \lcnamecref{sec:trdat:param:ranks}s will try to address these 
research questions. 

\section{Scalability of \dr s}\label{sec:pref:scalability}

In \cref{InRu11a} a separate data set was deliberately created for each 
dispatch iterations, as the initial feeling is that dispatch rules used in the 
beginning of the schedule building process may not necessarily be the same as 
in the middle or end of the schedule. As a result there are $K$ linear 
scheduling rules for solving a $n \times m$ \jsp. 
Now, if we were to create a global rule, then there would have to be one 
model for all dispatches iterations. The approach in \cref{InRu11a} was to take 
the mean weight for all stepwise linear models, i.e., 
$\bar{w}_i=\frac{1}{K}\sum_{k=1}^K w_i^{(k)}$ where $\vec{w}^{(k)}$ is 
the linear weight resulting from learning preference set $\Psi(k)$ at 
dispatch $k$. 

A more sophisticated way, would be to create a \emph{new} linear model, where 
the preference set, $\Psi$, is the aggregation  of all preference pairs across 
the $K$ dispatches. 
This would amount to a substantial training set, and for $\Psi$ to 
be computationally feasible to learn, $\Psi$ has to be filtered to size 
$l_{\max}$. The default set-up will be, 
\begin{equation}\label{eq:lmax}
l_{\max} := \Bigg \{ \begin{array}{rccc} 
5 \cdot 10^5 & \quad\text{for} & 10\times 10 & \text{\JSP} \\
10^5 & \quad\text{for} & 6\times 5 & \text{\JSP}
\end{array}
\end{equation}
which is roughly 60\%-70\% amount of preferences encountered from one pass of 
sampling a \mbox{$K$-stepped} trajectory using a fixed policy $\hat{\pi}$ for 
the default $N_{\train}$ (cf. \cref{tbl:size:Psi:K}). 
Sampling is done randomly, with equal probability.

\section{Ranking strategies}\label{sec:trdat:param:ranks}
First let's address \ref{PREF:param:ranks}. 
The various ranking strategies for adding preference pairs to $\Psi$ defined by 
\cref{eq:Psi:jsp} were first reported in \cref{InRu15a}, and are the following,
\begin{description}
    \item[Basic ranking, \PsiSet{b},] i.e., all optimum rankings $r_1$ versus 
    all 
    possible suboptimum rankings $r_i$, $i\in\{2,\ldots,n'\}$, preference pairs 
    are added -- same basic set-up introduced in \cref{InRu11a}. Note, 
    $\abs{\Psi_b}$ is defined in \cref{eq:sizePsi_b}.
    \item[Full subsequent rankings, \PsiSet{f},] i.e., all possible 
    combinations 
    of $r_i$ and $r_{i+1}$ for $i\in\{1,\ldots,n'\}$, preference pairs are 
    added.
    \item[Partial subsequent rankings, \PsiSet{p},] i.e., sufficient set of 
    combinations of $r_i$ and $r_{i+1}$ for $i\in\{1,\ldots,n'\}$, are added to 
    the training set -- e.g. in the cases that there are more than one 
    operation with the same ranking, only one of that rank is needed to 
    compared to the subsequent rank. Note that $\Psi_p\subset \Psi_f$.
    \item[All rankings, \PsiSet{a},] denotes that all possible rankings were 
    explored, i.e.,
    $r_i$ versus $r_j$ for $i,j\in\{1,\ldots,n'\}$ and $i\neq j$, preference 
    pairs are added.
\end{description}
where $r_1>r_2>\ldots>r_{n'}$ ($n'\leq n$) are the rankings of the job-list, 
$\mathcal{L}^{(k)}$, at time step $k$.
By definition the following property holds:
\begin{equation}\label{eq:Psi:size}
    \Psi_p \subset \Psi_f \subset \Psi_b \subset \Psi_a
\end{equation}

To test the validity of different ranking strategies for 
\ref{PREF:param:ranks}, 
a training set of $N_{\train}=500$ problem instances of \jrnd{6}{5} and 
\frnd{6}{5} is collected for all trajectories described in 
\cref{sec:trdat:tracks}. 
The size of the preference set, $\abs{\Psi}$, is depicted in 
\cref{fig:size:Psi:K} for each iteration $k$. 
From which, a linear preference model is created for each preference 
set, $\Psi$. A box-plot for \fullnamerho, is presented in 
\cref{fig:boxplot:prefset}. 
From the figure it is apparent there can be a performance edge gained by 
implementing a particular trajectory strategy, yet ranking scheme seems to be 
irrelevant. Moreover, the behaviour is analogous across all other 
\Problem[6\times5]{\train} in \cref{tbl:data}.

First let's restrict  the models to \Problem[6\times5]{\train}. 
There is no statistical difference between \PsiSet{f} and \PsiSet{p} 
ranking-schemes across all disciplines,
which is expected since \PsiSet{f} is designed to contain the same preference 
information as \PsiSet{f} (cf. \cref{eq:Psi:size}). 
However, neither of the Pareto ranking-schemes outperform the original 
\PsiSet{b} set-up from \cref{InRu11a}. 
The results hold for the test set as well. 
Any statistical difference between ranking schemes were for \PsiSet{a}, where 
it 
was considered slightly lacking than some of its counterparts. 
Since a smaller preference set is preferred, its opted to use the \PsiSet{p} 
ranking scheme henceforth as the default set-up for \ref{PREF:param:ranks}. 

Moving on to higher dimension, results for \jrnd{10}{10} were similar to 
\Problem[6\times5]{\train}. Only exception begin that ranking schemes 
showed difference in performance when using \PhiSet{\OPT}, where 
\PsiSet[\OPT]{p} come on top. Strengthening our previous choice of \PsiSet{p} 
as standard ranking scheme.


\begin{figure}[p]
  \includegraphics[width=\textwidth]{{prefdat.size.6x5}.pdf}
  \vspace*{-30pt}
  \caption[Size of preference set, $\abs{\Psi}$]{Size of 
  \Problem[6\times5]{\train} preference set, $l=\abs{\Psi}$, for 
  different trajectory strategies and ranking schemes (where 
  $N_{\train}=500$) }
  \label{fig:size:Psi:K}
\end{figure}

\begin{figure}[p]
  \includegraphics[width=\textwidth]{{prefdat.boxplot.6x5}.pdf}
  \vspace*{-30pt}
  \caption[Box-plot for various $\Phi$ and $\Psi$ set-up]{
      Box-plot for various $\Phi$ and $\Psi$ set-up using
      \Problem[6\times5]{\train}. The trajectories 
      the models are based on are depicted in white on the far right.}
  \label{fig:boxplot:prefset}
\end{figure}

\section{Trajectory strategies}\label{sec:trdat:param:tracks:passive}

We'd like to inspect which trajectory is the best to use for $\Psi$. 
\Cref{InRu15a} only considered \jrnd{6}{5} and \jrndn{6}{5}, 
however, results for \Problem[6\times5]{\train} and 
\jrnd{10}{10} are currently available.\footnote{Additional problem spaces can 
    be found in \shiny: Preference Models $>$ Trajectories \& ranks.}
Models from \cref{fig:boxplot:prefset} are limited to the ones corresponding to 
\PsiSet{p}. Moreover, main statistics for \Problem[10\times10]{} are given in 
\cref{tbl:tracks:boxplot}.
\Cref{fig:size:Psi.p:K} jointly illustrates the size of the preference set 
used, i.e., $\abs{\Psi_p}$ from \cref{fig:size:Psi:K}.
\Cref{tbl:size:Psi:K} reports the total amount of preferences for all $K$ 
dispatches.

\Cref{tbl:param:tracks} reports the relative ordering of trajectories, 
ordered w.r.t. their mean \namerho, and their size of preference set, i.e., 
$\abs{\Psi_p}$.
Models that are statistically better are denoted by `$\succ$' otherwise 
considered equivalent. 

For most problem spaces \PsiSet[\LPT]{p} was the worst trajectory to pursue. 
Looking back at \cref{fig:size:Phi:K}, then even though \PhiSet{\LPT} was not 
the trajectory with the least features, the amount of equivalent features 
w.r.t. $C_{\max}$ are far too many to make a meaningful preference set out of 
it. It's only for \jrndn{6}{5} that there is another trajectory with fewer 
preferences, namely \PsiSet[\LWR]{p} (cf. \cref{fig:size:Psi:K}), and in that 
case LWR is the worst model.
Model that come on top, are those that have a varied $\Psi$. However, 
aggregating features from all trajectories is not a good idea, as the 
preference set then becomes too varied for a satisfactory result. 

\input{tables/size.prefdat.tex}
\input{tables/PREF.track.ordering.tex}
\input{tables/stats.tracks.10x10.tex}

Learning preference pairs from a good scheduling policies, such as 
\PhiSet{\minCmax}, \PhiSet{\minRho} and \PhiSet{\MWR}, gave considerably more 
favourable results than tracking optimal paths, save for \fjc{6}{5} where the 
ordering is reversed. Generally, suboptimal routes are preferred. 
However, even though LWR is a better policy than MWR for \FSP, then 
\PhiSet{\LWR} is a worse candidate than e.g. \PhiSet{\MWR}, but as discussed 
before, it's due to the lack of varied dispatches for the trajectory.

It is particularly interesting there is statistical difference between 
\PhiSet{\OPT} and \PhiSet{\RND}, where the latter had improved performance for 
all \JSP\ problem spaces. In those cases, tracking optimal dispatches gives 
worse performance as pursuing completely random dispatches. 
This indicates that exploring only expert policy can result in a 
training set which the learning algorithm is inept to determine good dispatches 
in the circumstances when newly encountered features have diverged from the 
learned feature set labelled to optimum solutions. 

Generally, adding suboptimal trajectories with the expert policy, i.e., 
\PhiSet{\ALL}, gives the learning algorithm a greater variety of preference 
pairs for getting out of local minima. However, for some problem spaces, e.g., 
\frnd{6}{5} and \fmc{6}{5} then additional suboptimal solutions that are too 
diverse yield a worse outcome than \PhiSet{\OPT} would achieve on its own.

%   Problem Dimension   SPT   LPT   LWR   MWR   RND ES.rho ES.Cmax
%     j.rnd       6x5 Track Track Track Track Track    SDR     SDR
%    j.rndn       6x5 Track Track Track Track Track   SAME    SAME
%  j.rnd,J1       6x5 Track Track Track   SDR Track    SDR     SDR
%  j.rnd,M1       6x5 Track  SAME Track   SDR Track    SDR     SDR
%     f.rnd       6x5 Track Track Track Track Track    SDR     SDR
%    f.rndn       6x5 Track Track Track Track Track    SDR     SDR
%      f.jc       6x5 Track Track Track Track Track    SDR     SDR
%      f.mc       6x5 Track Track   SDR Track Track    SDR     SDR
%     f.mxc       6x5 Track Track   SDR Track Track    SDR     SDR
%     j.rnd     10x10 Track Track Track  SAME Track    SDR     SDR

\pagebreak
Comparing \PsiSet[\pi]{p} to its corresponding policy $\pi$ used to guide its 
collection, then usually the preference model outperformed the $\pi$ it 
was trying to mimic. The exceptions being 
\begin{enumerate*}
  \item MWR for \jrndJ{6}{5} and \jrndM{6}{5} (and \jrnd{10}{10} was 
  statistically insignificant)
  \item LWR for \fmc{6}{5} and \fmxc{6}{5}
  \item LPT was statistically insignificant for \jrndM{6}{5}
  \item $\minCmax$ and $\minRho$ for all problem spaces, save for \jrndn{6}{5} 
  which was statistically insignificant
\end{enumerate*}
Revisiting \cref{fig:diff:case:SDR}, then when \PsiSet[\pi]{p} 
succeeds its original policy $\pi$, it implies the learning model was able to 
steer the learned policy towards $\zeta_{\min}^{\pi}$. 
In fact, its improvement is proportional to its spread\footnote{Consult \shiny:
    Optimality $>$ Best and worst case scenario.}  
from $\zeta_{\mu}^{\pi}$ to $\zeta_{\min}^{\pi}$ or $\zeta_{\max}^{\pi}$.
Therefore, a good preference set based on \PhiSet{\pi} not only has to have a 
low $\zeta_{\mu}^\pi$ to mimic, but also the policy $\pi$ needs to be 
sufficiently different from $\zeta_{\min}^{\pi}$ and $\zeta_{\max}^{\pi}$ for 
adequate learning. That is why \PhiSet{\CMAES} strategies 
were not good enough for preference learning, as their 
$\zeta^\pi_{\any}$ spread was the lowest compared to the other fixed DRs.

The rational for using the \PhiSet{\CMAES} strategies
was mostly due to the fact a linear classifier is creating the training data 
(using the weights found via CMA-ES optimisation in \cref{eq:cma:objfun}), 
hence the training data created should be linearly separable, which in turn 
should boost the training accuracy for a linear classification learning model. 
However, these strategies is not outperforming the original DR used in guiding 
the training data collection. 

\begin{figure}[p]
    \includegraphics[width=\textwidth]{{prefdat.p.size.6x5}.pdf}
    \vspace*{-30pt}
    \caption[Size of preference set, $\abs{\Psi_p}$]{Size of 
        \Problem[6\times5]{\train} preference set, $l=\abs{\Psi_p}$, for 
        different trajectory strategies}
    \label{fig:size:Psi.p:K}
\end{figure}

\section{Stepwise sampling bias}\label{sec:pref:bias}

\begin{figure}[p]
    \includegraphics[width=\textwidth]{{bias.CDR.10x10}.pdf}
    \vspace*{-30pt}
    \caption{Log probability for stepwise sampling for \PsiSet[\OPT]{p} based 
    on \Problem[10\times10]{\train}}
    \label{fig:bias}
\end{figure}

Experiments in \cref{sec:trdat:param:tracks:passive} clearly showed that 
following the expert policy is not without its faults. 
There are many obstacles to consider to improve the model. 
For instance, it was chosen to sample $l_{\max}$ in \cref{eq:lmax} with equal 
probability. 
But inspecting the effects of making suboptimal choices varies as 
a function of time (cf. \cref{ch:analysingsol}), perhaps its stepwise bias 
should rather be done proportional to the mean cumulative loss to a particular 
time step?
Following strategies for stepwise bias for \ref{PREF:param:bias} will now 
be proportional to
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Bias.\arabic*}}, ref={{Bias.\arabic*}}]
    \item \textbf{(equal)} \label{bias:equal} equal probability. 
    \item \textbf{(opt)} \label{bias:opt} inverse optimality for random 
    dispatches, i.e., $1-\xi_{\RND}^{\star}$.
    \item \textbf{(bcs)} \label{bias:bcs} best case scenario for mean $\rho$, 
    i.e., $\zeta^{\star}_{\min}$.
    \item \textbf{(wcs)} \label{bias:wcs} worst case scenario for mean $\rho$, 
    i.e., $\zeta^{\star}_{\max}$.
    \item \textbf{(featsize)} \label{bias:featsize} inversely proportional to 
    $\abs{\Phi^{\OPT}}$, defined as
    $$\max_{k'}\{\abs{\Phi^{\OPT}(k')}\}-\abs{\Phi^{\OPT}(k)}+\min_{k}\{\abs{\Phi^{\OPT}(k')}\}$$
    \item \textbf{(prefsize)} \label{bias:prefsize} inversely proportional to 
    $\abs{\Psi^{\OPT}_p}$, defined as  
    $$\max_{k'}\{\abs{\Psi^{\OPT}_p(k')}\}-\abs{\Psi^{\OPT}_p(k)}+\min_{k'}\{\abs{\Psi^{\OPT}_p(k')}\}$$
    \item \textbf{(dbl1st)} \label{bias:dbl1st} twice as much weight on the 
    first half of the dispatches.
    \item \textbf{(dbl2nd)} \label{bias:dbl2nd} twice as much weight on the 
    second half of the dispatches.
\end{enumerate}
Moreover, all strategies are also adjusted to the preference set size, i.e., 
$\nicefrac{1}{\abs{\Psi^{\OPT}_p}}$.
The sampling strategy for \PsiSet{p} in \cref{InRu14,InRu15a} was 
\ref{bias:equal} and serves as a baseline.  
Motivation for \ref{bias:opt} is that way samples from dispatches that are less 
likely to be optimal than simply at random (cf. \cref{fig:diff:opt:rnd}) are 
emphasised. 
Whereas, \ref{bias:bcs} and \ref{bias:wcs} are more focused on 
sampling w.r.t. the final measure, where the mean $\rho$ is given \emph{one} 
suboptimal move, otherwise it's assumed expert policy is followed (cf. 
\cref{fig:diff:case:OPT}). 
The adjustment of preference set tries to give equal emphasis on stepwise 
features, as they substantially decrease over time (cf. 
\cref{fig:size:Phi:K,fig:size:Psi.p:K}), which proved favourable in preliminary 
experiments, then \ref{bias:featsize} and \ref{bias:prefsize} motivation is to 
boost that adjustment even further.
Lastly, \ref{bias:dbl1st} and \ref{bias:dbl2nd} are very simplified 
versions of the aforementioned strategies.
\Cref{fig:bias} jointly illustrates the stepwise bias strategies for 
\Problem[10\times10]{\train}. 

It's possible to circumvent the choice of stepwise sampling strategy by 
creating a preference model for each time step $k$, for a grand total of $K$ 
models. 
By doing so it's possible to capture local changes in the schedule, as we've 
already seen the evolution of features varies. Moreover, in 
\cref{fig:cma:boxplot} then a stepwise \emph{new} model was generally better 
than a \emph{single} global one.
However, in that case it's not possible to test those models against 
other dimensions, e.g., test benchmarks suite from OR-Library (cf. 
\cref{tbl:data:orlib}). 

\input{tables/stats.bias.10x10}

\begin{figure}[t]
    \includegraphics[width=\textwidth]{{bias.boxplotRho.CDR.10x10}.pdf}
    \caption[Box-plots for \namerho, using various stepwise bias sampling 
    strategies]{Box-plots for \namerho, where \PsiSet[\OPT]{p} is sampled with 
        various stepwise bias strategies. 
        Moreover, time dependent models shown on  far left as reference.}
    \label{fig:bias:boxplot}
\end{figure}

\Cref{fig:bias:boxplot} depicts box-plots for \namerho, using the various 
sampling strategies for \Problem[10\times10]{}. 
Main statistics for \Problem[10\times10]{\train} are reported in 
\cref{tbl:bias:boxplot}. 
In addition to the stepwise bias strategies (both adjusted and not) a stepwise 
model (one for each step $k$) is given for reference.

First off, counter-intuitively the stepwise model is not the best 
configuration. 
By applying one of these aforementioned sampling strategies it's possible to 
achieve better results than applying a local model for each time step. 
In fact, for \jrndn{10}{10} a stepwise model was the worst approach (with up to 
12\% increased error). 
This could possibly be explained by the fact that there are quite a few 
non-conflicting operations. As a result there is this vague change in `time' 
for consecutive steps. Therefore, using a complete data set which aggregates  
all time steps (or arguably over a few steps) is more beneficial for learning, 
it is dealt with on a more sustainable grounds.

Adjusting \PsiSet[\OPT]{p} to its stepwise size generally improved the sampling 
strategy (up to mean $\Delta\rho\approx-7\%$), where \ref{bias:opt} and 
\ref{bias:equal} were equivalent for \jrnd{10}{10}. 
Whereas, \jrndn{10}{10} \ref{bias:wcs} was significantly worsened 
by mean $\Delta\rho\approx+3\%$ if adjusted. Other \jrndn{10}{10} strategies 
were equivalent w.r.t. adjustment.
Reverting back to \cref{fig:jrnd:feat:KStest}, then we saw that near the end of 
the dispatching process then for all SDRs there was a clear segregation of 
features w.r.t. its difficulty. This implies great predictability of 
features in that time region. However, since those data points are scarce they 
get overrun by the superabundant preference pairs from the preceding 
dispatches, unless they are appropriately superimposed to be relevant for 
classification.

For all problem spaces, there was no significant difference between stepwise 
models to either \ref{bias:featsize} or \ref{bias:prefsize}. 
In the case of \jrnd{10}{10} and \frnd{10}{10} when a stepwise model is 
promising, then superimposing the adjustment of preference set gives the 
best overall outcome. 
Whereas, in the case of \jrndn{10}{10} then a single stepwise model is not as 
adequate as its single counterparts, then over emphasising w.r.t. the set works 
poorly. 
All other bias strategies for \jrndn{10}{10} came out similar, with the 
exception of \cref{bias:bcs} being slightly lacking, yet better than 
\ref{bias:featsize} and \ref{bias:prefsize}.

Furthermore, \ref{bias:opt} work just as well as its simplified version 
\ref{bias:dbl1st} in \jrnd{10}{10} and \jrndn{10}{10}, but for \frnd{10}{10} 
the simpler version was slightly better. 
Similarly, \ref{bias:dbl2nd} was equivalent to \ref{bias:wcs}.
Note in \frnd{10}{10} then \ref{bias:dbl2nd} serves as just as well as its best 
strategies \ref{bias:prefsize}.

To summarise, adjusting the preference set to give each step equal probability 
is a good first step. 
Moreover, when time dependent model are good then further exaggeration of the  
adjustment to the preference set (such as \ref{bias:featsize} and 
\ref{bias:prefsize}) is best. 
However, a severely simplified version can be just as good. 
With these configurations it was possible to improve mean \namerho, by
\begin{enumerate*}
    \item 14\% using adjusted \ref{bias:prefsize} for \jrnd{10}{10}
    \item 1\% using adjusted \ref{bias:dbl1st} (not significant improvement) 
    for \jrndn{10}{10} 
    \item 8\% using adjusted \ref{bias:prefsize} for \frnd{10}{10}
\end{enumerate*}

%Problem Rank Track Bias Adjusted   Set Min. 1st Qu. Median  Mean 3rd Qu.  Max.
% j.rnd 10x10 p ES.Cmax equal  F train 4.20  11.580  15.56 16.07   19.64 38.24
% j.rnd 10x10 p ES.Cmax equal  F test  3.23  11.490  15.07 16.17   19.67 47.52
% j.rnd 10x10 p ES.Cmax dbl2nd T train 0.66   7.938  11.18 11.39   14.48 32.04
% j.rnd 10x10 p ES.Cmax dbl2nd T test  2.57   8.823  11.30 11.73   14.56 27.23

Using the simplified version of the best configuration for \PsiSet[\OPT]{p}, 
i.e., adjusted \ref{bias:dbl2nd}, for the best \jrnd{10}{10} trajectory from  
\cref{sec:trdat:param:tracks:passive}, namely \PsiSet[\minCmax]{p}, then it's 
possible to get a 4.5\% mean boost in performance, i.e., 11.39\% and 11.73\% 
for training and test set, respectively. 
Note, optimisation w.r.t. \cref{eq:cma:makespan} achieved 10.57\% 
and 11.33\% mean $\rho$ for training and test set, respectively, which is 
statistically insignificant from the adjusted preference model.

\section{Conclusions}
Since the preference set is ideally aggregated and possibly re-sampled 
to adjust for lacking $\abs{\Psi(k)}$ count, then $\Psi$ needs to be sampled to 
size $l_{\max}$ such that it contains maximum information, yet with minimal 
amount of preference pairs. 
By use of partial subsequent Pareto ranking to address \ref{PREF:param:ranks}, 
denoted \PsiSet{p}, from \cref{sec:trdat:param:ranks}, it's possible to reduce 
$\abs{\Psi}$ significantly, without loss in performance. 

Experimental results in \cref{sec:trdat:param:tracks:passive} for 
\ref{PREF:param:tracks} illustrated that unlike 
\citet{Siggi10,Malik08,Russell09} learning only on optimal training data was 
not fruitful. 
However, \cref{sec:pref:bias} showed if stepwise sampling for 
\ref{PREF:param:bias} is done appropriately then it's possible to boost 
performance significantly, even outperforming a single model for each time step.
First and foremost the stepwise bias in sampling needs to counter-act the 
disproportionate amount of features towards the end. 
Moreover, additional emphasis to the latter stages of the dispatches is 
beneficial as that's when \JSP\ is more susceptible to failure. 
Furthermore, since the problem spaces showed difference boost in performance 
depending on the various complexities of its best sampling strategy, its 
simpler version is recommended, namely configuration denoted \ref{bias:dbl2nd}.

Inspired by the original work by \cite{Siggi05}, having fixed DRs guide the 
generation of training data (except correctly labelling with analytic means) 
gives meaningful preference pairs which the learning algorithm could learn. 
The best strategy was by using the weights from CMA-ES optimisation, obtained 
by optimising \cref{eq:cma:objfun} directly. 
Its preference model was able to be statistically insignificant to its guiding 
policy (cf. \cref{sec:pref:bias}). 
However, we have yet been able to outperform direct optimisation. 

Generally aggregating trajectories, from optimal and suboptimal policies, 
boosts performance. However, they need to be chosen carefully, since
with increased aggregation it can become counter-productive as the features are 
too dissimilar. 
A more sophisticated approach in combing the two strategies is needed. 

