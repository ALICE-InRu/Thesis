\HeaderQuote{It was much pleasanter at home, when one wasn't always growing larger and smaller, and being ordered about by mice and rabbits.}{Alice} 
%\HeaderQuote{The adventures first\dots explanations take such a dreadful time.}{The Gryphon} 

\chapter{Preference Learning of CDRs}\label{ch:prefmodels} 
\FirstSentence{L}{earning models considered in this } dissertation are based on ordinal regression in which the learning task is formulated as learning preferences. In the case of scheduling, learning which operations are preferred to others. Ordinal regression has been previously presented in \cite{Ru06:PPSN}, and given in \cref{ch:ordinal} for completeness. 

\section{Ordinal regression for \jsp}
Using the training set $\{\Phi^\pi,\mathcal{Y}^\pi\}$, given in 
\cref{eq:trdat:metadata} by following some policy $\pi$, 
let $\vphi_{o}\in\Phi^\pi$ denote the post-decision state when dispatching 
job $J_o$ corresponds to an optimal schedule being built. All post-decisions 
states corresponding to suboptimal dispatches, $J_s$, are denoted by 
$\vphi_{s}\in\Phi^\pi$. 

Let's label feature sets which were considered optimal, 
$\vec{z}_{o}=\vphi_{o}-\vphi_{s}$, and suboptimal, 
$\vec{z}_{s}=\vphi_{s}-\vphi_{o}$ by $y_o=+1$ and $y_s=-1$ respectively. 
The preference learning problem is specified by a set of preference pairs,
\begin{equation}\label{eq:Psi:jsp}
	\Psi := \bigcup_{\{\vec{x}_i\}_{i=1}^{N_{\text{train}}}}
    \condset{
        \left\{\vec{z}_o,+1\right\},\left\{\vec{z}_s,-1\right\}}{\forall 
        (J_o,J_s) 
        \in \mathcal{O}^{(k)} \times \mathcal{S}^{(k)}}_{k=1}^K 
    \subset \Phi\times Y 
\end{equation}
where 
\begin{enumerate*}
  \item $\Phi\subset\mathcal{F}$ is the training set of $d=\NrFeatLocal$ 
  features (cf. the local features from \cref{tbl:features}) 
  \item $Y=\{-1,+1\}$ is the outcome space
  \item at each dispatch $k\in\{1,\ldots,K\}$
  \item $J_o\in\mathcal{O}^{(k)},~J_s\in \mathcal{S}^{(k)}$
  are optimal and suboptimal dispatches, respectively
\end{enumerate*}

A negative example is only created as long as $J_s$ actually 
results in a worse makespan, i.e., $C_{\max}^{\pi_\star(\vchi^s)} \gneq 
C_{\max}^{\pi_\star(\vchi^o)}$, since there can exist situations in which more 
than one operation can be considered optimal. 
Hence, $\mathcal{O}^{(k)}\cup\mathcal{S}^{(k)}=\mathcal{L}^{(k)}$, and 
$\mathcal{O}^{(k)}\cap\mathcal{S}^{(k)}=\emptyset$.
If the makespan would be unaltered, the pair is omitted from $\Psi$, since they 
give the same optimal makespan. 
This way, only features from a dispatch resulting in a suboptimal solution is 
labelled undesirable.
The approach taken here is to verify analytically, at each time step, by 
retaining the current temporal schedule as an initial state, whether it can 
indeed \emph{somehow} yield an optimal schedule by manipulating the remainder 
of the sequence, i.e., $C_{\max}^{\pi_\star(\vchi^j)}$ for all 
$J_j\in\mathcal{L}^{(k)}$. 
This also takes care of the scenario that having dispatched a job resulting in 
a different temporal makespan would have resulted in the same final makespan if 
another optimal dispatching sequence would have been chosen. 
That is to say the data generation takes into consideration when there are 
multiple optimal solutions to the same problem instance. 

Since $Y=\{+1,-1\}$, we can use logistic regression, which makes decisions 
regarding optimal dispatches and at the same time efficiently estimates a 
posteriori probabilities. 
When using linear classification model (cf. \cref{sec:ord:linpref}) for 
\cref{eq:CDR:feat},then the optimal $\vec{w}^*$ obtained from the preference 
set can be used on any new data point (i.e. partial schedule), $\vchi$, and 
their inner product is proportional to probability estimate \cref{eq:prob}. 
%Similarly for non-linear classification models. 
Hence, for each job on the job-list, $J_j\in\mathcal{L}$, let $\vphi_j$ denote 
its corresponding post-decision state. Then the job chosen to be dispatched, 
$J_{j^*}$, is the one corresponding to the highest preference estimate
from \cref{eq:CDR:feat} where $\pi(\cdot)$ is the classification model obtained 
by the preference set, $\Psi$, defined by \cref{eq:Psi:jsp}. 

\section{Selecting Preference Pairs}\label{sec:trdat:param}
Defining the size of the preference set as $l=\abs{\Psi}$, then 
\cref{eq:Psi:jsp} gives the size of the feature training set as 
$\abs{\Phi}=\frac{1}{2}l$, which is given in 
\cref{fig:size:Phi:K,tbl:size:Phi:K}.
If $l$ is too large, than sampling needs to be done in order for the ordinal 
regression to be computationally feasible.

The strategy approached in  \cref{InRu11a} was to follow a \emph{single} 
optimal job $J_j\in\mathcal{O}^{(k)}$ (chosen at random), thus creating 
$\abs{\mathcal{O}^{(k)}}\cdot\abs{\mathcal{S}^{(k)}}$ feature pairs at each 
dispatch $k$, resulting in a preference set of size,
\begin{equation}\label{eq:sizePsi_b}
l =  \sum_{i=1}^{N_{\text{train}}} \left(2 \abs{\mathcal{O}^{(k)}_i}\cdot 
\abs{\mathcal{S}^{(k)}_i} \right)
\end{equation}
For the problem spaces considered in \cref{InRu11a}, that sort of simple 
sampling of the state space was sufficient for a favourable outcome. 
However, for a considerably harder problem spaces (cf. \cref{ch:defdifficulty}) 
and not to mention increased number of jobs and machines, preliminary 
experiments were not satisfactory. 

A brute force approach was adopted to investigate the feasibility of finding 
optimal weights $\vec{w}$ for \cref{eq:CDR:feat}. 
By applying CMA-ES (discussed thoroughly in \cref{ch:esmodels}) to directly 
minimize the mean $C_{\max}$  w.r.t. the weights $\vec{w}$, gave a considerably 
more favourable result in predicting optimal versus suboptimal dispatching 
paths. 
So the question put forth is, why was the ordinal regression not able to detect 
it?
The nature of the CMA-ES is to explore suboptimal routes until it converges to 
an optimal one. 
Implying that the previous approach of only looking into one optimal route is 
not sufficient information. 
Suggesting that the preference set should incorporate a more complete knowledge 
about \emph{all} possible preferences, i.e., make also the distinction between 
suboptimal and sub-suboptimal features, etc.  
This would require a Pareto ranking for the job-list, $\mathcal{L}$, which can 
be used to make the distinction to which feature sets are equivalent, better or 
worse, and to what degree (i.e. giving a weight to the preference)? 
By doing so, the preference set becomes much greater, which of course would 
again need to be sampled in order to be computationally feasible to learn. 

For instance \cite{Siggi05} used decision trees to `rediscover' LPT by using 
the dispatching rule to create its training data. The limitations of using 
heuristics to label the training data is that the learning algorithm will mimic 
the original heuristic (both when it works poorly and well on the problem 
instances) and does not consider the real optimum. In order to learn new 
heuristics that can outperform existing heuristics then the training data needs 
to be correctly labelled. This drawback is confronted in 
\citep{Malik08,Russell09,Siggi10} by using an optimal scheduler, computed 
off-line. 

All problem instances are correctly labelled w.r.t. their optimum makespan, 
found with analytical means.\footnote{Optimal solution were found using 
  \cite{gurobi}, a commercial software package for solving large-scale linear 
  optimisation and a state-of-the-art solver for mixed integer programming.} 
The main motivation for the data generation of $\Psi$ that will be used in 
preference learning, will now need to consider the following main aspects
\begin{enumerate}[after={{}}, leftmargin=*, label={\textbf{PREF.\arabic*}}, 
ref={{PREF.\arabic*}}]
    \item Which path(s) should be investigated to collect training instances, 
    i.e., $\Phi$. Should they be features gathered resulting in
    \label{PREF:param:tracks}
    \begin{enumerate*}[label=\textit{\roman*)},before=\unskip{: }, itemjoin={{? 
    }}, itemjoin*={{, or }},after={{? }}]
      \item optimal solutions (querying expert policy $\pi_\star$)
      \item sub-optimal solutions when a DR is implemented (following a fixed 
      policy $\pi$)
      \item combination of both
    \end{enumerate*}
    \item What sort of rankings should be compared during each step?
    \label{PREF:param:ranks}
    \item What sort of stepwise sampling strategy is needed for a good
    \emph{single} time independent model?
    \label{PREF:param:sampling}
\end{enumerate}
The collection of the training set $\Phi$ in \ref{PREF:param:tracks} (which is 
described in \cref{ch:gentrdat}) is of paramount of importance, as the 
subsequent preference pairs in $\Psi$ are highly dependent on the quality of 
$\Phi$. 
Since the labelling of $\Phi$ is quite computationally intensive, its 
collection should be done parsimoniously in order to not waste valuable time 
and resources. 
On the other hand, \ref{PREF:param:ranks} and \ref{PREF:param:sampling} are 
easy to inspect, once $\Phi$ has been chosen.
The followingÂ \lcnamecref{sec:trdat:param:ranks}s will try to address these 
research questions. 

\section{Scalability of \dr s}\label{sec:pref:scalability}

In \cref{InRu11a} a separate data set was deliberately created for each 
dispatch iterations, as the initial feeling is that dispatch rules used in the 
beginning of the schedule building process may not necessarily be the same as 
in the middle or end of the schedule. As a result there are $K$ linear 
scheduling rules for solving a $n \times m$ \jsp. 
Now, if we were to create a global rule, then there would have to be one 
model for all dispatches iterations. The approach in \cref{InRu11a} was to take 
the mean weight for all stepwise linear models, i.e., 
$\bar{w}_i=\frac{1}{K}\sum_{k=1}^K w_i^{(k)}$ where $\vec{w}^{(k)}$ is 
the linear weight resulting from learning preference set $\Psi^{(k)}$ at 
dispatch $k$. 

A more sophisticated way, would be to create a \emph{new} linear model, where 
the preference set, $\Psi$, is the aggregation  of all preference pairs across 
the $K$ dispatches. 
This would amount to a substantial training set, and for $\Psi$ to 
be computationally feasible to learn, $\Psi$ has to be filtered to size 
$l_{\max}$. The default set-up will be, 
\begin{equation}\label{eq:lmax}
l_{\max} := \Bigg \{ \begin{array}{rccc} 
5 \cdot 10^5 & \quad\text{for} & 10\times 10 & \text{\JSP} \\
10^5 & \quad\text{for} & 6\times 5 & \text{\JSP}
\end{array}
\end{equation}
which is roughly 60\%-70\% amount of preferences encountered from one pass of 
sampling a \mbox{$K$-stepped} trajectory using a fixed policy $\hat{\pi}$ for 
the default $N_{\text{train}}$ (cf. \cref{tbl:size:Psi:K}). 
Sampling is done randomly, with equal probability.

\section{Ranking strategies}\label{sec:trdat:param:ranks}
First let's address \ref{PREF:param:ranks}. 
The various ranking strategies for adding preference pairs to $\Psi$ defined by 
\cref{eq:Psi:jsp} were first reported in \cref{InRu15a}, and are the following,
\begin{description}
    \item[Basic ranking, $\Psi_b$,] i.e., all optimum rankings $r_1$ versus all 
    possible suboptimum rankings $r_i$, $i\in\{2,\ldots,n'\}$, preference pairs 
    are added -- same basic set-up introduced in \cref{InRu11a}. Note, 
    $\abs{\Psi_b}$ is defined in \cref{eq:sizePsi_b}.
    \item[Full subsequent rankings, $\Psi_f$,] i.e., all possible combinations 
    of $r_i$ and $r_{i+1}$ for $i\in\{1,\ldots,n'\}$, preference pairs are 
    added.
    \item[Partial subsequent rankings, $\Psi_p$,] i.e., sufficient set of 
    combinations of $r_i$ and $r_{i+1}$ for $i\in\{1,\ldots,n'\}$, are added to 
    the training set -- e.g. in the cases that there are more than one 
    operation with the same ranking, only one of that rank is needed to 
    compared to the subsequent rank. Note that $\Psi_p\subset \Psi_f$.
    \item[All rankings, $\Psi_a$,] denotes that all possible rankings were 
    explored, i.e.,
    $r_i$ versus $r_j$ for $i,j\in\{1,\ldots,n'\}$ and $i\neq j$, preference 
    pairs are added.
\end{description}
where $r_1>r_2>\ldots>r_{n'}$ ($n'\leq n$) are the rankings of the job-list, 
$\mathcal{L}^{(k)}$, at time step $k$.
By definition the following property holds:
\begin{equation}\label{eq:Psi:size}
    \Psi_p \subset \Psi_f \subset \Psi_b \subset \Psi_a
\end{equation}

\subsection*{Experiments for \ref{PREF:param:ranks}}
To test the validity of different ranking strategies, a training set of 
$N_{\text{train}}=500$ problem instances of \jrnd{6}{5} and \frnd{6}{5}
is collected for all trajectories described in \cref{sec:trdat:tracks}. 
The size of the preference set, $\abs{\Psi}$, is depicted in 
\cref{fig:size:Psi:K} for each iteration $k$. \Cref{tbl:size:Psi:K} reports the 
total amount of preferences for all $K$ dispatches.
From which, a linear preference model is created for each preference 
set, $\Psi$. A box-plot for \fullnamerho, is presented in 
\cref{fig:boxplot:prefset}. 
From the figure it is apparent there can be a performance edge gained by 
implementing a particular trajectory strategy, yet ranking scheme seems to be 
irrelevant. Moreover, the behaviour is analogous across all other 
\Problem[6\times5]{\text{train}} in \cref{tbl:data}.

There is no statistical difference between $\Psi_f$ and $\Psi_p$ 
ranking-schemes across all disciplines,
which is expected since $\Psi_f$ is designed to contain the same preference 
information as $\Psi_f$ (cf. \cref{eq:Psi:size}). 
However, neither of the Pareto ranking-schemes outperform the original $\Psi_b$ 
set-up from \cref{InRu11a}. 
The results hold for the test set as well. 
Any statistical difference between ranking schemes were for $\Psi_a$, where it 
was considered slightly lacking than some of its counterparts. 
Since a smaller preference set is preferred, its opted to use the $\Psi_{p}$ 
ranking scheme henceforth as the default set-up for \ref{PREF:param:ranks}. 

\input{tables/size.prefdat.tex}

\begin{figure}[t]
  \includegraphics[width=\textwidth]{{prefdat.size.6x5}.pdf}
  \vspace*{-30pt}
  \caption[Size of preference set, $\abs{\Psi}$]{Size of 
  \Problem[6\times5]{\text{train}} preference set, $l=\abs{\Psi}$, for 
  different trajectory strategies and ranking schemes (where 
  $N_{\text{train}}=500$) }
  \label{fig:size:Psi:K}
\end{figure}

\begin{figure}[p]
  \includegraphics[width=\textwidth]{{prefdat.boxplot.6x5}.pdf}
  \vspace*{-30pt}
  \caption{Box-plot for various $\Phi$ and $\Psi$ set-up using
    \Problem[6\times5]{\text{train}}. The trajectories 
    the models are based on are depicted in white on the far right.}
  \label{fig:boxplot:prefset}
\end{figure}

\subsection*{Preliminary results for \ref{PREF:param:tracks}}

Limiting ourselves to models corresponding to $\Phi_p$ in 
\cref{fig:boxplot:prefset}. We'd like to inspect which trajectory is the best 
to use for $\Psi$. \Cref{InRu15a} only considered \jrnd{6}{5} and \jrndn{6}{5}, 
however, results for \Problem[6\times5]{\text{train}} and 
\jrnd{10}{10} is currently available.\footnote{Additional box-plots can be 
found in \shiny: Preference Models $>$ Trajectories \& ranks.}

are now ordered w.r.t. their mean \namerho, and is 
given in \cref{tbl:param:tracks}. 
Models that are statistically better are denoted by `$\succ$' (read left to 
right), otherwise considered equivalent. 


\input{tables/PREF.track.ordering.tex}

Learning preference pairs from a good scheduling policies, such as 
$\Phi^{\minCmax},~\Phi^{\minRho}$ and \PhiSet{MWR}, gave considerably more 
favourable results than tracking optimal paths, save for \fjc{6}{5} where the 
ordering is reversed. Generally, suboptimal routes are preferred. 

It is particularly interesting there is statistical difference between 
\PhiSet{OPT} and \PhiSet{RND}, where the latter had improved performance for 
all \JSP\ problem spaces. In those cases, tracking optimal dispatches gives 
worse performance as pursuing completely random dispatches. 
This indicates that exploring only expert policy can result in a 
training set which the learning algorithm is inept to determine good dispatches 
in the circumstances when newly encountered features have diverged from the 
learned feature set labelled to optimum solutions. 

Generally, adding suboptimal trajectories with the optimal trajectory, i.e., 
\PhiSet{ALL},  gives the learning algorithm a greater variety of preference 
pairs for getting out of local minima. However, for some problem spaces, e.g., 
\frnd{6}{5} and \fmc{6}{5} then additional sub-optimal solutions that are too 
diverse, yield a worse outcome than \PhiSet{OPT} on its own.

\subsubsection{Following CMA-ES guided trajectory}
The rational for using the $\Psi^{\langle \text{CMA-ES} \rangle}$ strategies 
was mostly due to the fact a linear classifier is creating the training data 
(using the weights found via CMA-ES optimisation in \cref{eq:cma:objfun}), 
hence the training data created should be linearly separable, which in turn 
should boost the training accuracy for a linear classification learning model. 
However, this strategy is easily outperformed by the \sdr\ MWR guiding the 
training data collection, \PhiSet{MWR}. \todoFind{Is this still accurate?}

\subsection{Summary and conclusion}
As the experimental results showed in \cref{sec:trdat:param:expr}, the ranking 
of optimal\footnote{Here the tasks labelled `optimal' do not necessarily yield 
    the optimum makespan (except in the case of following expert policy 
    $\pi_\star$), instead these are the optimal dispatches for the given 
    partial schedule.} 
and suboptimal features are of paramount importance. The subsequent rankings 
are not of much value, since they are disregarded anyway. However, the 
trajectories to create training instances have to be varied.

Unlike \citep{Siggi10,Malik08,Russell09}, learning only on optimal training 
data was not fruitful. However, inspired by the original work by 
\cite{Siggi05}, having DR guide the generation of training data (except 
correctly labelling with analytic means) gave meaningful preference pairs which 
the learning algorithm could learn. In conclusion, henceforth, the training 
data will be generate with $\Psi_{ b}^{all}$ scheme.





\todoWrite{No longer the case, one model instead of $K$ stepwise-models}

\section{Discussion and conclusions}



\section{JOH.tex}

\subsection{Feature Selection}
The SDRs we've inspected so-far are based on two job-attributes from
\cref{tbl:jssp:feat}, namely
\begin{enumerate*}[after={{,}}]
  \item \phiproc\ for SPT and LPT 
  \item \phijobWrm\ for LWR and MWR 
\end{enumerate*}
by choosing the lowest value for SPT and LWR, and highest value for LPT and 
MWR, i.e., the extremal values for those attributes. 
There is nothing that limits us to using just only these two attributes. 

For this study we will consider all combinations of attributes using either one,
two, three or all $d$ of them, for a total of
\begin{equation}
{d \choose 1}+{d \choose 2}+{d \choose 3}+{d \choose d} 
\stackrel{d=\NrFeatLocal}{=} 697
\end{equation}
The reason for such a limiting number of active attributes,
are due to the fact we want to keep the models simple enough for improved model
interpretability.

For each feature combination, a linear preference model is created, where 
$\Phi$ is limited to the predetermined feature combination. 
This was done with the software package from
\cite{liblinear},\footnote{Software available at
  \url{http://www.csie.ntu.edu.tw/~cjlin/liblinear}}
by training on the full preference set $\Psi$ obtained from 
\mbox{$N_{\text{train}}=300$} problem instances following the framework set up 
in \cref{sec:liblinear}. 
Note, in order to report the validation accuracy, 20\% ($N_{\text{val}}=60$) of 
the training set was set aside for validation of reporting the accuracy.

\subsection{Validation accuracy}\label{sec:CDR:acc}
As the preference set $\Psi$ has both preference pairs belonging to optimal
ranking, and subsequent rankings, it is not of primary importance to classify
\emph{all} rankings correctly, just the optimal ones. Therefore, instead of
reporting the validation accuracy based on the classification problem of the
correctly labelling the problem set $\Psi$, it's opted the validation accuracy 
is
obtained in the same manner as done in \cref{sec:opt:sdr} for SDRs, i.e., the
probability of choosing optimal decision given the resulting linear weights,
however, in this context, the mean throughout the dispatching process is
reported. \Cref{fig:stepwise_vs_classification} shows the difference between
the two measures of reporting validation accuracy. Validation accuracy based on
stepwise optimality only takes into consideration the likelihood of choosing
the optimal move at each time step. However, the classification accuracy is
also trying to correctly distinguish all subsequent rankings in addition of
choosing the optimal move, as expected that measure is considerably lower. 

\begin{figure}[th!]
  \centering
  \includegraphics[width=\linewidth]{figures/{training.accuracy}.pdf}
  \caption{Various methods of reporting validation accuracy for preference 
    learning}
  \label{fig:stepwise_vs_classification}
\end{figure}

\subsection{Pareto front}\label{sec:CDR:pareto}
When training the learning model one wants to keep the validation accuracy 
high, as that would imply a higher likelihood of making optimal decisions, 
which would in turn translate into a low final makespan. To test the validity 
of this assumptions, each of the 697 models is run on the preference set, and 
its mean $\rho$ is reported against its corresponding validation accuracy in 
\cref{fig:CDR:scatter}. The models are colour-coded w.r.t. the number of active 
features, and a line is drawn through its Pareto front. Moreover, those 
solutions are labelled with their corresponding model ID. Moreover, the Pareto 
front over all 697 models, irrespective of active feature count, is denoted 
with triangles. Moreover, their values are reported in \cref{tbl:CDR:pareto}, 
where the best objective is given in boldface. 

\begin{table}
  \caption{Mean validation accuracy and mean expected deviation from 
    optimality, $\rho$, for all CDR models on the Pareto front from 
    \cref{fig:CDR:scatter}.}\label{tbl:CDR:pareto}
  \input{tables/PREF-equal}
\end{table}

\Cref{eq:jssp:linweights} showed how to interpret the linear preference models 
by their weights $\vec{w}$. \Cref{fig:CDR:weights} depicts $\vec{w}$ for all of 
the CDR models reported in \cref{tbl:CDR:pareto}. 
The weights have been normalised for clarity purposes, such that it is scaled 
to $\norm{\vec{w}}=1$, thereby giving each feature their proportional 
contribution to the preference $I_j^{\pi}$ defined by \cref{eq:CDR}. 
These weights will now be explored further, along with testing whether models 
are statistically significant to one another, using a 
Kolmogorov-Smirnov test with $\alpha=0.05$.

For \jrnd{10}{10}  there is no statistical difference between models (2.69, 
3.355, 3.358, 3.524), w.r.t. $\rho$ and the latter three w.r.t. 
accuracy. These models are therefore equivalently `best' for the problem space.
As \cref{fig:CDR:weights} shows, \phiendTime, \phijobWrm\ and \phimacWrm\ are 
similar in value, and in the case of 3.358, then \phimacFree\ has similar 
contribution as \phiendTime\ for the other models. 
Which, as standalone models are 1.6 and 1.3, respectively, and yield 
equivalent mean $\rho$ and accuracy.
As these features often coincide in \jsp\, it is justifiable to use only 
either one, as the it contains the same information as its 
counterpart.\footnote{Note, \phiendTime$~\leq~$\phimacFree, where
  \phiendTime$~=~$\phimacFree\ when $J_j$ is the latest job on $M_a$, 
  otherwise $J_j$ is placed in a previously created slot on $M_a$.}
Most likely, the equivalence of these features is indicating that the 
schedules are rarely able to dispatch in earlier slots, i.e., 
\phiendTime$~\approx~$\phimacFree. 

In addition, (2.111, 3.300) and (16.1, 3.355) are statistically insignificant 
w.r.t. validation accuracy for \jrnd{10}{10}. However, they have considerable 
performance difference w.r.t. $\rho$ ($\Delta\rho \approx 18\%$). 
So even looking at stepwise optimality by itself is very fickle, because slight 
variations can be quite dramatic to the end result. 

The solutions on the Pareto front for \jrndn{10}{10} are quite a few models
with no (or minimal) statistical difference w.r.t. $\rho$, and 
considerably more w.r.t. validation accuracy. 
Most notably are (3.281, 2.73, 2.75, 1.13), 
(note, first one has the lowest mean $\rho$) which are all statistically 
insignificant w.r.t. validation accuracy yet none w.r.t. $\rho$, with 
difference up to $\Delta\rho=6.32\%$.

For \frnd{10}{10} almost all models are statistically different w.r.t. $\rho$, 
only exception is (1.6, 1.7).
Although, w.r.t. validation accuracy, there are a few equivalent models, 
namely, (3.151, 2.51), (2.94, 1.6) and (3.216, 3.213, 16.1), with $1.2\%$, 
$2.3\%$ and $5.75\%$ difference in mean $\rho$, respectively. 

It's interesting to inspect the full model for \frnd{10}{10}, 16.1. 
Despite having similar contributions, yet statistically significantly 
different, as all the active features as (3.213, 3.216), then the (slight) 
interference from of other features, hinders the full model from achieving a 
low $\rho$. 
Only considering \phijobOps\ and \phimacOps\ with either \phiendTime\ and 
\phimacFree, boosts performance by 5.28\% and 5.72\%, respectively. 
Thereby stressing the importance of feature selection, to steer clear of 
over-fitting. Note, unlike \jrnd{10}{10}, now \phiendTime\ differs from 
\phimacFree, indicating that there are some slots created, which could be 
better utilised.
Moreover, looking at model 2.111 for \frnd{10}{10}, which has similar 
contributions as the best model, 3.539. Then introducing a third feature, 
\phimacWrm, is the key to the success of the CDR, with a boost of $\rho$ 
performance by 1.33\%. 

Note, for both \jrnd{10}{10} and \jrndn{10}{10}, model 1.13 is on the Pareto 
front. The model corresponds to feature \phijobWrm, and in both cases has a 
weight strictly greater than zero (cf. \cref{fig:CDR:weights}). Revisiting 
\cref{eq:jssp:linweights}, we observe that this implies the learning 
model was able to discover MWR as one of the Pareto solutions, and as is 
expected, there is no statistical difference to between 1.13 and MWR.

As one can see from \cref{fig:CDR:scatter}, adding additional features to 
express the linear model boosts performance in both validation accuracy and 
expected mean for $\rho$, i.e., the Pareto fronts are cascading towards more 
desirable outcome with higher number  of active features. However, there is a 
cut-off point for such improvement, as using all features is generally 
considerably worse off due to overfitting of classifying the preference set.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/{pareto.rho.vs.acc}.pdf}
  \caption{Scatter plot for validation accuracy  (\%) against its 
    corresponding mean expected $\rho$ (\%) for all 697 linear models, 
    based on either one, two, three or all $d$ combinations of features.
    Pareto fronts for each active feature count based on maximum validation 
    accuracy and minimum mean expected $\rho$ (\%), and labelled with their 
    model ID. Moreover, actual Pareto front over all models is marked with 
    triangles.} \label{fig:CDR:scatter}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{figures/{j.rnd}/{pareto.10x10.phi}.pdf}\\
  \includegraphics[width=\textwidth]{figures/{j.rndn}/{pareto.10x10.phi}.pdf}\\
  \includegraphics[width=\textwidth]{figures/{f.rnd}/{pareto.10x10.phi}.pdf}
  \caption{Normalised weights for CDR models from \cref{tbl:CDR:pareto}, 
    models are grouped w.r.t. its dimensionality, $d$. Note, a triangle 
    indicates a solution on the Pareto front.}\label{fig:CDR:weights}
\end{figure}

Now, let's inspect the models corresponding to the minimum mean $\rho$ and 
highest mean validation accuracy, highlighted in \cref{tbl:CDR:pareto} and 
inspect the stepwise optimality for those models in \cref{fig:CDR:opt}, again 
using probability of randomly guessing an optimal move from \cref{fig:opt:SDR} 
(denoted RND) 
as a benchmark.
As one can see for both \jrnd{10}{10} and \jrndn{10}{10}, despite having a 
higher mean validation accuracy overall, the probabilities vary significantly. 
A lower mean $\rho$ is obtained when the validation accuracy is gradually 
increasing over time, and especially during the last phase of the 
scheduling.\footnote{It's almost illegible to notice this shift directly 
  from \cref{fig:CDR:opt}, as the difference between the two best models is 
  oscillating up to only 3\% at any given step. In fact \jrndn{10}{10} has 
  the most clear difference w.r.t. classification accuracy of indicating when 
  a minimum $\rho$ model excels at choosing the preferred move.} 
Revisiting \cref{fig:case}, this trend indicates that it's likelier for the 
resulting makespan to be considerably worse off if suboptimal moves are made at 
later stages, than at earlier stages. Therefore, it's imperative to make the 
`best' decision at the `right' moment, not just look at the overall mean 
performance. 
Hence, the measure of validation accuracy as discussed in \cref{sec:CDR:acc} 
should take into consideration the impact a suboptimal move yields on a 
step-by-step basis, e.g., weighted w.r.t. a curve such as depicted in 
\cref{fig:case}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/{stepwise.10x10.FeatSelect}.pdf}
  \caption{Probability of choosing optimal move for models corresponding to 
    highest mean validation accuracy (grey) and lowest mean deviation from 
    optimality, $\rho$, (black) compared to the baseline of probability of 
    choosing an optimal move at random (dashed).}
  \label{fig:CDR:opt}
\end{figure}

Let's revert back to the original SDRs discussed in \cref{sec:opt:sdr} and 
compare the best CDR models, a box-plot for $\rho$ is depicted in 
\cref{fig:boxplot:CDR}. Firstly, there is a statistical difference between all 
models, and  clearly the CDR model corresponding to minimum mean $\rho$ value, 
is the clear winner, and outperforms the  SDRs substantially. However, the best 
model w.r.t. maximum validation accuracy, then the CDR model shows a lacklustre 
performance. In some cases it's better off, e.g., compared to LWR, yet for 
\jsp\ it doesn't surpass the performance of MWR. This implies, the learning 
model is over-fitting the training data. Results hold for the test set. 

\begin{figure}
  \includegraphics[width=1\linewidth]{figures/{boxplotRho.FeatSelect.10x10}.pdf}
  \caption{Box-plot for deviation from optimality, $\rho$, (\%) for the best 
    CDR models (cf. \cref{tbl:CDR:pareto}) and compared against the best SDRs 
    from \cref{sec:opt:sdr}, both for training and test sets.} 
  \label{fig:boxplot:CDR}
\end{figure}

\section{Conclusions}\label{sec:con}
Current literature still hold \sdr s in high regard, 
as they are simple to implement and quite efficient. 
However, they are generally taken for granted as there is clear lack of 
investigation of \emph{how} these \dr s actually work, and what 
makes them so successful (or in some cases unsuccessful)? 
For instance, of the four SDRs this study focuses on, why does MWR outperform 
so significantly for \jsp\, yet completely fail for \fsp? 
MWR seems to be able to adapt to varying distributions of processing times, 
however, manipulating the machine ordering causes MWR to break down. 
By inspecting optimal schedules, and meticulously researching what's going on, 
every step of the way of the dispatching sequence, some light is shed where 
these SDRs vary w.r.t. the problem space at hand. 
Once these simple rules are understood, then it's feasible to extrapolate the 
knowledge gained and create new \cdr s that are likely to be 
successful. 

Creating new \dr s is by no means trivial. For \jsp\ there is 
the hidden interaction between processing times and machine ordering that's 
hard to measure.
Due to this artefact, feature selection is of paramount importance, and then it 
becomes the case of not having too many features, as they are likely to hinder 
generalisation due to over-fitting in training. However, the features need to 
be explanatory enough to maintain predictive ability. 
For this reason \cref{ch:expr:CDR} was limited to up to three active features, 
as the full feature set was clearly suboptimal w.r.t. the SDRs used as a 
benchmark. 
By using features based on the SDRs, along with some additional local features 
describing the current schedule, it was possible to `discover' the SDRs when 
given only one active feature. %Although there is not much to be gained by 
%these models, they at least are a sanity check the learning models are on the 
%right track. 
Furthermore, by adding on additional features, a boost in performance was 
gained, resulting in a \cdr\ that outperformed all of the 
SDR baseline. 

When training the learning model, it's not sufficient to only optimize w.r.t. 
highest mean validation accuracy. As \cref{sec:CDR:pareto} showed, there is a 
trade-off between making the over-all best decisions versus making the right 
decision on crucial time points in the scheduling process, as \cref{fig:case} 
clearly illustrated. It is for this reason, traditional feature selection such 
as add1 and drop1 were unsuccessful in preliminary experiments, and thus 
resorting to having to exhaustively search all feature combinations.
This also opens of the question of how should validation accuracy be measured? 
Since the model is based on learning preferences, both based on optimal versus 
suboptimal, and then varying degrees of suboptimality. As we are only looking 
at the ranks in a black and white fashion, such that the makespans need to be 
strictly greater to belong to a higher rank, then it can be argued that some 
ranks should be grouped together if their makespans are sufficiently close. 
This would simplify the training set, making it (presumably) less of 
contradictions and more appropriate for linear learning. Or simply the 
validation accuracy could be weighted w.r.t. the  difference in 
makespan.
During the dispatching process, there are some pivotal times which need to be 
especially taken care off. \Cref{fig:case} showed how making suboptimal 
decisions were more of a factor during the later stages, whereas for flow-shop 
the case was exact opposite. \todo[inline]{Could discuss new sampling 
  strategies, e.g., proportional to best/worst case, optimality, etc. -- have 
  done some experiments, but not clear what strategy is best, so only equal 
  probability reported}

Despite the abundance of information gathered by following an optimal 
trajectory, the knowledge obtained is not enough by itself. Since the learning 
model isn't perfect, it is bound to make a mistake eventually. When it does, 
the model is in uncharted  territory as there is not certainty the samples 
already collected are able to explain the current situation. For this we 
propose investigating features from suboptimal trajectories as well, since the 
future observations depend on previous predictions. 
A straight forward approach would be to inspect 
the trajectories of promising SDRs or CDRs. 
In fact, it would be worth while to try out imitation learning by 
\cite{RossB10,RossGB11}, such that the learned policy following an optimal 
trajectory is used to collect training data, and the learned model is updated. 
This can be done over several iterations, with the benefit being, that the 
states that are likely to occur in practice are investigated, and as such used 
to dissuade the model from making poor choices. Alas, this comes at great 
computational cost due to the substantial amounts of states that need to be 
optimised for their correct labelling. Making it only practical for \jsp\ of 
a considerable lower dimension. 

Although this study has been structured around the \jsp\ scheduling problem, 
it is easily extended to other types of deterministic optimisation problems 
that involve sequential decision making. 
The framework presented here collects snap-shots of the state space by 
following an optimal trajectory, and verifying the resulting optimal makespan 
from each possible state. 
From which the stepwise optimality of individual features can be inspected, 
which could for instance justify omittance in feature selection. 
\todo[color=pink]{Not done, but possible} 
Moreover, by looking at the best and worst case scenario of suboptimal 
dispatches, it is possible to pinpoint vulnerable times in the scheduling 
process. 

\section{IL.TEX}


\begin{quote}
    %\todo{Lure the reader in a with a good first sentence}
    So a prudent man must always follow in the footsteps of great men and 
    imitate those who have been outstanding. If his own prowess fails to 
    compare with theirs, at least it has an air of greatness about it. 
    
    \raggedleft Niccolo Maachiavelli \cite{Maachiavelli}
\end{quote}
\todo{Eftirleifar galsa. Mun umorda seinna.}
Just as this quote applied to \emph{new principalities acquired with one's own 
    arms and prowess} centuries ago, it equally applies when setting up novel 
supervised learning algorithms. 
When it comes to designing algorithms there needs to be emphasis on where to 
innovate and imitate when visiting state-spaces. 
%\todo{What is the problem?} 
This study will show, that when using these guidelines when accumulating 
training data for supervised  learning, it's possible to automate its 
generation in such a way that the resulting model will be an accurate 
representative of the instances it will later come across. 

For this purpose, the \JSP\ is used as a case study 
to illustrate a methodology for generating meaningful training set 
autonomously, which can be successfully learned using preference-based 
imitation learning (IL).

The approach was to use supervised learning to determine which feature states 
are preferable to others. 

The training data from \cref{InRu11a} was created from optimal solutions of 
randomly generated problem instances, i.e., traditional \emph{passive} 
imitation learning (IL). 
As \JSP\ is a sequential decision making process, errors are bound to emerge.  
Due to compound effect of making suboptimal dispatches, the model leads the 
schedule astray from learned state-spaces, resulting in the new input being 
foreign to the learned model. 

Alternatively, training data could be generated using suboptimal solution 
trajectories as well, as was done in \cref{InRu15a}, where the training data 
also incorporated following the trajectories obtained by applying successful 
SDRs from the literature. 
The reasoning behind it was that  they would be beneficial for learning, 
as they might help the model to escape from local minima once off the coveted 
optimal path. 
By simply adding training data obtained by following the trajectories of 
well-known SDRs, their aggregated training set yielded better models with lower 
\namerho.


%\todo{What are   your contributions?}
Inspired by the work of \cite{RossB10,RossGB11}, the methodology of generating 
training data will now be such that it will iteratively improve upon the model, 
such that the state-spaces learned will be representative of the state-spaces 
the eventual model would likely encounter, known as DAgger for imitation 
learning.
%\todo{Why is it interesting?} 
Thereby, eliminating the ad-hoc nature of choosing trajectories to learn, by 
rather letting the model lead its own way in a self-perpetuating manner until 
it converges.

%\todo{What is the outline of what you will show?}  
The outline of the paper is the following, \cref{sec:background} will define 
formalism for \JSP, and \cref{sec:il,sec:expertPolicy,sec:perturbedLeader} 
explain the trajectories for sampling meaningful schedule state-spaces used in 
preference learning, 
along with some general adjustments for performance boost in \cref{sec:ext}.
Followed by experimental results in \cref{sec:expr} with comparison for several 
randomly generated problem spaces. 
The paper finally concludes in \cref{sec:con} with discussion and conclusions.

\section{Passive Imitation Learning}\label{sec:il:passive}
Using the terms from game-theory used in \cite{CesaBianchi06}, % chapter 2: 
then our problem is a basic version of the sequential prediction problem where 
the predictor (or forecaster), $\pi$, observes each element of a sequence 
$\vchi$ of jobs, where at each time step $k \in \{1,...,K\}$, before the 
$k$-th job of the sequence is revealed, the predictor guesses its value 
$\chi_k$ on the basis of the previous $k-1$ observations. 

\subsection{Prediction with Expert Advice}\label{sec:expertPolicy}
Let's assume we know the expert policy $\pi^\star$, which we can query what 
is the optimal choice of $\chi_k={j^*}$ at any given time step $k$. 
Now we can use \cref{eq:jstar} to back-propagate the relationship between 
post-decision states and $\hat{\pi}$ with preference learning via our collected 
feature set, denoted $\Phi^\text{OPT}$, i.e., we collect the features set 
corresponding following optimal tasks $J_{j^*}$ from $\pi^\star$ in 
\cref{pseudo:constructJSP}.
This baseline trajectory sampling for adding features to the feature set 
is a pure strategy where at each dispatch, an optimal task was originally 
introduced in \cite{InRu11a} and explored further in \cite{InRu15b}. 

By querying the expert policy, $\pi_\star$, the ranking of the job-list, 
$\mathcal{L}$, is determined such that,
\begin{equation}
r_1 \succ r_2 \succ \cdots \succ r_{n'} \quad (n' \leq n)
\end{equation}
implies $r_1$  is preferable to $r_2$, and $r_2$ is preferable to $r_3$, etc. 
In  our study, we know $r \propto C_{\max}^{\pi_\star}$, hence the optimal 
job-list is the following, 
\begin{equation}
\mathcal{O}=\condset{r_i}{r_i \propto \min_{J_j \in \mathcal{L}} 
    C_{\max}^{\pi_\star(\vchi^j)}}
\end{equation}
found by solving the current partial schedule to optimality using a 
commercial software package such as \cite{gurobi}. 

When $\abs{\mathcal{O}^{(k)}}>1$, there can be several trajectories worth 
exploring. However, only one is chosen at random. This is deemed sufficient as 
the number of problem instances, $N_{\text{train}}$, is relatively large.

Experiments in \cite{InRu15b} clearly showed that following the expert policy 
is not without its faults. There are many obstacles to consider to improve the 
model. For instance, their experiments $\Psi$ to size $l$ with equal 
probability. But inspecting the effects of making suboptimal choices varies as 
a function of times steps, perhaps its stepwise bias should rather be done 
proportional to the mean cumulative loss to a particular time step?

\todoWrite{
The sampling strategy for $\Psi$ in \cite{InRu11a} was \ref{bias:equal} 
and serves as a baseline. \ref{bias:opt} motivation is to give 
samples from dispatches that are less than likely to be optimal than simply at 
random (cf. \cref{fig:opt:SDR}). On the other hand, \ref{bias:bcs} and 
\ref{bias:wcs} are more focused on sampling w.r.t. the final measure, where the 
mean $\rho$ is given \emph{one} suboptimal move, otherwise it's assumed expert 
policy is followed (cf. \cref{fig:case}). 
Lastly, \ref{bias:dbl1st} and \ref{bias:dbl2nd} are very simplified versions of 
the aforementioned strategies, depending on the problem space at hand. 
\Cref{fig:bias} depicts the stepwise bias strategies for the problem spaces in 
\cref{tbl:sim}.
}
Following strategies for stepwise bias will be proportional to
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Bias.\arabic*}}, ref={{Bias.\arabic*}}]
    \item \label{bias:equal} equal probability (equal) 
    \item \label{bias:opt} inverse optimality for random dispatches (opt) 
    \item \label{bias:bcs} best case scenario for mean $\rho$ (bcs)
    \item \label{bias:wcs} worst case scenario for mean $\rho$ (wcs)
    \item \label{bias:dbl1st} twice as much weight on the first half of the 
    dispatches (dbl1st) 
    \item \label{bias:dbl2nd} twice as much weight on the second half of the 
    dispatches (dbl2nd)
\end{enumerate}
The reader is referred to \cite{InRu15b} for a detailed explanation of how to 
obtain the reference values for \ref{bias:opt} to \ref{bias:wcs}.
The sampling strategy for $\Psi$ in \cite{InRu11a,InRu15b} was \ref{bias:equal} 
and serves as a baseline. \ref{bias:opt} motivation is to give 
samples from dispatches that are less than likely to be optimal than simply at 
random. On the other hand, \ref{bias:bcs} and \ref{bias:wcs} are more focused 
on sampling w.r.t. the final measure, where the mean $\rho$ is given \emph{one} 
suboptimal move, otherwise it's assumed expert policy is followed. 
Lastly, \ref{bias:dbl1st} and \ref{bias:dbl2nd} are very simplified versions of 
the aforementioned strategies, depending on the problem space at hand. 
\Cref{fig:bias} depicts the stepwise bias strategies for the problem spaces in 
\cref{tbl:sim}.

\subsection{Follow the Perturbed Leader}\label{sec:perturbedLeader}
By allowing a predictor to randomise it's possible to achieve improved 
performance \cite{CesaBianchi06,Hannan57}, which is the inspiration for our new 
strategy, where we follow the Perturbed Leader (PL). 
Its pseudo code is given in \cref{pseudo:perturbedLeader} and describes how the 
expert policy (i.e. optimal trajectory) from \cref{sec:expertPolicy} is subtly
``perturbed'' with $\epsilon=10\%$ likelihood, by choosing a job corresponding 
to the second best $C_{\max}$ instead of a optimal one with some small 
probability. 

\input{pseudocode/perturbedLeader}

\subsection{Summary}

Results showed that the expert policy is a promising starting point. 
However, since \jsp\ is a sequential prediction problem, all future 
observations are dependent on previous operations. 
Therefore, learning sampled states that correspond only to optimal or 
near-optimal schedules isn't of much use when the preference model has 
diverged too far. 
This is due to the learner's predictions affects future input observations 
during its execution, which violates the crucial i.i.d. assumptions of the  
learning approach, and ignoring this interaction leads to poor performance.
In fact, \cite{RossB10} proves, that assuming the preference model has a 
training error of $\epsilon$, then the total compound error (for all $K$ 
dispatches) the classifier induces itself grows quadratically, $\bigOh{\epsilon 
    K^2}$, for the entire schedule, rather than having linear loss, 
$\bigOh{\epsilon K}$, if it were i.i.d.


\section{Active Imitation Learning}\label{sec:il:active}

To amend performance from $\Phi^{\text{OPT}}$-based models, suboptimal 
state-spaces were explored in \cite{InRu15a} by inspecting the features from 
successful SDRs, $\Phi^{\langle\text{SDR}\rangle}$, by passively observing a 
full execution of following the task chosen by the corresponding SDR. 
This required some trial-and-error as the experiments showed that features 
obtained by SDR trajectories were not equally useful for learning.


\todo[inline]{DAgger can be interpreted as Follow-the-leader algorithm in that 
    at iteration $i$ we pick the best policy $\hat{\pi}_i$ in hindsight, i.e., 
    under all trajectories seen so far over the iterations}

``\cite{RossGB11} approach is similar to regularised follow the leader 
algorithm from sec.'' 
In these models the adversary and nature are the same, and the nature chooses a 
new cost function for each action of the learner at the each iteration of the 
game. The goal is to minimise the regret that the learner would suffer, 
compared to the time that if it knew all of the costs imposed by nature in 
hindsight and had chosen a fixed strategy as the response
Ross et  al use a regret minimisation setting for learning to drive a computer 
simulated car, where the output is a sequence of actions in a limited horizon. 
in their problem the true cost of taking action $a$ in state $s$, $C(s,a)$ is 
not 
known, but they use some expert's knowledge about the loss $l(s,\pi)$ incurred 
by the policy $a=\pi(s)$ -- policy is the function $\pi:\mathcal{S}\to 
\mathcal{D}(\mathcal{A})$ that maps an state to an action or a distribution 
over action, and is almost equivalent hypothesis function $h:\mathcal{X}\to 
\mathcal{Y}$



To automate this process, inspiration from \emph{active} imitation learning 
presented in \cite{RossGB11} is sought, called \emph{Dataset Aggregation} 
(DAgger) method, which addresses a no-regret algorithm in an on-line learning 
setting. 
The novel meta-algorithm for IL learns a deterministic policy guaranteed to 
perform well under its induced distribution of states. 
The method is closely related to Follow-the-leader (cf. 
\cref{sec:perturbedLeader}), however, with a more sophisticated leverage to the 
expert policy. 
In short, it entails the model $\pi_i$ that queries an expert policy (same as 
in \cref{sec:expertPolicy}), $\pi_\star$, its trying to mimic, 
but also ensuring the learned model updates itself in an iterative fashion, 
until it converges. 
The benefit of this approach is that the states that are likely to occur in 
practice are also investigated and as such used to dissuade the model from 
making poor choices. In fact, the method queries the expert about the desired 
action at individual post-decision states which are both based on past queries, 
and the learner's interaction with the \emph{current} environment.

DAgger has been proven successful on a variety of benchmarks, such as
\begin{enumerate*}[label={{}}]
    \item the video games Super Tux Kart and Super Mario Bros. or
    handwriting recognition -- in all cases greatly improving traditional 
    supervised imitation learning approaches \cite{RossGB11}
    \item real-world applications, e.g. autonomous navigation for large 
    unmanned 
    aerial vehicles \cite{Ross13}
\end{enumerate*}
To illustrate the effectiveness of DAgger, the Super Mario Bros. experiment 
gives a very simple and informative understanding of the benefits of the 
algorithm. In short, Super Mario Bros. is a platform game where the 
protagonist, Mario, must move across the stage without being hit by enemies or 
falling through gaps within a certain time limit. 
One of the reasons the supervised approaches failed, were due to Mario getting 
stuck up against an obstacle, instead of jumping over it. 
However, the expert would always jump over them at a greater distance 
beforehand, and therefore the learned controller would not know of these 
scenarios. 
With iterative methods, Mario would encounter these problematic situations and 
eventually learn how to get himself unstuck. 

The policy of imitation learning at iteration $i>0$ is a mixed strategy given 
as follows, 
\begin{equation}\label{eq:il}
\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}
% i: ith iteration of imitation learning
% pi_star is expert policy (i.e. optimal)
% pi_i^hat: is pref model from prev. iteration
\end{equation}
where $\pi_\star$ is the expert policy and $\hat{\pi}_{i-1}$ is the learned 
model from the previous iteration. 
Note, for the initial iteration, $i=0$, a pure strategy of $\pi_\star$ is 
followed. Hence, $\hat{\pi}_0$ corresponds to the preference model from 
\cref{sec:expertPolicy} (i.e. $\Phi^{\text{IL}0}=\Phi^{\text{OPT}}$). 

\Cref{eq:il} shows that $\beta$ controls the probability distribution of 
querying the expert policy $\pi_\star$ instead of the previous imitation model, 
$\hat{\pi}_{i-1}$.  
The only requirement for $\{\beta_i\}_i^\infty$ according to \cite{RossGB11} is 
that $\limit{\frac{1}{T}\sum_{i=0}^T\beta_i}{T\to\infty}{0}$ to guarantee 
finding a policy $\hat{\pi}_i$ that achieves $\epsilon$ surrogate loss under 
its own state distribution limit.

%\todo[inline]{Adopting a game-theoretic terminology, we often call an action a 
%    pure strategy and a probability distribution over actions a mixed strategy 
%    -- direct quote from \cite{CesaBianchi06}}

\Cref{pseudo:imitationLearning} explains the pseudo code for how to collect 
partial training set, $\Phi^{\text{IL}i}$ for $i$-th iteration of imitation 
learning.
Subsequently, the resulting preference model, $\hat{\pi}_i$, learns on the 
aggregated datasets from all previous iterations, namely,  
\begin{equation}\label{eq:DAgger}
\Phi^{\text{DA}i}=\bigcup_{i'=0}^{i}\Phi^{\text{IL}i'}
\end{equation}
and its update procedure is detailed in \cref{pseudo:DAgger}.

\input{pseudocode/imitationLearning}
\input{pseudocode/DAgger}

\section{Experiments}\label{sec:expr}

\subsection*{Performance boost}

In order to boost training accuracy, two strategies were explored 
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Boost.\arabic*}}, ref={{Boost.\arabic*}}]
    \item \label{expr:boost:varylmax} increasing number of preferences used 
    in training (i.e. varying \mbox{$l_{\max} \leq \abs{\Psi}$}),
    \item \label{expr:boost:newdata} introducing more problem instances 
    (denoted 
    EXT in experimental setting).
\end{enumerate}
Note, that in preliminary experiments for \ref{expr:boost:varylmax} showed no 
statistical significance in boost of performance, hence $l_{\max}$ is the same 
as set in \cref{eq:lmax}.
However, \ref{expr:boost:newdata} strategy showed a considerable change in 
performance. Summary of $N_{\text{train}}$ is given in \cref{tbl:Ntrain}. 
Note, for the conventional $\Phi^{\text{OPT}}$ trajectory the extended training 
set was simply obtained by iterating over more examples. 
However, for the DAgger trajectories the extended set consisted of each 
iteration encountering $N_{\text{train}}$ \emph{new} problem 
instances. For a grand total of 
\begin{equation}
N^{\text{DA}i}_{\text{train, EXT}}=N_{\text{train}}\cdot (i+1) 
\end{equation}
problem instances explored for the aggregated extended training set used for 
the learning model at iteration $i$.

\input{tables/Ntrain}

\subsection*{DAgger Parameters}
Due to time constraints, only $T=7$ iterations will be inspected.
In addition, there will be three mixed strategies for $\{\beta_i\}_{i=0}^T$ in 
\cref{eq:il} considered
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{DA.\arabic*}}, ref={{DA.\arabic*}}]
    \item \emph{fixed} supervision with $\beta_i=0.5$ save for $\beta_0=1$, 
    \label{expr:ILFIXSUP}
    \item \emph{decreasing supervision} with $\beta_i=0.5^i$, \label{expr:ILSUP}
    \item \emph{unsupervised} with $\beta_i=I(i=0)$, where $I$ is the indicator 
    function.\footnote{$\beta_0=1$ and $\beta_i=0,\forall i>0$.} 
    \label{expr:ILUNSUP}
\end{enumerate}
Note, \ref{expr:ILSUP} starts as \ref{expr:ILFIXSUP} and decays exponentially 
towards \ref{expr:ILUNSUP}.
Moreover, \ref{expr:ILUNSUP} is a simple parameter-free version of the DAgger 
algorithm and often performs best in practice \cite{RossGB11}.

\begin{figure}
    \subcaptionbox{$6\times5$}{\includegraphics[width=\textwidth]{figures/{j.rnd}/{DAGGER.6x5}.pdf}}\\
    \subcaptionbox{$10\times10$}{\includegraphics[width=\textwidth]{figures/{j.rnd}/{DAGGER.10x10}.pdf}}
    \caption{Box plot for deviation from optimality, $\rho$, (\%) using either 
        imitation learning or following perturbed leader 
        strategies.}\label{fig:il} 
\end{figure}

\section{Conclusions}\label{sec:con}
\todo[inline]{"What general lessons might be learnt from this study?"}
DAgger for \jsp\ is not sensitive to choice of $\beta_i$ in \cref{eq:il}.
\todo[inline]{Place work in wider context}

\todo[inline,color=red]{``Training such a predictor, however, is non-trivial as 
    the interdependencies in the sequence of predictions make global 
    optimisation is to leverage information local to modules to aid learning 
    ... To provide good guarentees and performance in practice in this 
    non-i.i.d. (as predictions are interdependent), we also leverage key 
    iterative training methods developped in prior work for imitation learning 
    and structured prediction''}


\todo[inline,color=green]{George Bernard Shaw once said: ``Imitation is not 
    just the sincerest form of flattery -- it's the sincerest form of 
    learning.''}

\todo[inline]{Future Work: \cite{Judah12} Unfortunately [DAgger] query the 
    expert quite aggressively making them impractical for human experts. In 
    contrast, [Reduction-based Active Imitation Learning (RAIL)] focuses on 
    active querying for the purpose of minimizing the expert's labelling 
    effort. Like our work, they also require a dynamics simulator to help 
    select queries }

Maximum Mean Discrepancy (MMD) imitation learning by \cite{Kim13} is an 
iterative algorithm similar to DAgger. 
However, the expert policy is only queried when needed in order to reduce 
computational cost. 
This occurs when a metric of a new state is sufficiently large enough from a 
previously queried states (to ensure diversity of learned optimal states). 
Moreover, in DAgger all data samples are equally important, irrespective of its 
iteration, which can require great number of iterations to learn how to recover 
from the mistakes of earlier policies. To address the naivety of the data 
aggregation, MMD suggests only aggregating a new data point 
if it is sufficiently different to previously gathered states, \emph{and} if 
the current policy has made a mistake. 
Additionally, there are multiple policies, each specializing in a particular 
region of the state space where previous policies made mistakes.
Although MMD has better empirical performance (based on robot applications), it 
requires defining metrics, which in the case of \jsp\ is non-trivial (cf. 
\cite{InRu12}), and fine-tuning thresholds etc., whereas DAgger can be 
straightforwardly implemented, parameter-free and obtains competitive results, 
although with some computational overhead due to excess expert queries. 

In fact, it's possible to circumvent querying the expert altogether and still 
have reasonable performance. By applying Locally Optimal Learning to Search 
(LOLS) \cite{ChangKADL15} it is possible to use imitation learning (similar to 
DAgger framework) when the reference policy is poor (i.e. $\pi_\star$ in 
\cref{eq:il} is suboptimal), 
although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 

