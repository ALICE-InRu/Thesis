\HeaderQuote{Begin at the beginning and go on till you come to the end: then stop.}{The King}


\chapter{Introduction}\label{ch:introduction} 
%\section{Motivation and research objectives}

\FirstSentence{H}{and crafting heuristics} for NP-hard problems is a time consuming trial-and-error process, requiring inductive reasoning or problem specific insights from their human designers. Furthermore, within a problems class, such as scheduling, it is possible to construct problem instances where one heuristic would outperform another. 

Depending on the underlying data distribution of the problem instances, each 
heuristics perform distinctly. This is due to the fact that any algorithm which 
has superior performance in one class of problems is inevitably inferior over 
another class, i.e., \emph{no free lunch} theorem \citep{Wolpert97nofree}. 
%  number of “no free lunch” (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. 
The success of a heuristic is how it manages to deal with and manipulate the 
characteristics of its given problem instance. Thus, in order to understand 
more fully how a heuristic will eventually perform, one needs to look into what 
kind of problem instances are being introduced to the system? What defines a 
problem instance, e.g., what are its key features? And how can they help with 
designing better heuristics? Once the problem instances are fully understood, 
an appropriate learning algorithm can be implemented in order to create 
heuristics that are  self-adapting to its problem instances.

Given the ad-hoc nature of the heuristic design process there is clearly room 
for improvement. A number of attempts have been made to automate the heuristic 
design process, and it is the ultimate goal of this dissertation to automate 
optimisation heuristics via ordinal regression. 
The focal point will be based on a scheduling processes named \jsp\ scheduling 
problem (\JSP), and one of its subclass, namely, \fsp\ scheduling problem 
(\FSP).
There are two main viewpoints on how to approach scheduling problems, namely,
\begin{description}
  \item[Local level] by building schedules for one problem instance at a time.
  \item[Global level] by building schedules for all problem instances at once.
\end{description}
For local level construction
\begin{enumerate*}[label={{}}]
  \item a simple construction heuristic is applied 
  \item the schedule's features are collected at each dispatch iteration
  \item from which a learning model will inspect the feature set to 
  discriminate which operations are preferred to others via ordinal regression 
\end{enumerate*}
The focus is essentially on creating a meaningful preference set composed of 
features and their ranks, as the learning algorithm is only run \emph{once} to 
find suitable operators for the value function. 
However, for global level construction, there is no feature set collected 
beforehand since the learning model is optimised directly via evolutionary 
search. This requires numerous costly value function evaluations. 
In fact it involves an indirect method of evaluation whether one learning model 
is preferable to another w.r.t. which one yields the better expected mean. 
Evolutionary search only requires the rank of the candidates, and therefore it 
is appropriate to retain a sufficiently accurate surrogate for the value 
function during evolution in order to reduce the number of costly true value 
function evaluations. 
In this paradigm, ordinal regression can be used for surrogate assisted 
evolutionary optimisation, where models are ranked -- whereas on local level 
features were ranked. 

%\section{Relationship between problem structure and heuristic efficiency}
\section{Rice's framework for algorithm selection}\label{sec:rice}
The problem is to understand what underlying characteristics of the problem 
instances distinguishes \emph{good} and on the other hand \emph{bad} solutions 
when implementing a particular algorithm. \citet{SmithMilesLion5} were 
interested in discovering whether synthetic instances were in fact similar to 
real-world instances for timetabling problems. Moreover,  
\citeauthor{SmithMilesLion5} focused on how varying algorithms perform 
on different data distributions. Hence, the investigation of heuristic 
efficiency is closely intertwined to problem generation. 

In order to formulate the relationship between problem structure and heuristic efficiency one can utilise \citeauthor{Rice76}'s framework for algorithm selection problem from \citeyear{Rice76}. The framework consists of four fundamental components, namely,
\begin{description}
	\item[Problem space or instance space] $\mathcal{P}$, \hfill\\
	set of problem instances; 
	\item[Feature space] $\mathcal{F}$, \hfill\\
	measurable properties of the instances in $\mathcal{P}$;
	\item[Algorithm space] $\mathcal{A}$, \hfill\\
	set of all algorithms under inspection;
	\item[Performance space] $\mathcal{Y}$, \hfill\\
	the outcome for $\mathcal{P}$ using an algorithm from $\mathcal{A}$.
\end{description}
For a given problem instance $\vec{x}\in\mathcal{P}$ with $d$ features 
$\vec{\phi}(\vec{x})=\left[\phi_1(\vec{x}),...,\phi_d( 
\vec{x})\right]^T\in\mathcal{F}$ and using algorithm $a\in\mathcal{A}$ the 
performance is $y=\Upsilon(a,\vec{\phi}(\vec{x}))\in\mathcal{Y}$, where 
$\Upsilon:\;\mathcal{A}\times\mathcal{F} \mapsto \mathcal{Y}$ is the mapping 
for algorithm and feature space onto the performance space. This data 
collection is often referred to as meta-data. 

In the context of Rice's framework, the aforementioned approaches to scheduling problems are to maximise its expected performance: 

\begin{description}
  \item[Local level] 
  \begin{equation}
    \max_{\mathcal{F}'\subset\mathcal{F}} 
    \mathbb{E}\left[\Upsilon\left(a,\vphi(\vec{x})\right)\right]
  \end{equation}
  The  focal point is only using  problem instances that represent the problem 
  space, $\vec{x}\in\mathcal{P'}\subset\mathcal{P}$, in addition finding a 
  suitable subset of the feature space, 
  $\mathcal{F}'\subset\mathcal{F}|_{\mathcal{P}'}$. If done effectively, then 
  the resulting learning model $a\in\mathcal{A}$ needs only be run once via 
  ordinal regression.
  \item[Global level]
  \begin{equation}
    \max_{a\in\mathcal{A}} 
    \mathbb{E}\left[\Upsilon\left(a,\vphi(\vec{x})\right)\right]
  \end{equation}
  This is  straightforward  approach as the algorithm $a\in\mathcal{A}$ is 
  optimised directly given the entire instances space $\vec{x}\in\mathcal{P}$ 
  dedicated for training. Alas, this comes at a great computational cost.
\end{description}
Note, the mappings $\vphi:\mathcal{P}\mapsto\mathcal{F}$ and 
$\Upsilon:\mathcal{A}\mapsto\mathcal{Y}$ are the same for both paradigms.

\section{Previous work}
In order to find an optimal (or near optimal) solution for scheduling problems 
one could either use exact methods or heuristics methods. Exact methods 
guarantee an optimal solution, however, \jsp\ scheduling is strongly NP-hard 
\citep{Garey76:NPhard}. Any exact algorithm generally suffers from the curse of 
dimensionality, which impedes the application in finding the global optimum in 
a reasonable amount of time. Heuristics are generally more time efficient but 
do not necessarily attain the global optimum. Therefore, \JSP\ has the 
reputation of being notoriously difficult to solve. As a result, it's 
been widely studied in deterministic scheduling theory and its class of 
problems has been tested on a plethora of different solution methodologies from 
various research fields \citep{Meeran12}, all from simple and straight forward 
\dr s to highly sophisticated frameworks.

In the field of Artificial Intelligence, \cite{Meeran12} point out that despite 
their `intelligent' solutions, the effectiveness of finding the optimum has 
been rather limited. However, combined with local-search methodologies, they 
can be improved upon significantly, as \citeauthor{Meeran12} showed with the 
use of a hybrid method using Genetic Algorithms (GA) and Tabu Search (TS). 
Therefore, getting the best of both worlds, namely
\begin{enumerate*}[label={{}}]
  \item the diverse global search obtained from GA 
  \item being complemented with the intensified local search capabilities of TS
\end{enumerate*}
Unfortunately, hybridisation of global and local methodologies is non-trivial. 
In general, combination of the two improves performance, however, they often 
come at a great computational cost.  

Various \emph{learning} approaches have been applied to solving \jsp\ 
scheduling, such as\todo{Extend!}
\begin{enumerate*}
  \item reinforcement learning \citep{Zhang95}
  \item evolutionary learning \citep{Tay08}
  \item supervised learning \citep{Siggi05,Malik08}
\end{enumerate*}
The approach taken in this 
dissertation is a supervised learning classifier using ordinal regression. 


A common way of finding a good feasible solution for \JSP\ is 
applying construction heuristics with some \dr s, e.g., choosing a task 
corresponding to
\begin{enumerate*}[itemjoin*={{, or }}]
  \item longest or shortest processing time
  \item most or least successors (i.e. operation number)
  \item ranked positional weight, i.e., sum of processing times of its 
  predecessors or successors
\end{enumerate*} 
Ties are broken in an arbitrary fashion or by another heuristic rule.
A summary of over $100$ classical \dr s for scheduling can be found in 
\citet{Panwalkar77}, and it is noted that these classical \dr s are continually 
used in research.  
There is no dominant rule, but the most effective have been \sdr s based on job 
processing attributes \citep{Haupt89}. \citet{Tay08} showed that combining \dr 
s, with the aid of genetic programming, is promising. However, there is large 
number of rules to choose from, thus their combinations require expert 
knowledge or extensive trial-and-error process.

The literature in scheduling mainly focuses on different objectives, \todo{Add 
multiobjective \jsp\ citations}
e.g., \citet{Chang96} minimised the due-date tightness and 
\citet{Drobouchevitch2000,Gao2007} looked into solving for bottleneck machines. 
In this dissertation only minimisation of the makespan will be considered, thus 
ignoring all due-date constraints. 
Model assumptions (i.e. shop floor constraints) can also vary, e.g., 
\citet{Thiagarajan05} incorporate different earliness, tardiness and holding 
costs. 
\citet{Brandimarte1993,Xia2005,Pezzella2008} extend the classical \JSP\ set-up, 
called \emph{flexible} \jsp, by allowing tasks to be processed by any machine 
from a given set, i.e., adding assignment of operations to the constraints.
Moreover, it is possible to reduce \JSP\ to a \FSP, since in practice, 
most jobs in the \jsp\ use the machines in the same order 
\citep{Guinet1998,Ho2007}.

Instead of using construction heuristics that creates \jsp\ schedules by sequentially dispatching one job at a time, one could work with complete feasible schedules and iteratively repairing them for a better result. Such was the approach by \cite{Zhang95} who studied space shuttle payload processing by using reinforcement learning, in particular, temporal difference learning. Starting with a relaxed problem, each job was scheduled as early as its temporal partial order would permit, there by initially ignoring any resource constraints on the machines, yielding the schedule's critical path. Then the schedule would be repaired so the resource constraints were satisfied in the minimum amount of iterations.
This approach of a two phased process of construction and improvement is also implemented in timetable scheduling, e.g., \citet{Asmuni09} used a fuzzy approach in considering multiple heuristic ordering in the construction process, and only allowed feasible schedules to be passed to the improvement phase. 

The alternative to hand-crafting heuristics, is to implement an automatic way of learning heuristics using a data driven approach. %Generating training data for job shop scheduling
Data can be generated using a known heuristic, such an approach is taken in 
\cite{Siggi05} for \jsp\, where a LPT-heuristic is applied. Afterwards, a 
decision tree is used to create a \dr\ with similar logic. However, this method 
cannot outperform the original LPT-heuristic used to guide the search. For 
instruction scheduling, this drawback is confronted in 
\citet{Malik08,Russell09,Siggi10} by using an optimal scheduler, computed 
off-line. The optimal solutions are used as training data and a decision tree 
learning algorithm applied as before. Preferring simple to complex models, the 
resulting \dr s gave significantly better schedules than using popular 
heuristics in that field, and a lower worst-case factor from optimality. A 
similar approach is taken for timetable scheduling in \cite{Burke06}, using 
case based reasoning, where training data is guided by the two best heuristics 
for timetable scheduling. 
\citeauthor{Burke06} point out that in order for their framework to be 
successful, problem features need to be sufficiently explanatory and training 
data need to be selected carefully so they can suggest the appropriate solution 
for a specific range of new cases. 
Again, stressing the importance of meaningful feature selection. 

\section{Supplements}
The Prologue will mostly focus on traditional \jsp\ problem instances. 
However, in \cref{ch:genprobleminstances} there a greater variety of problem 
spaces introduced, and when seen fit some of them will be investigated as well 
in the subsequent \lcnamecref{ch:problemstructure}s. 
Since most experiments have been run on all problem spaces, they can be 
inspected in the supplementary \href{http://tgax89.rhi.hi.is:3838/alice}{Shiny 
application written in R}. 

All source code and data is freely distributed from 
\url{github.com/tungufoss/alice.code} and 
\url{github.com/tungufoss/alice.data}, respectively, under the GNU 
General Public License.

\section{Contributions}\todo{Discuss contributions once draft is ready...}
The approach in this dissertation differs from previous studies, as it uses a simple linear combination of features found using a linear classifier based on ordinal regression.  

\section{Outline}\todo{Update outline once draft is ready...}
An approach based on supervised learning on optimal schedules will be 
investigated and its effectiveness illustrated by improving upon well known 
dispatch rules for \jsp\ scheduling in \cref{ch:prefmodels}. The method of 
generating training data is shown to be critical for the success of the method, 
as shown in \cref{sec:gentrainingdata}. Moreover the choice of problem 
instances under consideration is worth considering, as discussed in 
\cref{ch:genprobleminstances}, and will be used throughout in the subsequent 
\lcnamecref{ch:problemstructure}s. 

The preliminary experiments done in \cref{InRu12} investigated the 
characteristics of difficult \jsp\ schedules for a single heuristic, continuing 
with that research, \cref{ch:problemstructure} compares a set of widely used 
\dr s on different problem spaces in the hopes of extrapolating 
where an algorithm excels in order to aid its failing aspects, which will be 
beneficial information for the creation of learning models in 
\cref{ch:prefmodels}, as they are dependant  on features based on those same 
\dr s under investigation.

