\chapter{Imitation Learning}\label{ch:imitation}

\FirstSentence{D}{espite the abundance of information gathered} by following 
expert policy, the knowledge obtained is not enough by itself. Since the 
learning model isn't perfect, it is bound to make a mistake eventually. 
When it does, the model is in uncharted  territory as there is not certainty 
the samples already collected are able to explain the current situation. For 
this we propose investigating features from suboptimal trajectories as well, 
since the future observations depend on previous predictions. 
A straight forward approach would be to inspect the trajectories of promising 
SDRs or CDRs. 
In fact, it would be worth while to try out imitation learning by 
\cite{RossB10,RossGB11}, such that the learned policy following an optimal 
trajectory is used to collect training data, and the learned model is updated. 
This can be done over several iterations, with the benefit being, that the 
states that are likely to occur in practice are investigated, and as such used 
to dissuade the model from making poor choices. Alas, this comes at great 
computational cost due to the substantial amounts of states that need to be 
optimised for their correct labelling. Making it only practical for \jsp\ of 
a considerable lower dimension. 

Although this study has been structured around the \jsp\ scheduling problem, 
it is easily extended to other types of deterministic optimisation problems 
that involve sequential decision making. 
The framework presented here collects snap-shots of the state space by 
following an optimal trajectory, and verifying the resulting optimal makespan 
from each possible state. 
From which the stepwise optimality of individual features can be inspected, 
which could for instance justify omittance in feature selection. 
\todo[color=pink]{Not done, but possible} 
Moreover, by looking at the best and worst case scenario of suboptimal 
dispatches, it is possible to pinpoint vulnerable times in the scheduling 
process. 



\begin{quote}
    %\todo{Lure the reader in a with a good first sentence}
    So a prudent man must always follow in the footsteps of great men and 
    imitate those who have been outstanding. If his own prowess fails to 
    compare with theirs, at least it has an air of greatness about it. 
    
    \raggedleft NiccolÃ² \cite{Maachiavelli}
\end{quote}
\todo{Eftirleifar galsa. Mun umorda seinna.}
Just as this quote applied to \emph{new principalities acquired with one's own 
    arms and prowess} centuries ago, it equally applies when setting up novel 
supervised learning algorithms. 
When it comes to designing algorithms there needs to be emphasis on where to 
innovate and imitate when visiting state-spaces. 
%\todo{What is the problem?} 
This study will show, that when using these guidelines when accumulating 
training data for supervised  learning, it's possible to automate its 
generation in such a way that the resulting model will be an accurate 
representative of the instances it will later come across. 

For this purpose, the \JSP\ is used as a case study 
to illustrate a methodology for generating meaningful training set 
autonomously, which can be successfully learned using preference-based 
imitation learning (IL).

The approach was to use supervised learning to determine which feature states 
are preferable to others. 

The training data from \cref{InRu11a} was created from optimal solutions of 
randomly generated problem instances, i.e., traditional \emph{passive} 
imitation learning (IL). 
As \JSP\ is a sequential decision making process, errors are bound to emerge.  
Due to compound effect of making suboptimal dispatches, the model leads the 
schedule astray from learned state-spaces, resulting in the new input being 
foreign to the learned model. 

Alternatively, training data could be generated using suboptimal solution 
trajectories as well, as was done in \cref{InRu15a}, where the training data 
also incorporated following the trajectories obtained by applying successful 
SDRs from the literature. 
The reasoning behind it was that  they would be beneficial for learning, 
as they might help the model to escape from local minima once off the coveted 
optimal path. 
By simply adding training data obtained by following the trajectories of 
well-known SDRs, their aggregated training set yielded better models with lower 
\namerho.


%\todo{What are   your contributions?}
Inspired by the work of \cite{RossB10,RossGB11}, the methodology of generating 
training data will now be such that it will iteratively improve upon the model, 
such that the state-spaces learned will be representative of the state-spaces 
the eventual model would likely encounter, known as DAgger for imitation 
learning.
%\todo{Why is it interesting?} 
Thereby, eliminating the ad-hoc nature of choosing trajectories to learn, by 
rather letting the model lead its own way in a self-perpetuating manner until 
it converges.

%\todo{What is the outline of what you will show?}  
The outline of the paper is the following, \cref{sec:background} will define 
formalism for \JSP, and \cref{sec:il,sec:expertPolicy,sec:perturbedLeader} 
explain the trajectories for sampling meaningful schedule state-spaces used in 
preference learning, 
along with some general adjustments for performance boost in \cref{sec:ext}.
Followed by experimental results in \cref{sec:expr} with comparison for several 
randomly generated problem spaces. 
The paper finally concludes in \cref{sec:con} with discussion and conclusions.

\section{Passive Imitation Learning}\label{sec:il:passive}
Using the terms from game-theory used in \cite{CesaBianchi06}, % chapter 2: 
then our problem is a basic version of the sequential prediction problem where 
the predictor (or forecaster), $\pi$, observes each element of a sequence 
$\vchi$ of jobs, where at each time step $k \in \{1,...,K\}$, before the 
$k$-th job of the sequence is revealed, the predictor guesses its value 
$\chi_k$ on the basis of the previous $k-1$ observations. 

\subsection{Prediction with Expert Advice}\label{sec:expertPolicy}
Let's assume we know the expert policy $\pi^\star$, which we can query what 
is the optimal choice of $\chi_k={j^*}$ at any given time step $k$. 
Now we can use \cref{eq:jstar} to back-propagate the relationship between 
post-decision states and $\hat{\pi}$ with preference learning via our collected 
feature set, denoted \PhiSet{OPT}, i.e., we collect the features set 
corresponding following optimal tasks $J_{j^*}$ from $\pi^\star$ in 
\cref{pseudo:constructJSP}.
This baseline trajectory sampling for adding features to the feature set 
is a pure strategy where at each dispatch, an optimal task was originally 
introduced in \cite{InRu11a} and explored further in \cite{InRu15b}. 

By querying the expert policy, $\pi_\star$, the ranking of the job-list, 
$\mathcal{L}$, is determined such that,
\begin{equation}
r_1 \succ r_2 \succ \cdots \succ r_{n'} \quad (n' \leq n)
\end{equation}
implies $r_1$  is preferable to $r_2$, and $r_2$ is preferable to $r_3$, etc. 
In  our study, we know $r \propto C_{\max}^{\pi_\star}$, hence the optimal 
job-list is the following, 
\begin{equation}
\mathcal{O}=\condset{r_i}{r_i \propto \min_{J_j \in \mathcal{L}} 
    C_{\max}^{\pi_\star(\vchi^j)}}
\end{equation}
found by solving the current partial schedule to optimality using a 
commercial software package such as \cite{gurobi}. 

When $\abs{\mathcal{O}^{(k)}}>1$, there can be several trajectories worth 
exploring. However, only one is chosen at random. This is deemed sufficient as 
the number of problem instances, $N_{\train}$, is relatively large.


\subsection{Follow the Perturbed Leader}\label{sec:perturbedLeader}
By allowing a predictor to randomise it's possible to achieve improved 
performance \cite{CesaBianchi06,Hannan57}, which is the inspiration for our new 
strategy, where we follow the Perturbed Leader (PL). 
Its pseudo code is given in \cref{pseudo:perturbedLeader} and describes how the 
expert policy (i.e. optimal trajectory) from \cref{sec:expertPolicy} is subtly
``perturbed'' with $\epsilon=10\%$ likelihood, by choosing a job corresponding 
to the second best $C_{\max}$ instead of a optimal one with some small 
probability. 

\input{pseudocode/perturbedLeader}

\subsection{Summary}

Results showed that the expert policy is a promising starting point. 
However, since \jsp\ is a sequential prediction problem, all future 
observations are dependent on previous operations. 
Therefore, learning sampled states that correspond only to optimal or 
near-optimal schedules isn't of much use when the preference model has 
diverged too far. 
This is due to the learner's predictions affects future input observations 
during its execution, which violates the crucial i.i.d. assumptions of the  
learning approach, and ignoring this interaction leads to poor performance.
In fact, \cite{RossB10} proves, that assuming the preference model has a 
training error of $\epsilon$, then the total compound error (for all $K$ 
dispatches) the classifier induces itself grows quadratically, $\bigOh{\epsilon 
    K^2}$, for the entire schedule, rather than having linear loss, 
$\bigOh{\epsilon K}$, if it were i.i.d.


\section{Active Imitation Learning}\label{sec:il:active}

To amend performance from \PhiSet{OPT}-based models, suboptimal 
state-spaces were explored in \cite{InRu15a} by inspecting the features from 
successful SDRs, $\Phi^{\SDR}$, by passively observing a 
full execution of following the task chosen by the corresponding SDR. 
This required some trial-and-error as the experiments showed that features 
obtained by SDR trajectories were not equally useful for learning.


\todo[inline]{DAgger can be interpreted as Follow-the-leader algorithm in that 
    at iteration $i$ we pick the best policy $\hat{\pi}_i$ in hindsight, i.e., 
    under all trajectories seen so far over the iterations}

``\cite{RossGB11} approach is similar to regularised follow the leader 
algorithm from sec.'' 
In these models the adversary and nature are the same, and the nature chooses a 
new cost function for each action of the learner at the each iteration of the 
game. The goal is to minimise the regret that the learner would suffer, 
compared to the time that if it knew all of the costs imposed by nature in 
hindsight and had chosen a fixed strategy as the response
Ross et  al use a regret minimisation setting for learning to drive a computer 
simulated car, where the output is a sequence of actions in a limited horizon. 
in their problem the true cost of taking action $a$ in state $s$, $C(s,a)$ is 
not 
known, but they use some expert's knowledge about the loss $l(s,\pi)$ incurred 
by the policy $a=\pi(s)$ -- policy is the function $\pi:\mathcal{S}\to 
\mathcal{D}(\mathcal{A})$ that maps an state to an action or a distribution 
over action, and is almost equivalent hypothesis function $h:\mathcal{X}\to 
\mathcal{Y}$



To automate this process, inspiration from \emph{active} imitation learning 
presented in \cite{RossGB11} is sought, called \emph{Dataset Aggregation} 
(DAgger) method, which addresses a no-regret algorithm in an on-line learning 
setting. 
The novel meta-algorithm for IL learns a deterministic policy guaranteed to 
perform well under its induced distribution of states. 
The method is closely related to Follow-the-leader (cf. 
\cref{sec:perturbedLeader}), however, with a more sophisticated leverage to the 
expert policy. 
In short, it entails the model $\pi_i$ that queries an expert policy (same as 
in \cref{sec:expertPolicy}), $\pi_\star$, its trying to mimic, 
but also ensuring the learned model updates itself in an iterative fashion, 
until it converges. 
The benefit of this approach is that the states that are likely to occur in 
practice are also investigated and as such used to dissuade the model from 
making poor choices. In fact, the method queries the expert about the desired 
action at individual post-decision states which are both based on past queries, 
and the learner's interaction with the \emph{current} environment.

DAgger has been proven successful on a variety of benchmarks, such as
\begin{enumerate*}[label={{}}]
    \item the video games Super Tux Kart and Super Mario Bros. or
    handwriting recognition -- in all cases greatly improving traditional 
    supervised imitation learning approaches \cite{RossGB11}
    \item real-world applications, e.g. autonomous navigation for large 
    unmanned 
    aerial vehicles \cite{Ross13}
\end{enumerate*}
To illustrate the effectiveness of DAgger, the Super Mario Bros. experiment 
gives a very simple and informative understanding of the benefits of the 
algorithm. In short, Super Mario Bros. is a platform game where the 
protagonist, Mario, must move across the stage without being hit by enemies or 
falling through gaps within a certain time limit. 
One of the reasons the supervised approaches failed, were due to Mario getting 
stuck up against an obstacle, instead of jumping over it. 
However, the expert would always jump over them at a greater distance 
beforehand, and therefore the learned controller would not know of these 
scenarios. 
With iterative methods, Mario would encounter these problematic situations and 
eventually learn how to get himself unstuck. 

The policy of imitation learning at iteration $i>0$ is a mixed strategy given 
as follows, 
\begin{equation}\label{eq:il}
\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}
% i: ith iteration of imitation learning
% pi_star is expert policy (i.e. optimal)
% pi_i^hat: is pref model from prev. iteration
\end{equation}
where $\pi_\star$ is the expert policy and $\hat{\pi}_{i-1}$ is the learned 
model from the previous iteration. 
Note, for the initial iteration, $i=0$, a pure strategy of $\pi_\star$ is 
followed. Hence, $\hat{\pi}_0$ corresponds to the preference model from 
\cref{sec:expertPolicy} (i.e. $\Phi^{\IL{0}}=\Phi^{\OPT}$). 

\Cref{eq:il} shows that $\beta$ controls the probability distribution of 
querying the expert policy $\pi_\star$ instead of the previous imitation model, 
$\hat{\pi}_{i-1}$.  
The only requirement for $\{\beta_i\}_i^\infty$ according to \cite{RossGB11} is 
that $\limit{\frac{1}{T}\sum_{i=0}^T\beta_i}{T\to\infty}{0}$ to guarantee 
finding a policy $\hat{\pi}_i$ that achieves $\epsilon$ surrogate loss under 
its own state distribution limit.

%\todo[inline]{Adopting a game-theoretic terminology, we often call an action a 
%    pure strategy and a probability distribution over actions a mixed strategy 
%    -- direct quote from \cite{CesaBianchi06}}

\Cref{pseudo:imitationLearning} explains the pseudo code for how to collect 
partial training set, $\Phi^{\IL{i}}$ for $i$-th iteration of imitation 
learning.
Subsequently, the resulting preference model, $\hat{\pi}_i$, learns on the 
aggregated datasets from all previous iterations, namely,  
\begin{equation}\label{eq:DAgger}
\Phi^{\DA{i}}=\bigcup_{i'=0}^{i}\Phi^{\IL{i'}}
\end{equation}
and its update procedure is detailed in \cref{pseudo:DAgger}.

\input{pseudocode/imitationLearning}
\input{pseudocode/DAgger}

\section{Experiments}\label{sec:expr}

\subsection*{Performance boost}

In order to boost training accuracy, two strategies were explored 
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Boost.\arabic*}}, ref={{Boost.\arabic*}}]
    \item \label{expr:boost:varylmax} increasing number of preferences used 
    in training (i.e. varying \mbox{$l_{\max} \leq \abs{\Psi}$}),
    \item \label{expr:boost:newdata} introducing more problem instances 
    (denoted 
    EXT in experimental setting).
\end{enumerate}
Note, that in preliminary experiments for \ref{expr:boost:varylmax} showed no 
statistical significance in boost of performance, hence $l_{\max}$ is the same 
as set in \cref{eq:lmax}.
However, \ref{expr:boost:newdata} strategy showed a considerable change in 
performance. Summary of $N_{\train}$ is given in \cref{tbl:Ntrain}. 
Note, for the conventional \PhiSet{OPT} trajectory the extended training 
set was simply obtained by iterating over more examples. 
However, for the DAgger trajectories the extended set consisted of each 
iteration encountering $N_{\train}$ \emph{new} problem 
instances. For a grand total of 
\begin{equation}
N^{\DA{i}}_{\text{train, EXT}}=N_{\train}\cdot (i+1) 
\end{equation}
problem instances explored for the aggregated extended training set used for 
the learning model at iteration $i$.


\begin{figure}[t]
    %Problem Extended    lmax Default Training.Rho NTrain Test.Rho NTest
    %j.rnd.10x10 TRUE   50000   FALSE     15.46896   2400 15.48095  5000
    %j.rnd.10x10 TRUE  700000   FALSE     15.58600   2400 15.57798  5000
    %j.rnd.10x10 TRUE 1100000   FALSE     15.63480   2400 15.60422  5000
    %j.rnd.10x10 TRUE 1500000   FALSE     15.69659   2400 15.71586  5000
    %j.rnd.10x10 TRUE  500000    TRUE     15.70008   2400 15.72195  5000
    %j.rnd.10x10 TRUE 1350000   FALSE     15.70235   2400 15.72388  5000
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplotRho.varyLMAX.10x10}.pdf}
\caption{Box-plot for \namerho, where preference set is sampled to various 
sized $\abs{\Psi_p^{\DA{7}}}=l_{\max}$}
\end{figure}

\input{tables/Ntrain}

\subsection*{DAgger Parameters}
Due to time constraints, only $T=7$ iterations will be inspected.
In addition, there will be three mixed strategies for $\{\beta_i\}_{i=0}^T$ in 
\cref{eq:il} considered
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{DA.\arabic*}}, ref={{DA.\arabic*}}]
    \item \emph{fixed} supervision with $\beta_i=0.5$ save for $\beta_0=1$, 
    \label{expr:ILFIXSUP}
    \item \emph{decreasing supervision} with $\beta_i=0.5^i$, \label{expr:ILSUP}
    \item \emph{unsupervised} with $\beta_i=I(i=0)$, where $I$ is the indicator 
    function.\footnote{$\beta_0=1$ and $\beta_i=0,\forall i>0$.} 
    \label{expr:ILUNSUP}
\end{enumerate}
Note, \ref{expr:ILSUP} starts as \ref{expr:ILFIXSUP} and decays exponentially 
towards \ref{expr:ILUNSUP}.
Moreover, \ref{expr:ILUNSUP} is a simple parameter-free version of the DAgger 
algorithm and often performs best in practice \cite{RossGB11}.

\begin{figure}
    \subcaptionbox{$6\times5$}{\includegraphics[width=\textwidth]{figures/{j.rnd}/{DAGGER.6x5}.pdf}}\\
    \subcaptionbox{$10\times10$}{\includegraphics[width=\textwidth]{figures/{j.rnd}/{DAGGER.10x10}.pdf}}
    \caption{Box plot for deviation from optimality, $\rho$, (\%) using either 
        imitation learning or following perturbed leader 
        strategies.}\label{fig:il} 
\end{figure}

\section{Conclusions}\label{sec:con}
\todo[inline]{"What general lessons might be learnt from this study?"}
DAgger for \jsp\ is not sensitive to choice of $\beta_i$ in \cref{eq:il}.
\todo[inline]{Place work in wider context}

\todo[inline,color=red]{``Training such a predictor, however, is non-trivial as 
    the interdependencies in the sequence of predictions make global 
    optimisation is to leverage information local to modules to aid learning 
    ... To provide good guarentees and performance in practice in this 
    non-i.i.d. (as predictions are interdependent), we also leverage key 
    iterative training methods developped in prior work for imitation learning 
    and structured prediction''}


\todo[inline,color=green]{George Bernard Shaw once said: ``Imitation is not 
    just the sincerest form of flattery -- it's the sincerest form of 
    learning.''}

\todo[inline]{Future Work: \cite{Judah12} Unfortunately [DAgger] query the 
    expert quite aggressively making them impractical for human experts. In 
    contrast, [Reduction-based Active Imitation Learning (RAIL)] focuses on 
    active querying for the purpose of minimizing the expert's labelling 
    effort. Like our work, they also require a dynamics simulator to help 
    select queries }

Maximum Mean Discrepancy (MMD) imitation learning by \cite{Kim13} is an 
iterative algorithm similar to DAgger. 
However, the expert policy is only queried when needed in order to reduce 
computational cost. 
This occurs when a metric of a new state is sufficiently large enough from a 
previously queried states (to ensure diversity of learned optimal states). 
Moreover, in DAgger all data samples are equally important, irrespective of its 
iteration, which can require great number of iterations to learn how to recover 
from the mistakes of earlier policies. To address the naivety of the data 
aggregation, MMD suggests only aggregating a new data point 
if it is sufficiently different to previously gathered states, \emph{and} if 
the current policy has made a mistake. 
Additionally, there are multiple policies, each specializing in a particular 
region of the state space where previous policies made mistakes.
Although MMD has better empirical performance (based on robot applications), it 
requires defining metrics, which in the case of \jsp\ is non-trivial (cf. 
\cite{InRu12}), and fine-tuning thresholds etc., whereas DAgger can be 
straightforwardly implemented, parameter-free and obtains competitive results, 
although with some computational overhead due to excess expert queries. 

In fact, it's possible to circumvent querying the expert altogether and still 
have reasonable performance. By applying Locally Optimal Learning to Search 
(LOLS) \cite{ChangKADL15} it is possible to use imitation learning (similar to 
DAgger framework) when the reference policy is poor (i.e. $\pi_\star$ in 
\cref{eq:il} is suboptimal), 
although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 

