\chapter{Imitation Learning}\label{ch:imitation}

\FirstSentence{D}{espite the abundance of information gathered} by following 
expert policy, the knowledge obtained is not enough by itself. Since the 
learning model isn't perfect, it is bound to make a mistake eventually. 
When it does, the model is in uncharted territory as there is no certainty that
the samples already collected are able to explain the current situation. 
For this we propose investigating features from suboptimal trajectories as 
well, since the future observations depend on previous predictions. 
A straight forward approach would be to inspect the trajectories of promising 
SDRs or CDRs, this was done in \cref{sec:trdat:param:tracks:passive} with good 
results. 
The reasoning behind it was that they would be beneficial for learning, 
as they might help the model to escape from local minima once off the coveted 
optimal path. 
By simply adding training data obtained by following the trajectories of 
well-known SDRs, their aggregated training set yielded better models with lower 
\namerho.
However, this was done in a fairly ad-hoc manner, which we'd like to automate 
even further.  
Therefore, it would be worth while to try out active imitation learning by 
\cite{RossB10,RossGB11}, such that the learned policy following an optimal 
trajectory is used to collect training data, and the learned model is updated. 
This can be done over several iterations, the benefit being that the 
states which are likely to occur in practice are investigated, and as such used 
to dissuade the model from making poor choices. 
Alas, this comes at great computational cost due to the substantial amounts of 
states that need to be optimised for their correct labelling. 
Making it only practical for \jsp\ of relatively low dimension, or only a few 
iterations. 

The preference model presented in \cref{ch:prefmodels,ch:featselect} are 
comprised of collecting snap-shots of the state space by following a fixed
policy, and verifying the resulting optimal makespan from each possible state. 
This can be looked at as \emph{imitation learning} (IL), since we're trying to 
imitate the fixed policy via preference learning. 

\begin{quote}
    So a prudent man must always follow in the footsteps of great men and 
    imitate those who have been outstanding. If his own prowess fails to 
    compare with theirs, at least it has an air of greatness about it. 
    
    \raggedleft NiccolÃ² \cite{Maachiavelli}
\end{quote}
Just as this quote applied to \emph{new principalities acquired with one's own 
arms and prowess} centuries ago, it equally applies when setting up novel 
supervised learning algorithms. 
Namely, when it comes to designing algorithms there needs to be emphasis on 
where to innovate and imitate when visiting state-spaces. 

Up until now, the training data from \cref{ch:prefmodels,ch:featselect} has 
been created from optimal \emph{or} suboptimal solutions of randomly generated 
problem instances, i.e., traditional \emph{passive} imitation learning. 
As \JSP\ is a sequential decision making process, errors are bound to emerge.  
Due to compound effect of making suboptimal dispatches, the model leads the 
schedule astray from learned state-spaces, resulting in the new input being 
foreign to the learned model. 

Inspired by the work of \cite{RossB10,RossGB11}, the methodology of generating 
training data will now be such that it will iteratively improve upon the model, 
such that the state-spaces learned will be representative of the state-spaces 
the eventual model would likely encounter, known as DAgger for imitation 
learning.
Thereby, eliminating the ad-hoc nature of choosing trajectories to learn, by 
rather letting the model lead its own way in a self-perpetuating manner until 
it converges.

\section*{Performance boost}
In order to boost training accuracy even further, two strategies will be explored 
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Boost.\arabic*}}, ref={{Boost.\arabic*}}]
    \item \label{Boost:lmax} increasing number of preferences used 
    in training (i.e. varying \mbox{$l_{\max} \leq \abs{\Psi}$}),
    \item \label{Boost:EXT} introducing more problem instances 
    (denoted EXT in experimental setting).
\end{enumerate}

Note, \ref{Boost:lmax} will be addressed in \cref{sec:il:active}. 
However, \ref{Boost:EXT} strategy will be explored in \cref{sec:il:passive,sec:il:active}. 
Summary of $N_{\train}$ is given in \cref{tbl:Ntrain}.

\input{tables/Ntrain}

\section{Passive Imitation Learning}\label{sec:il:passive}
Using the terms from game-theory used in \citet{CesaBianchi06}, %ch 2: 
then our problem is a basic version of the sequential prediction problem where 
the predictor (or forecaster), $\pi$, observes each element of a sequence 
$\vchi$ of jobs, where at each time step $k \in \{1,...,K\}$, before the 
$k$-th job of the sequence is revealed, the predictor guesses its value 
$\chi_k$ on the basis of the previous $k-1$ observations. 

\subsection{Prediction with Expert Advice}\label{sec:expertPolicy}
Before going further, let's formalise following an expert policy, 
\PhiSet{\OPT}, from \cref{sec:trdat:param:tracks:passive}.
Let's assume we know the expert policy $\pi^\star$, which we can query what 
is the optimal choice of $\chi_k={j^*}$ at any given time step $k$. 
Now we can use \cref{eq:jstar} to back-propagate the relationship between 
post-decision states and $\hat{\pi}$ with preference learning via our collected 
feature set, denoted \PhiSet{\OPT}, i.e., we collect the features set 
corresponding following optimal tasks $J_{j^*}$ from $\pi^\star$ in 
\cref{pseudo:constructJSP}.
This baseline trajectory sampling for adding features to the feature set 
is a pure strategy where at each dispatch, an optimal task was chosen.
This was originally introduced in \cref{InRu11a} and explored further in \cref{InRu15b}. 

By querying the expert policy, $\pi_\star$, the ranking of the job-list, 
$\mathcal{L}$, is determined such that,
\begin{equation}
r_1 \succ r_2 \succ \cdots \succ r_{n'} \quad (n' \leq n)
\end{equation}
implies $r_1$  is preferable to $r_2$, and $r_2$ is preferable to $r_3$, etc. 
In  our study, we know the rank is proportional to its optimum makespan, hence 
the optimal job-list is the following, 
\begin{equation}
\mathcal{O}^{(k)}=\condset{r_i}{r_i \propto \min_{J_j \in \mathcal{L}^{(k)}}
    C_{\max}^{\pi_\star(\vchi^j)}}
\end{equation}
found by solving the current partial schedule to optimality.
When $\abs{\mathcal{O}^{(k)}}>1$, there can be several trajectories worth 
exploring. However, only one is chosen at random. This is deemed sufficient as 
the number of problem instances, $N_{\train}$, is relatively large.

\subsection{Follow the Perturbed Leader}\label{sec:perturbedLeader}
By allowing a predictor to randomise it's possible to achieve improved 
performance \citep{CesaBianchi06,Hannan57}, which is the inspiration for our 
new strategy, where we follow the \emph{Perturbed Leader}, denoted 
$\OPT\epsilon$. 
Its pseudo code is given in \cref{pseudo:perturbedLeader} and describes how the 
expert policy (i.e. optimal trajectory) from \cref{sec:expertPolicy} is subtly
`perturbed' with $\epsilon=10\%$ likelihood, by choosing a job corresponding 
to the second best $C_{\max}$ instead of a optimal one with some small 
probability. 

\input{pseudocode/perturbedLeader}

\subsection{Experimental study}\label{sec:pil:expr}

To address \ref{Boost:EXT} for the conventional \PhiSet{\OPT} 
trajectory the extended training set was simply obtained by iterating over more 
examples, given in \cref{tbl:Ntrain}.

\Cref{fig:boxplot:passive} depicts a box-plot for \namerho, using 
\Problem[6\times5]{\train} and \jrnd{10}{10}. Main statistics are reported in 
\cref{tbl:stats:passive}.
Results show that following the perturbed leader significantly improved 
following the expert policy for \jrndn{6}{5}, \jrndJ{6}{5}, \jrndM{6}{5}, 
\frndn{6}{5} and \fmc{6}{5}. Other \Problem[6\times5]{\train} problem spaces 
and \jrnd{10}{10} had insignificant performance boost.

Results showed that the expert policy is a promising starting point. 
However, since \jsp\ is a sequential prediction problem, all future 
observations are dependent on previous operations. 
Therefore, learning sampled states that correspond only to optimal or 
near-optimal schedules isn't of much use when the preference model has 
diverged too far. 
This is due to the learner's predictions affects future input observations 
during its execution, which violates the crucial i.i.d. assumptions of the  
learning approach, and ignoring this interaction leads to poor performance.
In fact, \cite{RossB10} proves, that assuming the model has a 
training error of $\epsilon$, then the total compound error (for all $K$ 
dispatches) the classifier induces itself grows quadratically, $\bigOh{\epsilon 
    K^2}$, for the entire schedule, rather than having linear loss, 
$\bigOh{\epsilon K}$, if it were i.i.d.

\begin{figure}
    \includegraphics[width=\textwidth]{ALL/{boxplot.passive.6x5}.pdf}
    \includegraphics[width=\textwidth]{ALL/{boxplot.passive.10x10}.pdf}
    \caption{Box-plots for \namerho, following either expert policy or 
    perturbed leader for \Problem[6\times5]{\train} and \jrnd{10}{10}}
    \label{fig:boxplot:passive}
\end{figure}
\input{tables/stats.passive.tex}

\section{Active Imitation Learning}\label{sec:il:active}

To amend performance from \PhiSet{\OPT}-based models, suboptimal 
state-spaces were explored in \cref{InRu15a} by inspecting the features from 
successful SDRs, \PhiSet{\SDR}, by passively observing a 
full execution of following the task chosen by the corresponding SDR. 
This required some trial-and-error as the experiments showed that features 
obtained by SDR trajectories were not equally useful for learning.


To automate this process, inspiration from \emph{active} imitation learning 
presented in \cite{RossGB11} is sought, called \emph{Dataset Aggregation} 
(DAgger) method, which addresses a no-regret algorithm in an on-line learning 
setting. 
The novel meta-algorithm for IL learns a deterministic policy guaranteed to 
perform well under its induced distribution of states. 
The method is closely related to Follow-the-leader (cf. 
\cref{sec:perturbedLeader}), however, with a more sophisticated leverage to the 
expert policy. 
In short, it entails the model $\pi_i$ that queries an expert policy (same as 
in \cref{sec:expertPolicy}), $\pi_\star$, its trying to mimic, 
but also ensuring the learned model updates itself in an iterative fashion, 
until it converges. 
The benefit of this approach is that the states that are likely to occur in 
practice are also investigated and as such used to dissuade the model from 
making poor choices. In fact, the method queries the expert about the desired 
action at individual post-decision states which are both based on past queries, 
and the learner's interaction with the \emph{current} environment.

DAgger has been proven successful on a variety of benchmarks, such as
\begin{enumerate*}[label={{}}]
    \item the video games Super Tux Kart and Super Mario Bros. or
    handwriting recognition -- in all cases greatly improving traditional 
    supervised imitation learning approaches \cite{RossGB11}
    \item real-world applications, e.g. autonomous navigation for large 
    unmanned aerial vehicles \cite{Ross13}
\end{enumerate*}
To illustrate the effectiveness of DAgger, the Super Mario Bros. experiment 
gives a very simple and informative understanding of the benefits of the 
algorithm. In short, Super Mario Bros. is a platform game where the 
protagonist, Mario, must move across the stage without being hit by enemies or 
falling through gaps within a certain time limit. 
One of the reasons the supervised approaches failed, were due to Mario getting 
stuck up against an obstacle, instead of jumping over it. 
However, the expert would always jump over them at a greater distance 
beforehand, and therefore the learned controller would not know of these 
scenarios. 
With iterative methods, Mario would encounter these problematic situations and 
eventually learn how to get himself unstuck. 

\pagebreak
The policy of imitation learning at iteration $i>0$ is a mixed strategy given 
as follows, 
\begin{equation}\label{eq:il}
\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}
% i: ith iteration of imitation learning
% pi_star is expert policy (i.e. optimal)
% pi_i^hat: is pref model from prev. iteration
\end{equation}
where $\pi_\star$ is the expert policy and $\hat{\pi}_{i-1}$ is the learned 
model from the previous iteration. 
Note, for the initial iteration, $i=0$, a pure strategy of $\pi_\star$ is 
followed. Hence, $\hat{\pi}_0$ corresponds to the preference model from 
\cref{sec:expertPolicy} (i.e. $\Phi^{\IL{0}}=\Phi^{\OPT}$). 

\Cref{eq:il} shows that $\beta$ controls the probability distribution of 
querying the expert policy $\pi_\star$ instead of the previous imitation model, 
$\hat{\pi}_{i-1}$.  
The only requirement for $\{\beta_i\}_i^\infty$ according to \cite{RossGB11} is 
that $\limit{\frac{1}{T}\sum_{i=0}^T\beta_i}{T\to\infty}{0}$ to guarantee 
finding a policy $\hat{\pi}_i$ that achieves $\epsilon$ surrogate loss under 
its own state distribution limit.

%\todo[inline]{Adopting a game-theoretic terminology, we often call an action a 
%    pure strategy and a probability distribution over actions a mixed strategy 
%    -- direct quote from \cite{CesaBianchi06}}

\Cref{pseudo:imitationLearning} explains the pseudo code for how to collect 
partial training set, \PhiSet{\IL{i}} for $i$-th iteration of imitation 
learning.
Subsequently, the resulting preference model, $\hat{\pi}_i$, learns on the 
aggregated datasets from all previous iterations, namely,  
\begin{equation}\label{eq:DAgger}
\Phi^{\DA{i}}=\bigcup_{i'=0}^{i}\Phi^{\IL{i'}}
\end{equation}
and its update procedure is detailed in \cref{pseudo:DAgger}.

\input{pseudocode/imitationLearning}
\input{pseudocode/DAgger}

\subsection{DAgger Parameters}
Due to time constraints, then maximum number of iterations is $T=7$ are 
inspected for \jrnd{6}{5} and \jrnd{10}{10} using \ref{bias:equal}.
In addition, there will be three mixed strategies for $\{\beta_i\}_{i=0}^T$ in 
\cref{eq:il} considered
\begin{enumerate}[after={{}}, leftmargin=*,
  label={\textbf{DA.\arabic*}}, ref={{DA.\arabic*}}]
  \item \textbf{fixed} supervision with $\beta_i=0.5$ save for $\beta_0=1$, 
  \label{DA:FIXSUP}
  \item \textbf{decreasing supervision} with $\beta_i=0.5^i$, \label{DA:SUP}
  \item \textbf{unsupervised} with $\beta_i=I(i=0)$, where $I$ is the indicator 
  function.\footnote{$\beta_0=1$ and $\beta_i=0,\forall i>0$.} 
  \label{DA:UNSUP}
\end{enumerate}
Note, \ref{DA:SUP} starts as \ref{DA:FIXSUP} and decays exponentially 
towards \ref{DA:UNSUP}.
Moreover, \ref{DA:UNSUP} is a simple parameter-free version of the DAgger 
algorithm and often performs best in practice \cite{RossGB11}.

\pagebreak
\subsection{Experiments}\label{sec:ail:expr}

To address \ref{Boost:lmax}, then \PsiSet[\DA{7}]{p} for 
\jrnd{10}{10} was trained with varying size $l_{\max}$, from 50,000 to its full 
size 3,626,260 with 50,000 interval. The default value for $l_{\max}$ given in 
\cref{eq:lmax} is denoted in boldface. There was no statistical significance in 
boost of performance, hence $l_{\max}$ is kept unchanged.

\begin{figure}[t]
    %Problem Extended    lmax Default Training.Rho NTrain Test.Rho NTest
    %j.rnd.10x10 TRUE   50000   FALSE     15.46896   2400 15.48095  5000
    %j.rnd.10x10 TRUE  700000   FALSE     15.58600   2400 15.57798  5000
    %j.rnd.10x10 TRUE 1100000   FALSE     15.63480   2400 15.60422  5000
    %j.rnd.10x10 TRUE 1500000   FALSE     15.69659   2400 15.71586  5000
    %j.rnd.10x10 TRUE  500000    TRUE     15.70008   2400 15.72195  5000
    %j.rnd.10x10 TRUE 1350000   FALSE     15.70235   2400 15.72388  5000
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplotRho.varyLMAX.10x10}.pdf}
    \vspace*{-24pt}
\caption{Box-plot for \namerho, where preference set is sampled to various 
sized $\abs{\Psi_p^{\DA{7}}}=l_{\max}$ using \jrnd{10}{10}}
\label{fig:boxplot:varylmax}
\end{figure}

Regarding \ref{Boost:EXT} for DAgger trajectories the extended set 
consisted of each iteration encountering $N_{\train}$ \emph{new} problem 
instances. For a grand total of 
\begin{equation}
N^{\DA{i}}_{\text{train, EXT}}=N_{\train}\cdot (i+1) 
\end{equation}
problem instances explored for the aggregated extended training set used for 
the learning model at iteration $i$.
This way, we use the extended training data sparingly, as labelling for each 
problem instances is computationally intensive. As a result, the computational 
budget for DAgger is same regardless whether there are new problem instances 
used or not, i.e., $\abs{\Phi^{\DA{i}}}\approx\abs{\Phi^{\DA{i}}_{\text{EXT}}}$.

A box-plot of \namerho, is given in \cref{fig:active:boxplot:equal}. 
As we can see DAgger is not fruitful when the same problem instances 
are continually used, this applies to all problem spaces. 
This is due to the fact that there is not enough variance between 
\PhiSet{\IL{i}}, hence the aggregated feature set \PhiSet{\DA{i}} is only slightly perturbed with each iterations. 
Which from \cref{sec:pil:expr} we saw wasn't a very successful modification for the expert policy. Although, it's noted that by introducing sub-optimal state spaces 
the preference model is not as drastically bad as the extended optimal policy, 
even though $\abs{\Phi^{\DA{i}}}\ll\abs{\Phi^{\OPT}_{\text{EXT}}}$ if \Problem[6\times5]{}.
However, when using new problem instances at each iterations, the feature set 
becomes varied enough that situations arise that can be learned to achieve a 
better represented classification problem which yields a lower mean \namerho.

\begin{figure}
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplot.active.6x5}.pdf}
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplot.active.10x10}.pdf}
    \caption{Box-plots for \namerho, using active imitation learning for 
        \jrnd{6}{5} and \jrnd{10}{10} using equal re-sampling (i.e. \ref{bias:equal})}
    \label{fig:active:boxplot:equal}
\end{figure}

\todoWrite{DAgger for equal stepwise sampling}

The best $\{\beta_i\}_{i=1}^T$ configuration for \Problem[6\times5]{\train} and \jrnd{10}{10} was \ref{DA:UNSUP} using an extended data set (i.e. \ref{Boost:EXT}). Therefore, for \jrnd{10}{10} that configuration will also be tried for adjusted \ref{bias:dbl2nd}, however, iterations were stopped as soon as performance downgrades.
Box-plot for \namerho, for both stepwise biases are depicted in \cref{fig:active:boxplot:10x10:bias} and main statistics are reported in \cref{tbl:IL:stats}.

\begin{figure}
	\includegraphics[width=\textwidth]{figures/{j.rnd}/{boxplot.active.bias.10x10}.pdf}
	\caption{Box plot for \jrnd{10}{10} \namerho, using DAgger for \JSP}
	\label{fig:active:boxplot:10x10:bias} 
\end{figure}

\section{Summary of Imitation Learning Experiments}

A summary of \jrnd{10}{10} best passive and active imitation learning models
w.r.t. \namerho, from 
\cref{sec:pil:expr,sec:ail:expr}, respectively, are illustrated in 
\cref{fig:all:boxplot}, and main statistics are given in 
\cref{tbl:IL:stats}. To summarise, the following trajectories are used
\begin{enumerate*}
	\item expert policy, trained on \PhiSet{\OPT}
	\item perturbed leader, trained on \PhiSet{\OPT\epsilon}
	\item imitation learning, trained on $\Phi^{\DA{i}}_{\text{EXT}}$ for 
	iterations $i=\{1,..,3\}$ using extended training set
\end{enumerate*}
As a reference, the \sdr\ MWR is shown on the far right of \cref{fig:all:boxplot}.
	
At first we see that the perturbed leader ever so-slightly improves the mean 
for $\rho$, rather than using the baseline expert policy. 
However, active imitation learning is by far the best improvement. With each 
iteration of DAgger, the models improve upon the previous one with each 
iteration
\begin{enumerate*}  
	\item for \ref{bias:equal} with \ref{Boost:EXT} then $i=1$ starts 
	with increasing  $\Delta\rho\approx+1.39\%$. However, after that first 
	iteration there is a performance boost of $\Delta\rho\approx-15.11\%$ after 
	$i=2$ and $\Delta\rho\approx-0.19\%$ for the final iteration $i=3$
	\item on the other hand when using adjusted \ref{bias:dbl2nd}, only one iteration 
	is needed, as  $\Delta\rho\approx-11.68$ for $i=1$, and after that it 
	stagnates with $\Delta\rho\approx+0.55\%$ for $i=2$ 
	(therefore $i=3$ was not run)
\end{enumerate*}
\todoFind{Update to CMA-ES instead of MWR}
In both cases, DAgger outperforms MWR
\begin{enumerate*}
	\item after $i=3$ iterations by $\Delta\rho\approx-5.31\%$ for 
	\ref{bias:equal} with \ref{Boost:EXT}
	\item after $i=1$ iteration by $\Delta\rho\approx-9.31\%$ for 
	adjusted \ref{bias:dbl2nd}
\end{enumerate*}
Note, for \ref{bias:equal} without \ref{Boost:EXT}, then DAgger is 
unsuccessful, and the aggregated data set downgrades the performance of the 
previous iterations, making it best to learn solely on the initial expert 
policy for that model configuration.
						
\begin{figure}[t]
	\includegraphics[width=\textwidth]{figures/{j.rnd}/{boxplot.summary.10x10}.pdf}
	\caption{Box plot for \jrnd{10}{10} \namerho, using either expert policy, 
	DAgger or following perturbed leader strategies. 
	MWR shown on far right for reference.}\label{fig:all:boxplot} 
\end{figure}
					
\input{tables/stats.IL.10x10.tex}

\section{Conclusions}
This study showed, that when accumulating training data for supervised  
learning using DAgger, it's possible to automate its generation in such a way 
that the resulting model will be an accurate representative of the instances 
it will later come across. 

The experimental study in \cref{sec:ail:expr} showed that DAgger for \jsp\ is 
sensitive to choice of $\beta_i$ in \cref{eq:il}. The best configuration was 
an unsupervised approach (i.e. \ref{DA:UNSUP}), which concurs to the findings of 
\cite{RossGB11}.

Regarding using an extended data set (i.e. \ref{Boost:EXT}), then it's not successful for the expert policy, as $\rho$ increased approximately 10\%. This could most likely be 
counter-acted by increasing $l_{\max}$ to reflect the additional examples.
What is interesting though, is that \ref{Boost:EXT} is well suited for 
active imitation learning, using the same $l_{\max}$ as before. 
Note, the amount of problems used for $N^{\text{OPT}}_{\text{train, EXT}}$ is 
equivalent to $i=10$ or $i=2\tfrac{1}{3}$ iterations of extended DAgger 
for \Problem[6\times5]{\train} and \jrnd{10}{10}, respectively.
The \emph{new} varied data gives the aggregated feature set more information 
of what is important to learn in subsequent iterations, as those new states are 
more likely to be encountered `in practice' rather than `in theory.' Not only 
does the active imitation learning converge faster, it also consistently 
improves with each iterations.

The number of iterations needed depend on the quality of the model configurations. When using the baseline \ref{bias:equal} the imitation model was iterated for $T=7$ iterations. Slowly improving with each iteration.
However, when the preferred adjusted \ref{bias:dbl2nd} stepwise bias then after 
only two iterations a better performance was achieved. Alas, after the third 
iteration the model had already stagnated with slightly, yet insignificant, worse
mean \namerho.





\todo[inline]{Place work in wider context}

\todo[inline,color=red]{``Training such a predictor, however, is non-trivial as 
    the interdependencies in the sequence of predictions make global 
    optimisation is to leverage information local to modules to aid learning 
    ... To provide good guarentees and performance in practice in this 
    non-i.i.d. (as predictions are interdependent), we also leverage key 
    iterative training methods developped in prior work for imitation learning 
    and structured prediction''}


\todo[inline,color=green]{George Bernard Shaw once said: ``Imitation is not 
    just the sincerest form of flattery -- it's the sincerest form of 
    learning.''}

\emph{Maximum Mean Discrepancy} (MMD) imitation learning by \cite{Kim13} is an 
iterative algorithm similar to DAgger. 
However, the expert policy is only queried when needed in order to reduce 
computational cost. 
This occurs when a metric of a new state is sufficiently large enough from a 
previously queried states (to ensure diversity of learned optimal states). 
Moreover, in DAgger all data samples are equally important, irrespective of its 
iteration, which can require great number of iterations to learn how to recover 
from the mistakes of earlier policies. To address the naivety of the data 
aggregation, MMD suggests only aggregating a new data point 
if it is sufficiently different to previously gathered states, \emph{and} if 
the current policy has made a mistake. 
Additionally, there are multiple policies, each specializing in a particular 
region of the state space where previous policies made mistakes.
Although MMD has better empirical performance (based on robot applications), it 
requires defining metrics, which in the case of \jsp\ is non-trivial (cf. 
\cref{InRu12,ch:defdifficulty}), and fine-tuning thresholds etc., whereas 
DAgger can be straightforwardly implemented, parameter-free and obtains 
competitive results, although with some computational overhead due to excess 
expert queries. 

Main drawback of DAgger is that it quite aggressively quires the expert, making 
it impractical for some problems, especially if it involves human experts. 
To confront that, \cite{Judah12} introduce Reduction-based Active Imitation 
Learning (RAIL), which involves a dynamic approach similar to DAgger, but more 
emphasis is used to minimise the expert's labelling effort.
In fact, it's possible to circumvent querying the expert altogether and still 
have reasonable performance. By applying Locally Optimal Learning to Search 
(LOLS) by \cite{ChangKADL15}, it is possible to use imitation learning (similar to 
DAgger framework) when the reference policy is poor (i.e. $\pi_\star$ in 
\cref{eq:il} is suboptimal), 
although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 
