\chapter{Imitation Learning}\label{ch:imitation}

\FirstSentence{D}{espite the abundance of information gathered} by following 
expert policy, the knowledge obtained is not enough by itself. Since the 
learning model isn't perfect, it is bound to make a mistake eventually. 
When it does, the model is in uncharted  territory as there is no certainty that
the samples already collected are able to explain the current situation. 
For this we propose investigating features from suboptimal trajectories as 
well, since the future observations depend on previous predictions. 
A straight forward approach would be to inspect the trajectories of promising 
SDRs or CDRs, this was done in \cref{sec:trdat:param:tracks:passive} with good 
results. 
The reasoning behind it was that they would be beneficial for learning, 
as they might help the model to escape from local minima once off the coveted 
optimal path. 
By simply adding training data obtained by following the trajectories of 
well-known SDRs, their aggregated training set yielded better models with lower 
\namerho.
However, this was done in a fairly ad-hoc manner, which we'd like to automate 
even further.  
Therefore, it would be worth while to try out active imitation learning by 
\cite{RossB10,RossGB11}, such that the learned policy following an optimal 
trajectory is used to collect training data, and the learned model is updated. 
This can be done over several iterations, with the benefit being, that the 
states which are likely to occur in practice are investigated, and as such used 
to dissuade the model from making poor choices. 
Alas, this comes at great computational cost due to the substantial amounts of 
states that need to be optimised for their correct labelling. 
Making it only practical for \jsp\ of relatively low dimension, or only a few 
iterations. 

The preference model presented in \cref{ch:prefmodels,ch:featselect} are 
comprised of collecting snap-shots of the state space by following an expert 
policy, and verifying the resulting optimal makespan from each possible state. 
This can be looked at as \emph{imitation learning} (IL), since we're trying to 
imitate the expert policy via preference learning. 

\begin{quote}
    So a prudent man must always follow in the footsteps of great men and 
    imitate those who have been outstanding. If his own prowess fails to 
    compare with theirs, at least it has an air of greatness about it. 
    
    \raggedleft NiccolÃ² \cite{Maachiavelli}
\end{quote}
Just as this quote applied to \emph{new principalities acquired with one's own 
arms and prowess} centuries ago, it equally applies when setting up novel 
supervised learning algorithms. 
Namely, when it comes to designing algorithms there needs to be emphasis on 
where to innovate and imitate when visiting state-spaces. 

Up until now, the training data from \cref{ch:prefmodels,ch:featselect} has 
been crated from optimal \emph{or} suboptimal solutions of randomly generated 
problem instances, i.e., traditional \emph{passive} imitation learning. 
As \JSP\ is a sequential decision making process, errors are bound to emerge.  
Due to compound effect of making suboptimal dispatches, the model leads the 
schedule astray from learned state-spaces, resulting in the new input being 
foreign to the learned model. 

Inspired by the work of \cite{RossB10,RossGB11}, the methodology of generating 
training data will now be such that it will iteratively improve upon the model, 
such that the state-spaces learned will be representative of the state-spaces 
the eventual model would likely encounter, known as DAgger for imitation 
learning.
Thereby, eliminating the ad-hoc nature of choosing trajectories to learn, by 
rather letting the model lead its own way in a self-perpetuating manner until 
it converges.

\section*{Performance boost}
In order to boost training accuracy, two strategies will be explored 
\begin{enumerate}[after={{}}, leftmargin=*,
    label={\textbf{Boost.\arabic*}}, ref={{Boost.\arabic*}}]
    \item \label{expr:boost:varylmax} increasing number of preferences used 
    in training (i.e. varying \mbox{$l_{\max} \leq \abs{\Psi}$}),
    \item \label{expr:boost:newdata} introducing more problem instances 
    (denoted EXT in experimental setting).
\end{enumerate}

Note, \ref{expr:boost:varylmax} will be addressed in \cref{sec:il:active}. 
However, \ref{expr:boost:newdata} strategy will be explored in 
\cref{sec:il:passive,sec:il:active}. 
Summary of $N_{\train}$ is given in \cref{tbl:Ntrain}.

\input{tables/Ntrain}

\section{Passive Imitation Learning}\label{sec:il:passive}
Using the terms from game-theory used in \citet{CesaBianchi06}, %ch 2: 
then our problem is a basic version of the sequential prediction problem where 
the predictor (or forecaster), $\pi$, observes each element of a sequence 
$\vchi$ of jobs, where at each time step $k \in \{1,...,K\}$, before the 
$k$-th job of the sequence is revealed, the predictor guesses its value 
$\chi_k$ on the basis of the previous $k-1$ observations. 

\subsection{Prediction with Expert Advice}\label{sec:expertPolicy}
Before going further, let's formalise following an expert policy, 
\PhiSet{OPT}, from \cref{sec:trdat:param:tracks:passive}.
Let's assume we know the expert policy $\pi^\star$, which we can query what 
is the optimal choice of $\chi_k={j^*}$ at any given time step $k$. 
Now we can use \cref{eq:jstar} to back-propagate the relationship between 
post-decision states and $\hat{\pi}$ with preference learning via our collected 
feature set, denoted \PhiSet{OPT}, i.e., we collect the features set 
corresponding following optimal tasks $J_{j^*}$ from $\pi^\star$ in 
\cref{pseudo:constructJSP}.
This baseline trajectory sampling for adding features to the feature set 
is a pure strategy where at each dispatch, an optimal task was originally 
introduced in \cref{InRu11a} and explored further in \cref{InRu15b}. 

By querying the expert policy, $\pi_\star$, the ranking of the job-list, 
$\mathcal{L}$, is determined such that,
\begin{equation}
r_1 \succ r_2 \succ \cdots \succ r_{n'} \quad (n' \leq n)
\end{equation}
implies $r_1$  is preferable to $r_2$, and $r_2$ is preferable to $r_3$, etc. 
In  our study, we know the rank is proportional to its optimum makespan, hence 
the optimal job-list is the following, 
\begin{equation}
\mathcal{O}^{(k)}=\condset{r_i}{r_i \propto \min_{J_j \in \mathcal{L}^{(k)}}
    C_{\max}^{\pi_\star(\vchi^j)}}
\end{equation}
found by solving the current partial schedule to optimality.
When $\abs{\mathcal{O}^{(k)}}>1$, there can be several trajectories worth 
exploring. However, only one is chosen at random. This is deemed sufficient as 
the number of problem instances, $N_{\train}$, is relatively large.

\subsection{Follow the Perturbed Leader}\label{sec:perturbedLeader}
By allowing a predictor to randomise it's possible to achieve improved 
performance \citep{CesaBianchi06,Hannan57}, which is the inspiration for our 
new strategy, where we follow the \emph{Perturbed Leader}, denoted 
$\OPT\epsilon$. 
Its pseudo code is given in \cref{pseudo:perturbedLeader} and describes how the 
expert policy (i.e. optimal trajectory) from \cref{sec:expertPolicy} is subtly
`perturbed' with $\epsilon=10\%$ likelihood, by choosing a job corresponding 
to the second best $C_{\max}$ instead of a optimal one with some small 
probability. 

\input{pseudocode/perturbedLeader}

\subsection{Experimental study}

To address \ref{expr:boost:newdata} for the conventional \PhiSet{OPT} 
trajectory the extended training set was simply obtained by iterating over more 
examples, given in \cref{tbl:Ntrain}.

\Cref{fig:boxplot:passive} depicts a box-plot for \namerho, using 
\Problem[6\times5]{\train} and \jrnd{10}{10}. Main statistics are reported in 
\cref{tbl:stats:passive}.
Results show that following the perturbed leader significantly improved 
following the expert policy for \jrndn{6}{5}, \jrndJ{6}{5}, \jrndM{6}{5}, 
\frndn{6}{5} and \fmc{6}{5}. Other \Problem[6\times5]{\train} problem spaces 
and \jrnd{10}{10} were insignificant performance boost.

Results showed that the expert policy is a promising starting point. 
However, since \jsp\ is a sequential prediction problem, all future 
observations are dependent on previous operations. 
Therefore, learning sampled states that correspond only to optimal or 
near-optimal schedules isn't of much use when the preference model has 
diverged too far. 
This is due to the learner's predictions affects future input observations 
during its execution, which violates the crucial i.i.d. assumptions of the  
learning approach, and ignoring this interaction leads to poor performance.
In fact, \cite{RossB10} proves, that assuming the preference model has a 
training error of $\epsilon$, then the total compound error (for all $K$ 
dispatches) the classifier induces itself grows quadratically, $\bigOh{\epsilon 
    K^2}$, for the entire schedule, rather than having linear loss, 
$\bigOh{\epsilon K}$, if it were i.i.d.

\begin{figure}
    \includegraphics[width=\textwidth]{ALL/{boxplot.passive.6x5}.pdf}
    \includegraphics[width=\textwidth]{ALL/{boxplot.passive.10x10}.pdf}
    \caption{Box-plots for \namerho, following either expert policy or 
    perturbed leader for \Problem[6\times5]{\train} and \jrnd{10}{10}}
    \label{fig:boxplot:passive}
\end{figure}
\input{tables/stats.passive.tex}

\section{Active Imitation Learning}\label{sec:il:active}

To amend performance from \PhiSet{OPT}-based models, suboptimal 
state-spaces were explored in \cite{InRu15a} by inspecting the features from 
successful SDRs, $\Phi^{\SDR}$, by passively observing a 
full execution of following the task chosen by the corresponding SDR. 
This required some trial-and-error as the experiments showed that features 
obtained by SDR trajectories were not equally useful for learning.


To automate this process, inspiration from \emph{active} imitation learning 
presented in \cite{RossGB11} is sought, called \emph{Dataset Aggregation} 
(DAgger) method, which addresses a no-regret algorithm in an on-line learning 
setting. 
The novel meta-algorithm for IL learns a deterministic policy guaranteed to 
perform well under its induced distribution of states. 
The method is closely related to Follow-the-leader (cf. 
\cref{sec:perturbedLeader}), however, with a more sophisticated leverage to the 
expert policy. 
In short, it entails the model $\pi_i$ that queries an expert policy (same as 
in \cref{sec:expertPolicy}), $\pi_\star$, its trying to mimic, 
but also ensuring the learned model updates itself in an iterative fashion, 
until it converges. 
The benefit of this approach is that the states that are likely to occur in 
practice are also investigated and as such used to dissuade the model from 
making poor choices. In fact, the method queries the expert about the desired 
action at individual post-decision states which are both based on past queries, 
and the learner's interaction with the \emph{current} environment.

DAgger has been proven successful on a variety of benchmarks, such as
\begin{enumerate*}[label={{}}]
    \item the video games Super Tux Kart and Super Mario Bros. or
    handwriting recognition -- in all cases greatly improving traditional 
    supervised imitation learning approaches \cite{RossGB11}
    \item real-world applications, e.g. autonomous navigation for large 
    unmanned aerial vehicles \cite{Ross13}
\end{enumerate*}
To illustrate the effectiveness of DAgger, the Super Mario Bros. experiment 
gives a very simple and informative understanding of the benefits of the 
algorithm. In short, Super Mario Bros. is a platform game where the 
protagonist, Mario, must move across the stage without being hit by enemies or 
falling through gaps within a certain time limit. 
One of the reasons the supervised approaches failed, were due to Mario getting 
stuck up against an obstacle, instead of jumping over it. 
However, the expert would always jump over them at a greater distance 
beforehand, and therefore the learned controller would not know of these 
scenarios. 
With iterative methods, Mario would encounter these problematic situations and 
eventually learn how to get himself unstuck. 

The policy of imitation learning at iteration $i>0$ is a mixed strategy given 
as follows, 
\begin{equation}\label{eq:il}
\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}
% i: ith iteration of imitation learning
% pi_star is expert policy (i.e. optimal)
% pi_i^hat: is pref model from prev. iteration
\end{equation}
where $\pi_\star$ is the expert policy and $\hat{\pi}_{i-1}$ is the learned 
model from the previous iteration. 
Note, for the initial iteration, $i=0$, a pure strategy of $\pi_\star$ is 
followed. Hence, $\hat{\pi}_0$ corresponds to the preference model from 
\cref{sec:expertPolicy} (i.e. $\Phi^{\IL{0}}=\Phi^{\OPT}$). 

\Cref{eq:il} shows that $\beta$ controls the probability distribution of 
querying the expert policy $\pi_\star$ instead of the previous imitation model, 
$\hat{\pi}_{i-1}$.  
The only requirement for $\{\beta_i\}_i^\infty$ according to \cite{RossGB11} is 
that $\limit{\frac{1}{T}\sum_{i=0}^T\beta_i}{T\to\infty}{0}$ to guarantee 
finding a policy $\hat{\pi}_i$ that achieves $\epsilon$ surrogate loss under 
its own state distribution limit.

%\todo[inline]{Adopting a game-theoretic terminology, we often call an action a 
%    pure strategy and a probability distribution over actions a mixed strategy 
%    -- direct quote from \cite{CesaBianchi06}}

\Cref{pseudo:imitationLearning} explains the pseudo code for how to collect 
partial training set, $\Phi^{\IL{i}}$ for $i$-th iteration of imitation 
learning.
Subsequently, the resulting preference model, $\hat{\pi}_i$, learns on the 
aggregated datasets from all previous iterations, namely,  
\begin{equation}\label{eq:DAgger}
\Phi^{\DA{i}}=\bigcup_{i'=0}^{i}\Phi^{\IL{i'}}
\end{equation}
and its update procedure is detailed in \cref{pseudo:DAgger}.

\input{pseudocode/imitationLearning}
\input{pseudocode/DAgger}

\subsection{DAgger Parameters}
Due to time constraints, only $T=7$ iterations will be inspected for 
\jrnd{6}{5} and \jrnd{10}{10}.
In addition, there will be three mixed strategies for $\{\beta_i\}_{i=0}^T$ in 
\cref{eq:il} considered
\begin{enumerate}[after={{}}, leftmargin=*,
  label={\textbf{DA.\arabic*}}, ref={{DA.\arabic*}}]
  \item \textbf{fixed} supervision with $\beta_i=0.5$ save for $\beta_0=1$, 
  \label{expr:ILFIXSUP}
  \item \textbf{decreasing supervision} with $\beta_i=0.5^i$, \label{expr:ILSUP}
  \item \textbf{unsupervised} with $\beta_i=I(i=0)$, where $I$ is the indicator 
  function.\footnote{$\beta_0=1$ and $\beta_i=0,\forall i>0$.} 
  \label{expr:ILUNSUP}
\end{enumerate}
Note, \ref{expr:ILSUP} starts as \ref{expr:ILFIXSUP} and decays exponentially 
towards \ref{expr:ILUNSUP}.
Moreover, \ref{expr:ILUNSUP} is a simple parameter-free version of the DAgger 
algorithm and often performs best in practice \cite{RossGB11}.

\subsection{Experiments}

To address \ref{expr:boost:varylmax}, then $\Psi^{\DA{7}}_{p}$ for 
\jrnd{10}{10} was trained with varying size $l_{\max}$, from 50,000 to its full 
size 3,626,260 with 50,000 step-size. The default value for $l_{\max}$ given in 
\cref{eq:lmax} is denoted in boldface. There was no statistical significance in 
boost of performance, hence $l_{\max}$ is kept unchanged.

\begin{figure}[t]
    %Problem Extended    lmax Default Training.Rho NTrain Test.Rho NTest
    %j.rnd.10x10 TRUE   50000   FALSE     15.46896   2400 15.48095  5000
    %j.rnd.10x10 TRUE  700000   FALSE     15.58600   2400 15.57798  5000
    %j.rnd.10x10 TRUE 1100000   FALSE     15.63480   2400 15.60422  5000
    %j.rnd.10x10 TRUE 1500000   FALSE     15.69659   2400 15.71586  5000
    %j.rnd.10x10 TRUE  500000    TRUE     15.70008   2400 15.72195  5000
    %j.rnd.10x10 TRUE 1350000   FALSE     15.70235   2400 15.72388  5000
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplotRho.varyLMAX.10x10}.pdf}
    \vspace*{-24pt}
\caption{Box-plot for \namerho, where preference set is sampled to various 
sized $\abs{\Psi_p^{\DA{7}}}=l_{\max}$ using \jrnd{10}{10}}
\label{fig:boxplot:varylmax}
\end{figure}

Regarding \ref{expr:boost:newdata} for DAgger trajectories the extended set 
consisted of each iteration encountering $N_{\train}$ \emph{new} problem 
instances. For a grand total of 
\begin{equation}
N^{\DA{i}}_{\text{train, EXT}}=N_{\train}\cdot (i+1) 
\end{equation}
problem instances explored for the aggregated extended training set used for 
the learning model at iteration $i$.

\begin{figure}
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplot.active.6x5}.pdf}
    \includegraphics[width=\textwidth]{{j.rnd}/{boxplot.active.10x10}.pdf}
    \caption{Box-plots for \namerho, using active imitation learning for 
        \jrnd{6}{5} and \jrnd{10}{10}}
\end{figure}


\section{Conclusions}
This study showed, that when accumulating training data for supervised  
learning, it's possible to automate its generation in such a way that the 
resulting model will be an accurate representative of the instances it will 
later come across. 

DAgger for \jsp\ is not sensitive to choice of $\beta_i$ in \cref{eq:il}.

\todo[inline]{Place work in wider context}

\todo[inline,color=red]{``Training such a predictor, however, is non-trivial as 
    the interdependencies in the sequence of predictions make global 
    optimisation is to leverage information local to modules to aid learning 
    ... To provide good guarentees and performance in practice in this 
    non-i.i.d. (as predictions are interdependent), we also leverage key 
    iterative training methods developped in prior work for imitation learning 
    and structured prediction''}


\todo[inline,color=green]{George Bernard Shaw once said: ``Imitation is not 
    just the sincerest form of flattery -- it's the sincerest form of 
    learning.''}

\todo[inline]{Future Work: \cite{Judah12} Unfortunately [DAgger] query the 
    expert quite aggressively making them impractical for human experts. In 
    contrast, [Reduction-based Active Imitation Learning (RAIL)] focuses on 
    active querying for the purpose of minimizing the expert's labelling 
    effort. Like our work, they also require a dynamics simulator to help 
    select queries }

Maximum Mean Discrepancy (MMD) imitation learning by \cite{Kim13} is an 
iterative algorithm similar to DAgger. 
However, the expert policy is only queried when needed in order to reduce 
computational cost. 
This occurs when a metric of a new state is sufficiently large enough from a 
previously queried states (to ensure diversity of learned optimal states). 
Moreover, in DAgger all data samples are equally important, irrespective of its 
iteration, which can require great number of iterations to learn how to recover 
from the mistakes of earlier policies. To address the naivety of the data 
aggregation, MMD suggests only aggregating a new data point 
if it is sufficiently different to previously gathered states, \emph{and} if 
the current policy has made a mistake. 
Additionally, there are multiple policies, each specializing in a particular 
region of the state space where previous policies made mistakes.
Although MMD has better empirical performance (based on robot applications), it 
requires defining metrics, which in the case of \jsp\ is non-trivial (cf. 
\cite{InRu12}), and fine-tuning thresholds etc., whereas DAgger can be 
straightforwardly implemented, parameter-free and obtains competitive results, 
although with some computational overhead due to excess expert queries. 

In fact, it's possible to circumvent querying the expert altogether and still 
have reasonable performance. By applying Locally Optimal Learning to Search 
(LOLS) \cite{ChangKADL15} it is possible to use imitation learning (similar to 
DAgger framework) when the reference policy is poor (i.e. $\pi_\star$ in 
\cref{eq:il} is suboptimal), 
although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 