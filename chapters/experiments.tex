\HeaderQuote{The adventures first... explanations take such a dreadful time.}{The Gryphon} %Alice's Adventures in Wonderland:


\chapter{Experiments }\label{ch:experiments} 
\todoWrite{Compare CMA-ES to PREF models}

\FirstSentence{T}{here's something to be said} for having a good opening line. Morbi commodo, ipsum sed pharetra gravida, orci  $x = 1/\alpha$ magna rhoncus neque, id pulvinar odio lorem non turpis. Nullam sit amet enim. Suspendisse id velit vitae ligula volutpat condimentum. Aliquam erat volutpat. Sed quis velit. Nulla facilisi. Nulla libero. Vivamus pharetra posuere sapien. Nam consectetuer. Sed aliquam, nunc eget euismod ullamcorper, lectus nunc ullamcorper orci, fermentum bibendum enim nibh eget ipsum. Donec porttitor ligula eu dolor. Maecenas vitae nulla consequat libero cursus venenatis. Nam magna enim, accumsan eu, blandit sed, blandit a, eros.
$$\zeta = \frac{1039}{\pi}$$

\todoWrite{When applying rollout features from \cref{tbl:featues}, then is 
sensible to keep track of the best solution found (even though they hadn't been 
followed), they will referred to as \emph{fortified} solution.}


\clearpage
\section{ORLIB}
%\Cref{tbl:comp:orlibfssp,tbl:comp:orlibjssp}
\input{tables/stats.orlib}
\missingfigure{ORLIB JSP}
\missingfigure{ORLIB FSP}



The weights for \cref{eq:CDR:feat} in \cref{InRu11a} were found using 
supervised learning, where the training data was created from optimal solutions 
of randomly generated problem instances. As an alternative, this study showed  
that minimising the mean makespan directly using a brute force search via 
CMA-ES actually results in a better CDRs. The nature of CMA-ES is to explore 
suboptimal routes until it converges to an optimal one. Implying that the 
previous approach of only looking into one optimal route may not produce a 
sufficiently rich training set. That is, the training set should incorporate a 
more complete knowledge on \emph{all} possible preferences, i.e., make also the 
distinction between suboptimal and sub-suboptimal features, etc.  This would 
require a Pareto ranking of preferences which can be used to make the 
distinction to which feature sets are equivalent, better or worse -- and to 
what degree, i.e., by giving a weight to the preference. This would result in a 
very large training set, which of course could be re-sampled in order to make 
it computationally feasible to learn.
