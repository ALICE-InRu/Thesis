\HeaderQuote{The adventures first... explanations take such a dreadful 
time.}{The Gryphon} 


\chapter{OR-Library Comparison}\label{ch:experiments} 
\todoWrite{Compare CMA-ES to PREF models}

\FirstSentence{T}{here's something to be said} for having a good opening line. 

\todoWrite{When applying global (or roll-out) features from \cref{tbl:features} 
as was done in \cref{ch:rollout}, then is sensible to keep track of the best 
solution found (even though they hadn't been followed), this was referred to as 
its \emph{fortified} solution.}

\input{tables/stats.orlib}
\missingfigure{ORLIB JSP}
\missingfigure{ORLIB FSP}

The weights for \cref{eq:CDR:feat} in \cref{InRu11a} were found using 
supervised learning, where the training data was created from optimal solutions 
of randomly generated problem instances. As an alternative, this study showed  
that minimising the mean makespan directly using a brute force search via 
CMA-ES actually results in a better CDRs. The nature of CMA-ES is to explore 
suboptimal routes until it converges to an optimal one. Implying that the 
previous approach of only looking into one optimal route may not produce a 
sufficiently rich training set. That is, \PhiSet{\pi} should incorporate a 
more complete knowledge on \emph{all} possible preferences, i.e., make also the 
distinction between suboptimal and sub-suboptimal features, etc.  This would 
require a Pareto ranking of preferences which can be used to make the 
distinction to which feature sets are equivalent, better or worse -- and to 
what degree, i.e., by giving a weight to the preference. This would result in a 
very large training set, which of course could be re-sampled in order to make 
it computationally feasible to learn.
