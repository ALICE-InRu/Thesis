\HeaderQuote{Tut, tut, child! Everything's got a moral, if only you can find it.}{The Duchess} 

\chapter{Conclusions}\label{ch:conclusions} 
\FirstSentence{C}{urrent literature still hold} \sdr s in high regard, as they 
are simple to implement and quite efficient. 
However, they are generally taken for granted as there is clear lack of 
investigation of \emph{how} these \dr s actually work, and what 
makes them so successful (or in some cases unsuccessful)? 

For instance, of the four SDRs from \cref{sec:SDR} this dissertation focused 
on, why does MWR outperform so significantly for \jsp\, yet completely fail for 
\fsp? 
MWR seems to be able to adapt to varying distributions of processing times, 
however, manipulating the machine ordering causes MWR to break down. 
By inspecting optimal schedules, and meticulously researching what's going on, 
every step of the way of the dispatching sequence as was done in 
\cref{ch:analysingsol}, some light is shed where these SDRs vary w.r.t. the 
problem space at hand. 
Once these simple rules are understood, then it's feasible to extrapolate the 
knowledge gained and create new \cdr s that are likely to be successful. 
An example of which was a blended \dr\ in \cref{sec:diff:opt:bdr}, where we 
start with the SPT heuristic and switch over to MWR at a predetermined time 
points. Those pivotal time steps were chosen by inspecting where SPT succeeds 
MWR (and vice versa). 
By achieving a higher classification accuracy using the new BDR model, it 
substantially improve its inherited rule SPT.
Although, it doesn't transcend to a significantly lower \namerho, when compared 
to its other inherited rule MWR. 
Special care must be taken not to let SPT downgrade MWR performance. This can 
be avoided by inspecting how $\rho$ is evolving as a function of time (cf. 
\cref{fig:diff:case:SDR}) and only consider swapping trajectories before they 
intersect and subsequently diverge in performance. 
Moreover, the improved classification accuracy is proportional to its 
difference in performance spread (i.e. 
$\abs{\zeta^{\MWR}_{\any} - \zeta^{\SPT}_{\any}}$) in that region.

Creating new \dr s is by no means trivial. For \jsp\ there is 
the hidden interaction between processing times and machine ordering that's 
hard to measure.
Due to this artefact, feature selection is of paramount importance, and then it 
becomes the case of not having too many features, as they are likely to hinder 
generalisation due to over-fitting in training the preference model, as was 
seen in \cref{sec:trdat:param:tracks:passive}. 
However, the features need to be explanatory enough to maintain predictive 
ability. 
For this reason \cref{eq:CDR:feat} was limited to up to three active features 
in \cref{ch:featselect}, as the full feature set was clearly suboptimal 
w.r.t. its CMA-ES benchmark from \cref{ch:esmodels}. 
By using features based on the SDRs, along with some additional local features 
describing the current schedule, it was possible to `discover' the SDRs when 
given only one active feature. 
Although there is not much to be gained by these models, they at least serve as 
a sanity check for the learning models are on the right track. 
Furthermore, by adding on additional features, a boost in performance was 
gained, resulting in a \cdr\ that outperformed all of the SDR baseline. 
Although, the best preference model of 3 active features is still not better 
than the CMA-ES model for \jrnd{10}{10} using \NrFeatLocal\ features. 
However, we're starting to close in on the gap, as previously 
$\Delta\rho\approx6\%$ (using \PsiSet[\minCmax]{p} from 
\cref{sec:trdat:param:tracks:passive}) and now $\Delta\rho\approx2\%$ (cf. 
CDR \#3.524 in \cref{tbl:best.exhaust:stats}) in favour of evolutionary 
search (cf. \cref{tbl:cma:boxplot:set}).



We saw in \cref{sec:trdat:param:tracks:passive} that preference models using 
training data from following SDR policy, i.e., \PhiSet{\SDR}, are good for 
improving its original heuristic. However, this did not transcend for 
\PhiSet{\CMAES}.
Perhaps, if \PhiSet{\CMAES} wasn't based on following the CMA-ES trajectory, 
but rather using the actual features encountered during its optimisation. 
Alas, CMA-ES used a computational budget of 50,000 function evaluations, each 
consisting of the expectation of $N_{\train}$ problem instances. 
So even though \cref{fig:cma:fit} becomes relatively stable after a few 
generations, it would still yield a gigantic feature set that needs to be 
filtered before going through the optimisation phase of correctly labelling 
them.

The analysis-phase of \Alice\ is heavily dependent on having an expert 
policy it's trying to mimic, i.e., knowing the \emph{optimal} solutions for the 
sake of imitation learning. 

Understandably, knowing the true optimum is an unreasonable claim in many 
situations, especially for high dimensional problem instances. 
Luckily, there seems to be the possibility to circumvent querying the expert 
altogether, and still have reasonable performance. 
By applying \emph{locally optimal learning to search} by \citet{ChangKADL15} it 
is possible to use imitation learning even when the reference policy is poor. 
Although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 

\todoWrite{Write overall conclusions of dissertation!}


CMA-ES has the `unfair' advantage of optimising the end-result directly, 
whereas preference models are more focusing on predictability (via 
classification accuracy). Therefore, when training data is contradictory it's 
non-trivial to achieve exceptional performance. 

The dissertation has shown various strategies to improve preference based 
imitation learning, namely
\begin{enumerate}
    \item stepwise sampling bias
    \item exhaustive feature selection
    \item active update procedure using DAgger to where sample states the 
    learned model is likely to encounter is integrated to the preference set
    \item fortified roll-out features
\end{enumerate}
Moreover, several problem distributions and dimensionality were considered with 
sometimes contradictory results. Fortunately, the performance seemed to hold 
when going to higher dimension (i.e. from \Problem[6\times5]{} to 
\Problem[10\times10]{}). Thereby justifying only considering the 
`easy'\footnote{Easy in terms of computational effort.} \JSP\ before investing 
in $10\times10$ (or higher) experiments. 
However, problem distributions is a key component, and the learned model should 
try to represent its intended (test) dataset as close as possible.


\todoWrite{our ability to learn about the strengths and weaknesses of 
    algorithms from empirical evidence}

