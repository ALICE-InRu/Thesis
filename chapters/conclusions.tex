\HeaderQuote{Tut, tut, child! Everything's got a moral, if only you can find it.}{The Duchess} 

\chapter{Conclusions}\label{ch:conclusions} 
\FirstSentence{C}{urrent literature still hold} \sdr s in high regard, as they 
are simple to implement and quite efficient. 
However, they are generally taken for granted as there is clear lack of 
investigation of \emph{how} these \dr s actually work, and what 
makes them so successful (or in some cases unsuccessful)? 

For instance, of the four SDRs from \cref{sec:SDR} this dissertation focused 
on, why does MWR outperform so significantly for \jsp\, yet completely fail for 
\fsp? 
MWR seems to be able to adapt to varying distributions of processing times, 
however, manipulating the machine ordering causes MWR to break down. 
By inspecting optimal schedules, and meticulously researching what's going on, 
every step of the way of the dispatching sequence as was done in 
\cref{ch:analysingsol}, some light is shed where these SDRs vary w.r.t. the 
problem space at hand. 
Once these simple rules are understood, then it's feasible to extrapolate the 
knowledge gained and create new \cdr s that are likely to be successful. 
An example of which was a blended \dr\ in \cref{sec:diff:opt:bdr}, where we 
start with the SPT heuristic and switch over to MWR at predetermined time 
points. Those pivotal time steps were chosen by inspecting where SPT succeeds 
MWR (and vice versa). 
By achieving a higher classification accuracy using the new BDR model, it 
substantially improve its inherited rule SPT.
Although, it doesn't transcend to a significantly lower \namerho, when compared 
to its other inherited rule MWR. 
Special care must be taken not to let SPT downgrade MWR's performance. This can 
be avoided by inspecting how $\rho$ is evolving as a function of time for its 
intended policy (cf. \cref{fig:diff:case:SDR}) and only consider swapping 
trajectories before they intersect and subsequently diverge in performance. 
Moreover, the improved classification accuracy is proportional to its 
difference in performance spread (i.e. 
$\abs{\zeta^{\MWR}_{\any} - \zeta^{\SPT}_{\any}}$) in that region.

\section*{Summary}

\input{pseudocode/ALICE}

The framework proposed in the dissertation, called \fullnameAlice, or \Alice, 
is roughly described in \cref{pseudo:alice}. 
The dissertation focused on \emph{analysing} \sdr s, starting with 
\cref{ch:defdifficulty} by defining `difficulty' of problem instances. Then 
\cref{ch:analysingsol} continued exploring the \dr s even further, on  
step-by-step basis in order of trying to explain the performance 
of a \dr\ by investigating the strength and weaknesses from empirical evidence. 

The \emph{learning} phase of \Alice\ is focusing on linear preference based 
imitation learning models. The models classify as a tailored algorithm, and 
they're compared to a general algorithm called CMA-ES from \cref{ch:esmodels}. 
It's noted that CMA-ES has the `unfair' advantage of optimising the end-result 
directly, whereas preference models are more focusing on predictability (via 
classification accuracy). Therefore, when training data is contradictory it's 
non-trivial to achieve exceptional performance. 
To address that drawback, the latter half of the dissertation introduced 
various strategies to improve model configuration for improved learning, most 
notably 
\begin{enumerate}
    \item \label{conc:bias} stepwise sampling bias for balancing time-dependent 
    data sets (cf. \cref{sec:pref:bias}),
    \item exhaustive feature selection and bearing in mind the polysemy of 
    reporting training accuracy for preference learning, i.e., should it be the 
    traditional classification accuracy (that deals with classifying all ranks 
    correctly) or just focusing on the stepwise optimality (classifying the 
    optimal rank). Moreover, by incorporating \ref{conc:bias} it's possible to 
    weigh the training accuracy w.r.t. time-step 
    (cf. \cref{ch:featselect}),
    \item allowing the predictor to randomise boosts performance as following 
    the expert policy by itself is too difficult to learn on its own (cf. 
    \cref{sec:perturbedLeader}),
    \item following sub-optimal deterministic policies, yet labelling with an 
    optimal solver, generally improves the guiding policy (cf. 
    \cref{sec:trdat:param:tracks:passive}),
    \item active update procedure using DAgger ensures sample states the 
    learned model is likely to encounter is integrated to the preference set
    (cf. \cref{sec:il:active}),
    \item keeping track of fortified solutions using roll-out features (cf. 
    \cref{ch:rollout})
\end{enumerate}
Moreover, several problem distributions and dimensionality from 
\cref{ch:genprobleminstances} were considered with sometimes contradictory 
results. Fortunately, the performance seemed to hold when going to higher 
dimension (i.e. from \Problem[6\times5]{} to \Problem[10\times10]{}). 
Thereby justifying only considering `easy' \JSP\ in terms of computational 
effort before investing valuable time for higher dimensional experiments. 
However, problem distributions is a key component, and the learned model should 
try to represent its intended (test) dataset as close as possible.
Furthermore, \cref{ch:orlibrobust} showed that results from \Problem{\train} to 
corresponding \Problem{\test} holds for completely different test application, 
e.g., OR-Library benchmark suite.

Creating new \dr s is by no means trivial. For \jsp\ there is 
the hidden interaction between processing times and machine ordering that's 
hard to measure.
Due to this artefact, feature selection is of paramount importance, and then it 
becomes the case of not having too many features, as they are likely to hinder 
generalisation due to over-fitting in training the preference model, as was 
seen in \cref{sec:trdat:param:tracks:passive} for several proposed policies. 
However, the features need to be explanatory enough to maintain predictive 
ability. 
For this reason \cref{eq:CDR:feat} was limited to up to three active features 
in \cref{ch:featselect}, as the full feature set was clearly suboptimal 
w.r.t. its CMA-ES benchmark from \cref{ch:esmodels}. 
By using features based on the SDRs, along with some additional local features 
describing the current schedule, it was possible to `discover' the SDRs when 
given only one active feature. 
Although there is not much to be gained by these models, they at least serve as 
a sanity check for the learning models are on the right track. 

Furthermore, by adding additional features, a boost in performance was 
gained, resulting in a \cdr\ that outperformed all of the SDR benchmarks. 
Although, the best preference model of 3 active features was still not better 
than the CMA-ES model for \jrnd{10}{10} using \NrFeatLocal\ features. 
However, it's starting to close in on the gap, as previously 
$\Delta\rho\approx-6\%$ (using \PsiSet[\minCmax]{p} from 
\cref{sec:trdat:param:tracks:passive}) and now $\Delta\rho\approx-2\%$ (cf. 
CDR \#3.524 in \cref{tbl:best.exhaust:stats}) in favour of evolutionary 
search (cf. \cref{tbl:cma:boxplot:set}).

\section*{Future work}

The DAgger updating framework from \cref{sec:il:active} proposed starting with 
the model based on the expert policy, \cref{pseudo:DAgger} only relies on 
\emph{some} initial learned model. So in theory it should be possible to 
improve the \PsiSet[\minCmax]{p} set-up even further, by applying DAgger 
afterwards to that learned model. Or in general substituting the learned 
\PsiSet[\OPT]{p} to some other \emph{good} initial model.
Perhaps even starting with the perturbed leader which has very similar 
motivation as following the expert policy, yet with substantially better 
performance straight off the bat. 

We saw in \cref{sec:trdat:param:tracks:passive} that preference models using 
training data from following SDR policy (i.e. \PhiSet{\SDR}) are good for 
improving its original heuristic. However, this did not transcend for 
\PhiSet{\CMAES}, which was statistically insignificant with the right stepwise 
sampling bias.
The nature of CMA-ES is to explore suboptimal routes until it converges to an 
optimal one. So perhaps, if \PhiSet{\CMAES} wasn't based on following the 
CMA-ES trajectory, but rather using the actual features encountered during its 
optimisation. 
Alas, CMA-ES used a computational budget of 50,000 function evaluations, each 
consisting of the expectation of $N_{\train}$ problem instances. 
So even though \cref{fig:cma:fit} becomes relatively stable after a few 
generations, it would still yield a gigantic feature set that needs to be 
filtered before going through the optimisation phase of correctly labelling 
them.

From \cref{ch:rollout,ch:orlibrobust} we saw that pilot models achieved the 
lowest \namerho, of all other proposed models from the dissertation. 
Generally the more roll-outs that are made, then lower fortified makespan is 
found. However, in some cases using just four fixed roll-outs were better (cf. 
\cref{tbl:comp:orlib}). 
In particular, we saw in \cref{sec:singlerollout} how pursuing $-\phiSPT$ 
(which is basically repeatedly applying w.r.t. $-\phiproc$ for a 
$(K-k)$-lookahead) significantly increased performance for $\rho$.
Now, instead of doing fixed roll-outs based on SDRs, as \phiSDRRelated, then it 
could be worth investigating a single roll-out of learned policy, $\hat{\pi}$. 
Usually, the learned policy surpasses SPT (i.e. $-\phiproc$) w.r.t. stepwise 
optimality, i.e., $\xi^{\star}_{\hat{\pi}}\geq\xi^{\star}_{-\phiproc}$. 
So presumably even better performance could be achieved, without resorting 
intensive computational budget of a hundred (or more) random roll-outs.

The analysis-phase of \Alice\ is heavily dependent on having an expert 
policy it's trying to mimic, i.e., knowing the \emph{optimal} solutions for the 
sake of imitation learning. 
Understandably, knowing the true optimum is an unreasonable claim in many 
situations, especially for high dimensional problem instances. 
Luckily, there seems to be the possibility to circumvent querying the expert 
altogether, and still have reasonable performance. 
By applying \emph{Locally Optimal Learning to Search} by \citet{ChangKADL15} it 
is possible to use imitation learning even when the reference policy is poor. 
Although it's noted that the quality (w.r.t. near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 

\vfill
To conclude in the words of the Nobel-Prize-winning Irish playwright:
\begin{quote}
    ``Imitation is not just the sincerest form of flattery \\ -- it's the 
    sincerest form of learning'' \\
    \raggedleft George Bernard Shaw
\end{quote} 
\vfill
\vfill