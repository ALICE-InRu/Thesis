\HeaderQuote{Tut, tut, child! Everything's got a moral, if only you can find it.}{The Duchess} 

\chapter{Conclusions}\label{ch:conclusions} 
\todoWrite{Write overall conclusions of dissertation!}

\FirstSentence{L}{orem ipsum dolor sit amet}, consectetuer adipiscing elit. Morbi commodo, ipsum sed pharetra gravida, orci magna rhoncus neque, id pulvinar odio lorem non turpis. Nullam sit amet enim. Suspendisse id velit vitae ligula volutpat condimentum. Aliquam erat volutpat. Sed quis velit. Nulla facilisi. Nulla libero. Vivamus pharetra posuere sapien. Nam consectetuer. Sed aliquam, nunc eget euismod ullamcorper, lectus nunc ullamcorper 


We saw in \cref{sec:trdat:param:tracks:passive} that preference models using 
training data from following SDR policy, i.e., $\Phi^{\langle \text{SDR} 
\rangle}$, are good for improving its original heuristic. However, this did not 
transcend for $\Psi^{\langle \text{CMA-ES} \rangle}$. 
Perhaps, if $\Phi^{\langle \text{CMA-ES} \rangle}$ wasn't based on following 
the CMA-ES trajectory, but rather using the actual features encountered during 
its optimisation. Alas, CMA-ES used a computational budget of 50,000 function 
evaluations, each consisting of the expectation of $N_{\text{train}}$ problem 
instances. So even though \cref{fig:cma:fit} becomes relatively stable after a 
few generations, it would still yield a gigantic feature set that needs to be 
filtered before going through the optimisation phase of correctly labelling 
them.

The analysis-phase of \Alice\ is heavily dependent on having an expert 
policy one wants to mimic, i.e., knowing the \emph{optimal} solutions for the 
sake of imitation learning. 

Understandably, knowing the true optimum is an unreasonable claim in many 
situations, especially for high dimensional problem instances. 
Luckily, there seems to be the possibility to circumvent querying the expert 
altogether, and still have reasonable performance. 
By applying \emph{locally optimal learning to search} \citep{ChangKADL15} it is 
possible to use imitation learning even when the reference policy is poor. 
Although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 
