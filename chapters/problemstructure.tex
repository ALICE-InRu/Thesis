\HeaderQuote{I can't explain myself, I'm afraid, Sir, because I'm not myself you see.}{Alice} 

\chapter{Problem structure}\label{ch:problemstructure} 

%\begin{abstract} 
%Many heuristic methods have been proposed for the \jsp\ scheduling problem. Different solution methodologies outperform other depending on the particular problem instance under consideration. Therefore, one is interested in knowing how the instances differ in structure and determine when a particular heuristic solution is likely to fail and explore in further detail the causes. In order to achieve this, we seek to characterise features for different difficulties. Preliminary experiments show there are different significant features that distinguish between easy and hard \jsp\ problem, and that they vary throughout the scheduling process. 
%The insight attained by investigating the relationship between problem structure and heuristic performance can undoubtedly lead to better heuristic design that is tailored to the data distribution under consideration.
%\end{abstract}


\FirstSentence{P}{roblem structure and heuristic effectiveness} are closely intertwined. When investigating the relation between the two one can research what \citet{Corne10} call \emph{footprints} in instance space, which is an indicator how an algorithm generalises over a given instance space. This sort of investigation has also been conducted by \citet{Pfahringer00} under the alias \emph{landmarking}. 
% quote Corne10: ``such a footprint indicates how an algorithm's comparative performance generalises in instance space''
% quote Katie2009: ``Landmarking tries to directly characterise a domain by relating the performance to some learners -- the landmarkers -- to the performance of some other algorithm.''
From experiments performed by \citeauthor{Corne10}, it is evident that one-algorithm-for-all problem instances is not ideal, in accordance with no free lunch theorem \citep{Wolpert97nofree}. An algorithm may be favoured for its best overall performance, however it is rarely the best algorithm available over various subspaces of the instance space.
Therefore, when comparing different algorithms one needs to explore how they perform w.r.t. the instance space, i.e., their footprint. That is to say, one can look at it as finding which footprints correspond to a subset of the instance space that works \emph{well} for a given algorithm, and similarly finding which footprints correspond to a subset of the instance space that works \emph{poorly} for a given algorithm. 

In the context of \jsp\ this corresponds to finding \emph{good} (makespan close to its optimum)  and \emph{bad} (makespan far off its optimum) schedules. Note, good and bad schedules are interchangeably referred to as \emph{easy} and \emph{hard} schedules (pertaining to the manner they are achieved), respectively. 

\citet{SmithMilesLion5} also investigate algorithm performance in instance space using footprints. The main difference between \citeauthor{Corne10} and \citeauthor{SmithMilesLion5} is how they discretise the instance space. In the case of \citeauthor{Corne10} they use \jsp\ and discretise manually between different problem instances; on one hand w.r.t. processing times, e.g.,  $\vec{p}\sim \mathcal{U}(10,20)$ versus $\vec{p}\sim \mathcal{U}(20,30)$ etc., and on the other hand w.r.t. number of jobs $n$. 
They warn that footprinting can be uneven, so great care needs to be taken in how to discretise the instance space into subspaces. 
On the other than, \citeauthor{SmithMilesLion5} use a completely automated approach. Using timetabling instances, they implement a self-organizing map to group similar problem instances together, that were both real world instances and synthetic ones using different problem generators. 

Going back to the \jsp\ paradigm, then the interaction between problem distribution and its permutation is extremely important, because it introduces hidden properties in the data structure making it \emph{easy} or \emph{hard} to schedule for the given algorithm. These underlying characteristics, i.e., features, define its data structure. A more sophisticated way of discretising the instance space is grouping together problem instances that show the same kind of feature behaviour, especially given the fact the learning models from \cref{ch:prefmodels} will be heavily based on feature pairs. Making it possible to infer which sort of feature behaviour distinguishes  between \emph{good} and \emph{bad} schedules. 

In \citet{InRu12}, a single problem generator was used to create  $N=1,500$ synthetic $6\times6$ \jsp\ problem instances, where $\vec{p}\sim\mathcal{U}(1,200)$ and $\vsigma$ was a random permutation. The experimental study showed that MWR works either well or poorly on a subset of the instances, in fact 18\% and 16\% of the instances were classified as \emph{easy} and \emph{hard} for MWR, respectively. 
Since the problem instances were naively generated, not to mention given the high variance of the data distribution, it is intuitive that there are some inherent structural qualities that could explain this difference in performance. The experimental study investigated the feature behaviours for these two subsets, namely, the easy and hard problem instances. For some features, the trend was more or less the same, which are explained by the common denominating factor, that all instances were sampled from the same problem generator. Whereas, those features that were highly correlated with the end-result, i.e., the final makespan, which determined if an instance was labelled easy or hard, then the significant features varied greatly between the two difficulties, which imply the inherent difference in data structure. 

Moreover, the study in \citet{InRu12} gives support to that random problem instance generators are \emph{too} general and might not suit real-world applications. \citet{Whitley} argue that problem instance generator should be more structured, since real-world manufacturing environment is not completely random, but rather structured, e.g.,  job's tasks can be correlated or machines in the shop. \citeauthor{Whitley} propose a problem instance generator that relates to real-world \fsp\ attributes, albeit not directly modelled after real-world \fsp\ due to the fact that deterministic $Fm||C_{\max}$ is seldom directly applicable in practice \citep{Dudek92}, this is why \fjc{n}{m}, \fmc{n}{m} and \fmxc{n}{m} are also taken into consideration in \cref{ch:genprobleminstances} as they are an attempt to mimic the real-world characteristics of \fsp.

It is interesting to know if the difference in the structure of the schedule is time dependent, e.g.,  is there a clear time of divergence within the scheduling process? 
Moreover, investigation of how sensitive is the difference between two sets of features, e.g.,  can two schedules with similar feature values yield completely contradictory outcomes, i.e., one poor and one good schedule? Or will they more or less follow the their predicted trend? If the latter is the prevalent case, then these instances need to be segregated and each having their own learning algorithm implemented, for a meaningful outcome overall.  
This also, essentially, answers the question of whether  it is in fact feasible to discriminate between \emph{good} and \emph{bad} schedules using the currently selected features as a measure. If results are contradictory, it is an indicator the features selected are not robust enough to capture the essence of the data structure and some key features are missing from the feature set that could be able to discriminate between \emph{good} and \emph{bad} schedules. 
Additionally, there is also the question of how can one define ``similar'' schedules, and what measures should be used? This \lcnamecref{ch:problemstructure} describes some preliminary experiments with the aim of investigating the feasibility of finding distinguishing features corresponding to \emph{good} and \emph{bad} schedules in \jsp. To summarise:
\begin{inparaenum}[(a)]
\item Is there a time of divergence?
\item How does one define ``similar'' schedules?
\item Do similar features yield contradictory outcomes?
\item Are extra features needed?
\end{inparaenum}

Instead of searching through a large set of algorithms  and determining which algorithm is the most suitable for a given subset of the instance space, i.e., creating an algorithm portfolio, as is generally the focus in the current literature \citep{SmithMilesLion3,SmithMilesLion5,Corne10}, the focus of the experimental study in \cref{sec:easyhard:jrnd,sec:easyhard:jrndn,sec:easyhard:frnd,sec:easyhard:frndn,sec:easyhard:fjc,sec:easyhard:fmc,sec:easyhard:fmxc} 
(each corresponding to a given problem space from \cref{ch:genprobleminstances})
is rather on few simple algorithms, here the SDRs described in \cref{sec:SDR}, and understanding \emph{how} they work on the instance space, similar to \citet{Whitley} who analyse the fitness landscape of several problem classes for a fixed algorithm. 



\section{Distribution difficulty w.r.t. SDRs}
Depending on the data distribution, dispatching rules perform differently. Take for instance the common single-based priority dispatching rules; SPT, LPT, LWR and MWR (cf. \cref{sec:SDR}). 
A box-plot for \fullnamerho, for all problem spaces in \cref{ch:genprobleminstances} are depicted in \cref{fig:SDR:boxplot}. 
We can see there is a staggering difference between the interaction of SDRs and their problem space. 
As one can see, MWR is by far the best out of the four SDRs inspected for \JSP\ -- not only does it reach the known optimum most often but it also has the lowest worst-case factor from optimality. Similarly LWR for \FSP.
Although the same problem generator is used, there are some inherent structure in which MWR and LWR can exploit for \JSP\ and  \FSP, respectively, whereas the other SDRs cannot. However, \emph{all} of these dispatching rules are considered good and commonly used in practice and no one is better than the rest \citep{Haupt89}, it simply depends on the data distribution at hand. This indicates that some distributions are harder than others, and these \JSP\ problem generators simply favours MWR, whereas the \FSP\ problem generators favours LWR. 

\begin{figure}[p]\centering
\subfloat[][$6\times5$]{\includegraphics[width=\textwidth]{{boxplotRho.SDR.6x5}.eps}\label{fig:SDR:boxplot:6x5}}
\caption{Box-plots of \namerho, when applying SDRs for all problem spaces in \cref{ch:genprobleminstances}.}\label{fig:SDR:boxplot}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][$10\times10$]{\includegraphics[width=\textwidth]{{boxplotRho.SDR.10x10}.eps}\label{fig:SDR:boxplot:10x10}}
\end{figure}
\section{Characteristics of difficult problem instances}
% Katie orðar: A correlation analysis between the feature space and the performance space was conducted across all 1,500 problem instances revealed that instance features that appear to correlate (linearly) with heuristic performance are $phi(?)$ (correlation $=-0.59$) and $phi(?)$ (correlation $=0.44$). None of the other instance features appear to have a linear relationship with algorithmic performance.

The main focus is on knowing \emph{when} during the scheduling process easy and hard problems diverge and explore in further detail \emph{why} they diverged. Rather than visualising high-dimensional data projected onto two dimensional space (as was the focus in \cite{SmithMilesLion5} with self-organising maps), instead appropriate statistical tests with a significance level $\alpha=0.05$ are sufficient to determine if there is any difference between different data distributions. For this the two-sample Kolmogorov–Smirnov test (K-S test) is used to determine whether two underlying one-dimensional probability distributions differ. 
Furthermore, in order to find defining characteristics for easy or hard problems, a (linear) correlation was computed between features to the resulting \namerho.

Note, when inspecting any statistical difference between data distribution of the features on a step by step basis, the features at step $k+1$ are of course dependant on all previous $k$ steps. This results in repetitive statistical testing, therefore a Bonferroni adjustment is used to counteract the multiple comparisons, i.e., each stepwise comparison has the significant level $\alpha_k=\frac{\alpha}{\ell}$, and thus maintaining the $\sum_{k=1}^{\ell}\alpha_k=\alpha$ significance level.

\subsection{Defining `easy' versus `hard' schedules}\label{sec:diff:easyhard}
It's relatively ad-hoc how to define what makes a schedule difficult. Intuitively, it's logical to use the schedules objective to define it directly, i.e., inspecting \fullnamerho. Moreover, since the SDRs from \cref{sec:SDR} will be used throughout as a benchmark for subsequent models, the quantiles for \namerho, using the SDRs on their training set will be used to differentiate between easy and hard scheduling problems. In particular, the classification is defined as follows, 
\begin{description}
\item[Easy] schedules belong to the first quantile, i.e., \hfill \\
\begin{equation}\label{eq:easy}
\mathcal{E}(a):=\{\vec{x}\;|\;\rho=Y(a,\vec{x})<\rho^{\text{1st. Qu.}}\},
\end{equation} 
\item[Hard] schedules belong to the third quantile, i.e., \hfill \\
\begin{equation}\label{eq:hard}
\mathcal{H}(a):=\{\vec{x}\;|\;\rho=Y(a,\vec{x})>\rho^{\text{3rd. Qu.}}\,
\end{equation} 
\end{description}
where $\vec{x}\in\mathcal{P}_{\text{train}}$ for a given dispatching rule $a\in\mathcal{A}:=\{\text{SPT,~LPT,~LWR,~MWR}\}$.

\Cref{tbl:easyhard:quantile} reports the first and third quantiles for each problem space, i.e., the cut-off values that determine the SDRs difficulty, whose division, defined as percentage of problem instances, i.e., 
\begin{equation}\label{eq:easyhard:cnt}
\frac{\abs{\mathcal{E}(a)}}{N_{\text{train}}} \cdot 100\%
\quad \text{and} \quad 
\frac{\abs{\mathcal{H}(a)}}{N_{\text{train}}}\cdot 100\%
\end{equation}
is given in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10}. Furthermore, the intersection of pairwise SDRs being simultaneously easy or hard are given in \cref{tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}, i.e., 
\begin{equation}\label{eq:easyorhard:cnt}
\frac{\abs{\mathcal{E}(a_i)\cap \mathcal{E}(a_j) }}{N_{\text{train}}} \cdot 100\%
\quad \text{or} \quad 
\frac{\abs{\mathcal{H}(a_i)\cap \mathcal{H}(a_j)}}{N_{\text{train}}}\cdot 100\%
\end{equation}
where $a_i,a_j\in\mathcal{A}$. Note, when $a_i=a_j$ then \cref{eq:easyorhard:cnt} is equivalent to \cref{eq:easyhard:cnt}.

Even though this is a na\"ive way to inspect difference between varying SDRs, it's does give some initial insight of the potential of improving dispatching rules; a sanity check before going into extensive experiments, as will be done in \cref{sec:diff:stepwise}.

For the corresponding $10\times10$ training set (cf. \cref{tbl:easy:cnt:10x10,tbl:hard:cnt:10x10}), the intersections between SDRs from $6\times5$ (cf. \cref{tbl:easy:cnt:6x5,tbl:hard:cnt:6x5}) seem to hold. However, by going to a higher dimension, the performance edge between SDRs becomes more apparent, e.g., in \JSP\ when there was a slight possibility of LWR being simultaneously easy as other SDRs ($5\%<$ chance), it becomes almost impossible for $10\times10$. Making LWR a clear underdog. However, for \FSP\ the tables turn, and LWR has the performance edge. For instance, for \frnd{10}{10}n{6}{5} the second  best option is to apply LPT (13.22\%), however there is a quite high overlap with MWR (11.74\%), and since MWR is easier in significantly more often (85.18\%), the choice of SDR is quite obvious. Although, it goes to show that there is the possibility of improving MWR by sometimes applying LPT-based insight; by seeing what sets apart the intersection of the easy training sets between the two. 

Similarly for every $10\times10$ \JSP\ (cf. \cref{tbl:easy:cnt:10x10}), almost all easy LPT schedules are also easy  for MWR ($<1\%$ difference), as is to be expected as MWR is the more sophisticated counterpart for LPT, like LWR is for SPT. However, the greedy approach here is  not gaining any new information on how to improve MWR. In fact, MWR is never considered hard for any of the \JSP\ (cf. \cref{tbl:hard:cnt:10x10}), therefore no intersection with any hard schedules, but the LPT counterpart has a relatively high occurrence rate (3-14\%), so due to the similarity of the dispatching rules, the denominating factor between LPT and MWR can be an indicator for explaining some of MWR's pitfalls, that is to say, why aren't all of the \jsp\ schedules easy when applying MWR. 

These have up until now all been speculations about how SDRs differ. One thing is for certain, the underlying problem space plays a great role on a SDR's success. Even slight variations to one job or machine, i.e., \jrndJ{10}{10} and \jrndM{10}{10}, shows significant change in performance. Due to the presence of bottleneck, MWR is able to detect it and thus becomes the clear winner. Even outperforming the original \jrnd{10}{10} which they're based on, despite having processing times doubled for a single job or machine, with approximately 10\% lower first quantile (cf. \cref{tbl:easyhard:quantile:10x10}) in both cases. 

As the objective of this dissertation is not to choose which DR is best to use for each problem instance, as done in portfolio optimisation, the focus is set on finding what characterises of \jsp\ on a whole are of value, i.e., feature selection, and create a new model that works well for the problem space overall, by exploiting feature behaviour that is considered more favourable. The hypothesis being that features evolutions of easy schedules greatly differ from features evolutions corresponding to hard schedules, and \cref{sec:diff:stepwise} will attempt to explain the evidence show in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10,tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}.

Note, this \lcnamecref{sec:diff:easyhard} gave the definition of what constitutes an `easy' and `hard' schedule. Since these are based on four SDRs, $\mathcal{A}$, the training data for the experiments done in this \lcnamecref{ch:problemstructure} is based on $4N_{\text{train}}$ problem instances, per problem space, therefore,
\begin{equation}\label{eq:easyhard:all}
\sum_{a\in\mathcal{A}}\abs{\mathcal{E}(a)} \approx N_{\text{train}}
\quad\text{and}\quad
\sum_{a\in\mathcal{A}}\abs{\mathcal{H}(a)} \approx N_{\text{train}}
\end{equation} 
due to the fact \cref{eq:easy,eq:hard} are based on the first and third quantiles of the entire training set.
Now, as the SDRs vary greatly in performance, the contribution of a SDR to \cref{eq:easyhard:all} varies, resulting in an unbalanced sample size when restricted to a single SDR. 

\input{tables/quartiles.EasyHard}
{\setlength{\tabcolsep}{3pt}
\input{tables/quartiles.6x5.EasyHard}
\input{tables/quartiles.10x10.EasyHard}
\input{tables/quartiles.6x5.Easy}
\input{tables/quartiles.10x10.Easy}
\input{tables/quartiles.6x5.Hard}
\input{tables/quartiles.10x10.Hard}
}


\section{Probability of choosing optimal decision}\label{sec:diff:opt:rnd}
In order to create successful dispatching rules, a good starting point is to investigate the properties of optimal solutions and hopefully be able to learn how to mimic such `good' behaviour. For this, we follow an optimal solution, obtained by using a commercial software package \cite{gurobi}, and inspect the evolution of its features, defined in \cref{tbl:features}. Moreover, it is noted, that there are several optimal solutions available for each problem instance. However, it is deemed sufficient to inspect only one optimal trajectory per problem instance as there are $N_{\text{train}}$ independent instances which gives the training data variety. 

Figures within this and subsequent \lcnamecref{sec:diff:opt:rnd}s depict the mean over all the training data, which are quite noisy functions.\footnote{\citet{InRu14c} depicts the mean as is, albeit only for $10\times10$ problem spaces.} Thus, for clarity purposes, they are fitted with local polynomial regression, making the boundary points biased. 

Firstly, we can observe that on a step by step basis there are several optimal dispatches to choose from. \Cref{fig:diff:opt:unique} depicts how the number of optimal dispatches evolve at each dispatch iteration. Note, that only one optimal trajectory is pursued (chosen at random), hence this is only a lower bound of uniqueness of optimal solutions.
As the number of possible dispatches decrease over time, \cref{fig:diff:opt} depicts the probability of choosing an optimal dispatch at each iteration. 

\begin{figure}
\centering
\subfloat[][$6\times5$]{\missingfigure{trdat.prob.optUniqueness.6x5.OPT}}
\\
\subfloat[][$10\times10$]{\includegraphics[width=1\linewidth]{figures/{trdat.prob.optUniqueness.10x10.OPT}.pdf}}
\caption{Number of unique optimal dispatches (lower bound)}
\label{fig:diff:opt:unique}
\end{figure}

\begin{figure}
\centering
\subfloat[][$6\times5$]{\missingfigure{trdat.prob.moveIsOptimal.6x5.OPT.matlab}}
\\
\subfloat[][$10\times10$]{\includegraphics[width=1\linewidth]{figures/{trdat.prob.moveIsOptimal.10x10.OPT.matlab}.pdf}}
\caption{Probability of choosing optimal move}
\label{fig:diff:opt}
\end{figure}

\section{Making suboptimal decisions}\label{sec:diff:opt:sub}
Looking at \cref{fig:diff:opt}, \jrnd{10}{10} has a relatively high probability ($70\%$ and above) of choosing an optimal job. However, it is imperative to keep making optimal decisions, because once off the optimal track the consequences can be dire. To demonstrate this \cref{fig:diff:case} depicts the worst and best case scenario of \namerho, once you've fallen off the optimal track. Note, that this is given that you make \emph{one} wrong turn. Generally, there will be many mistakes made, and then the compound effects of making suboptimal decisions really start adding up. 

It is interesting that for \JSP, that over time making suboptimal decisions make more of an impact on the resulting makespan. This is most likely due to the fact that if a suboptimal decision is made in the early stages, then there is space to rectify the situation with the subsequent dispatches. However, if done at a later point in time, little is to be done as the damage is already been inflicted upon the schedule. 
However, for \FSP, the case is the exact opposite. Under those circumstances it's imperative to make good decisions right from the beginning. This is due to the major structural differences between \jsp\ and \fsp, namely the latter having a homogeneous machine ordering, constricting the solution immensely. Luckily, this does have the added benefit of making \fsp\ less vulnerable for suboptimal decisions later in the decision process. 


\begin{figure}
\centering
\subfloat[][$6\times5$]{\missingfigure{trdat.prob.casescenario.6x5.matlab}}
\\
\subfloat[][$10\times10$]{\includegraphics[width=1\linewidth]{figures/{trdat.prob.casescenario.10x10.matlab}.pdf}}
\caption{\Namerho, for best and worst case scenario of choosing suboptimal dispatch}
\label{fig:diff:case}
\end{figure}

\section{Optimality of simple priority dispatching rules}\label{sec:diff:opt:sdr}
The probability of optimality of the aforementioned SDRs from \cref{sec:SDR}, yet still maintaining our optimal trajectory, i.e., the probability of a job chosen by a SDR being able to yield an optimal makespan on a step by step basis, is depicted  in   \cref{fig:diff:opt:SDR}. Moreover, the dashed line represents the benchmark of random guessing (cf. \cref{fig:diff:opt}).

Now, let's bare in mind \namerho, of applying SDRs throughout the dispatching process (cf. box-plots of which in \cref{fig:SDR:boxplot}), then there is a some correspondence between high probability of stepwise optimality and low $\rho$. Alas, this isn't always the case, for \jrnd{10}{10} SPT always outperforms LPT w.r.t. stepwise optimality, however this does not transcend to SPT having a lower $\rho$ value than LPT. Hence, it's not enough to just learn optimal behaviour, one needs to investigate what happens once we encounter suboptimal state spaces.

\begin{figure}[p]
\centering
\subfloat[][$6\times5$]{\missingfigure{trdat.prob.moveIsOptimal.6x5.SDR.matlab}}
\caption{Probability of SDR being optimal}
\label{fig:diff:opt:SDR}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][$10\times10$]{\includegraphics[width=1\linewidth]{figures/{trdat.prob.moveIsOptimal.10x10.SDR.matlab}.pdf}}
\end{figure}

\section{Simple blended dispatching rule}\label{sec:diff:opt:bdr}
As stated before, the goal of this \lcnamecref{ch:problemstructure} is to utilise feature behaviour to motivate new, and \emph{hopefully} better, dispatching rules. 
A na\"ive approach would be creating a simple blended dispatching rule which would be for instance switch between two SDRs at a predetermined time point. Hence, going back to \cref{fig:diff:opt:SDR} a presumably good BDR for \jrnd{10}{10}  would be starting with SPT and then switching over to MWR at around time step $k=40$, where the SDRs change places in outperforming one another. A box-plot for \namerho, for all $10\times10$ problem spaces is depicted in \cref{fig:diff:boxplot:BDR}. Now, this little manipulation between SDRs does outperform SPT immensely, yet doesn't manage to gain the performance edge of MWR, save for \frnd{10}{10}. This gives us insight that for \jsp, the attribute based on MWR is quite fruitful for good dispatches, whereas the same cannot be said about SPT -- a more sophisticated BDR is needed to improve upon MWR. 

A reason for this lack of performance of our proposed BDR is perhaps that by starting out with SPT in the beginning, it sets up the schedules in such a way that it's quite greedy and only takes into consideration jobs with shortest immediate processing times. Now, even though it is possible to find optimal schedules from this scenario, as \cref{fig:diff:opt:SDR} show, the inherent structure that's already taking place, and might make it hard to come across by simple methods. Therefore it's by no means guaranteed that by simply swapping over to MWR will handle that situation which applying SPT has already created. \Cref{fig:diff:boxplot:BDR} does however show, that by applying MWR instead of SPT in the latter stages, does help the schedule to be more compact w.r.t. SPT. However, in the case of \jrnd{10}{10} and \jrndn{10}{10} the fact remains that the schedules have diverged too far from what MWR would have been able to achieve on its own. Preferably the blended dispatching rule should use  best of both worlds, and outperform all of its inherited DRs, otherwise it goes without saying one would simply still use the original DR that achieved the best results.

\begin{figure}
\centering
\missingfigure{boxplotRho.BDR.10x10}
%\includegraphics[width=1\linewidth]{figures/{boxplotRho.BDR.10x10}.pdf}
\caption{Box plot for \namerho, for BDR where SPT is applied for the first 40\% of the dispatches, followed by MWR}
\label{fig:diff:boxplot:BDR}
\end{figure}


\section{Extremal feature}\label{sec:diff:opt:ext}
The SDRs we've inspected so-far are based on two features from \cref{tbl:features}, namely
\begin{inparaenum}[(i)]
\item \phiproc\ for SPT and LPT, 
\item \phiwrmJob\ for LWR and MWR. 
\end{inparaenum}
By choosing the lowest value for the first SDR, and highest value for the latter SDR, i.e., the extremal values for those given features. Let's apply the same methodology from \cref{sec:diff:opt:sdr} to all varying features\footnote{Note, \phistep, \phimac\ and \phiwrmTotal\ describe the features, not the schedule. For instance, \phistep\, gives us no new information, as that feature is homogeneous for each time step, making it equivalent to random guessing.} described in \cref{tbl:features}. 
\Cref{fig:diff:opt:minmax}
depict the probability of all extremal features being an optimal dispatch, with random guessing from \cref{fig:diff:opt} as a dashed line. 

In order to put the extremal features into perspective, it's worth comparing them with how the evolution of the features are over time, depicted in \cref{fig:diff:opt:evol}. 

\begin{figure}
\centering
\subfloat[][\jrnd{10}{10}]{\missingfigure{j.rnd}}
%\includegraphics[width=1\linewidth]{figures/{j.rnd}/{trdat.feat.stepwise.10x10.OPT}.pdf}
\caption{Feature evolution of optimal trajectory}
\label{fig:diff:opt:evol}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][\jrndn{10}{10}]{\missingfigure{j.rndn}}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][\frnd{10}{10}]{\missingfigure{f.rnd}}
\end{figure}

\begin{figure}
\centering
\subfloat[][\jrnd{10}{10}]{\missingfigure{j.rnd}}
%\includegraphics[width=1\linewidth]{figures/{j.rnd}/{trdat.prob.moveIsOptimal.10x10.feat.minmax}.pdf}
\caption{Probability of extremal feature being optimal}
\label{fig:diff:opt:minmax}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][\jrndn{10}{10}]{\missingfigure{j.rndn}}
\end{figure}
\begin{figure}[p]\centering
\ContinuedFloat
\subfloat[][\frnd{10}{10}]{\missingfigure{f.rnd}}
\end{figure}

\section{Emergence of problem difficulty}\label{sec:diff:stepwise}
\subsection{\Jrnd}\label{sec:easyhard:jrnd}
\missingfigure{Easy-hard jrnd}
\subsection{\Jrndn}\label{sec:easyhard:jrndn}
\missingfigure{Easy-hard jrnd}
\subsection{\frnd{10}{10}}\label{sec:easyhard:frnd}
\missingfigure{Easy-hard frnd}
\subsection{\frnd{10}{10}n}\label{sec:easyhard:frndn}
\missingfigure{Easy-hard frnd}
\subsection{\Fjc}\label{sec:easyhard:fjc}
\missingfigure{Easy-hard fjc}
\subsection{\Fmc}\label{sec:easyhard:fmc}
\missingfigure{Easy-hard fmc}
\subsection{\Fmxc}\label{sec:easyhard:fmxc}
\missingfigure{Easy-hard fxc}

\section{Discussion and Conclusion}
Despite problem instances being created by the same problem generator, they vary enough -- all instances are not created equal -- some are always hard to solve, others always easy. 
Since the description of the problem space isn't enough to predict it's performance, we need a measure to understand what's going on. Why are some instances easier to find their optimum (or close enough), that is to say, what's their secret? This is where their feature evolution comes into play.
By using schedules obtained by applying SDRs we have the ability to get some insight into the matter. 



From the experimental study it is apparent that features have different %impact 
correlation with the resulting schedule depending in what stage it is in the scheduling process, implying that their influence varies over the dispatching sequencing. Moreover, features constant throughout the scheduling process are not correlated with the end-result.
There are some common features for both difficulties considered which define \JSP\ on a whole. However the significant features are quite different across the two difficulties, implying there is a clear difference in their data structure. The amount of significant features were considerably more for easy problems, indicating their key elements had been found. However, the features distinguishing hard problems were scarce. Most likely due to their more complex data structure their key features are of a more composite nature.

It is possible for a \JSP\ schedule to have more than one sequential dispatching representation. It is especially w.r.t. the initial dispatches. Visiting \cref{fig:example} again, if jobs $J_j\in\{J_1,J_2,J_6\}$ were to be dispatched first, then all permutations yield the same equivalent temporal schedule, this is because they don't create a conflict for one another (as is the case for jobs $J_4$ and $J_5$). This drawback of non-uniqueness of sequential dispatching representation explains why there is hardly any significant feature for the initial steps of the scheduling process (cf. \cref{tbl:JSP:feat:easy} and \cref{tbl:JSP:feat:hard}). 

Since feature selection is of paramount importance in order for algorithms to become successful, one needs to give great thought to how features are selected. What kind of features yield \emph{bad} schedules? And can they be steered onto the path of more promising feature characteristics. This sort of investigation can be an indicator how to create meaningful problem generators. On the account that real-world problem instances are scarce, their hidden properties need be drawn forth in order to generate artificial problem instances from the same data distribution. 

The feature attributes need to be based on statistical or theoretical grounds. Thus scrutiny in understanding the nature of problem instances is of paramount importance in feature engineering for learning. Which yields feedback into what features are important to devote more attention to, i.e., features that result in a failing algorithm. For instance, in \cref{tbl:JSP:feat:same} the slack features have the same distribution in the initial stages of the scheduling process, however there is a clear point of divergence which needs to be investigate why the sudden change? 
In general, this sort of investigation can undoubtedly be used in better algorithm design which is more equipped to deal with varying problem instances and tailor to individual problem instance's needs, i.e., a footprint-oriented algorithm. 

Although this methodology was only implemented on a simple single-priority dispatching rule heuristic, the methodology is easily adaptable for more complex algorithms. The main objective of this work is to illustrate the interaction of a specific algorithm on a given problem structure and its properties.
The authors fully expect that this could help improve performance in solving \JSP\ with ordinal regression, which is a per-instance tuning paradigm. This is currently underway as an extension to work introduced in \cite{InRu11a}.


\todo[inline]{Remake the experiment from \cite{InRu12} for the new problem spaces. Start with SDR, to understand the problem spaces better. Then apply them on the learning models - PREF \& CMA-ES: Comparative study.}
