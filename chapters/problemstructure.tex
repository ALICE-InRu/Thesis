\HeaderQuote{Sentence first -- verdict afterwards.}{The Queen}

\chapter{Problem Structure}\label{ch:problemstructure} 

%\begin{abstract} 
%Many heuristic methods have been proposed for the \jsp\ scheduling problem. Different solution methodologies outperform other depending on the particular problem instance under consideration. Therefore, one is interested in knowing how the instances differ in structure and determine when a particular heuristic solution is likely to fail and explore in further detail the causes. In order to achieve this, we seek to characterise features for different difficulties. Preliminary experiments show there are different significant features that distinguish between easy and hard \jsp\ problem, and that they vary throughout the scheduling process. 
%The insight attained by investigating the relationship between problem structure and heuristic performance can undoubtedly lead to better heuristic design that is tailored to the data distribution under consideration.
%\end{abstract}

\FirstSentence{P}{roblem structure and heuristic effectiveness} are closely intertwined. When investigating the relation between the two, one can research what \citet{Corne10} call \emph{footprints} in instance space, which is an indicator how an algorithm generalises over a given instance space. This sort of investigation has also been conducted by \citet{Pfahringer00} under the alias \emph{landmarking}. 
% quote Corne10: ``such a footprint indicates how an algorithm's comparative performance generalises in instance space''
% quote Katie2009: ``Landmarking tries to directly characterise a domain by relating the performance to some learners -- the landmarkers -- to the performance of some other algorithm.''
From experiments performed by \citeauthor{Corne10}, it is evident that 
one-algorithm-for-all problem instances is not ideal, in accordance with no 
free lunch theorem \citep{Wolpert97nofree}. An algorithm may be favoured for 
its best overall performance, however, it is rarely the best algorithm 
available over various subspaces of the instance space.
Therefore, when comparing different algorithms one needs to explore how they perform w.r.t. the instance space, i.e., their footprint. That is to say, one can look at it as finding which footprints correspond to a subset of the instance space that works \emph{well} for a given algorithm, and similarly finding which footprints correspond to a subset of the instance space that works \emph{poorly} for a given algorithm. 

\clearpage

In the context of \jsp\ this corresponds to finding \emph{good} (makespan close to its optimum)  and \emph{bad} (makespan far off its optimum) schedules. Note, good and bad schedules are interchangeably referred to as \emph{easy} and \emph{hard} schedules (pertaining to the manner they are achieved), respectively. 

\citet{SmithMilesLion5} also investigate algorithm performance in instance space using footprints. The main difference between \citeauthor{Corne10} and \citeauthor{SmithMilesLion5} is how they discretise the instance space. In the case of \citeauthor{Corne10} they use \jsp\ and discretise manually between different problem instances; on one hand w.r.t. processing times, e.g.,  $\vec{p}\sim \mathcal{U}(10,20)$ versus $\vec{p}\sim \mathcal{U}(20,30)$ etc., and on the other hand w.r.t. number of jobs, $n$. 
They warn that footprinting can be uneven, so great care needs to be taken in 
how to discretise the instance space into subspaces. 
This is why we consider the random vs. random-narrow problem spaces in 
\cref{sec:data:JSP,sec:data:FSP}.

On the other hand, \citeauthor{SmithMilesLion5} use a completely automated 
approach. Using timetabling instances, they implement a self-organizing map 
(SOM) on the feature space to group similar problem instances together, that 
were both real world instances and synthetic ones using different problem 
generators. And by that way it was possible to plot visually the footprints for 
several algorithms.

Going back to the \jsp\ paradigm, then the interaction between processing time 
distribution and its permutation is extremely important, because it introduces 
hidden properties in the data structure making it \emph{easy} or \emph{hard} to 
schedule for the given algorithm. These underlying characteristics, i.e., 
features, define its data structure. A more sophisticated way of discretising 
the instance space is grouping together problem instances that show the same 
kind of feature behaviour, especially given the fact the learning models in 
\cref{ch:prefmodels} will be heavily based on feature pairs. Thereby making it 
possible to infer what sort of feature behaviour distinguishes  between 
\emph{good} and \emph{bad} schedules. 

It is interesting to know if the difference in the structure of the schedule is 
time dependent, e.g.,  is there a clear time of divergence within the 
scheduling process? 
Moreover, investigation of how sensitive is the difference between two sets of 
features, e.g., can two schedules with similar feature values yield 
\begin{enumerate*}[label={{}}, 
  itemjoin={{? }}, itemjoin*={{? Or }}, after={{? }}]
  \item completely contradictory outcomes (i.e. one poor and one good schedule)
  \item will they more or less follow the their predicted trend
\end{enumerate*}
If the latter is the prevalent case, then instances need to be segregated 
w.r.t. their difficulty, where each has their own learning algorithm 
implemented, for a meaningful outcome overall.  

This also, essentially, answers the question of whether  it is in fact feasible 
to discriminate between \emph{good} and \emph{bad} schedules using the 
currently selected features as a measure. If results are contradictory, it is 
an indicator the features selected are not robust enough to capture the essence 
of the data structure and some key features are missing from the feature set 
that could be able to discriminate between \emph{good} and \emph{bad} 
schedules. 
Additionally, there is also the question of how can one define ``similar'' 
schedules, and what measures should be used? This 
\lcnamecref{ch:problemstructure} describes some preliminary experiments with 
the aim of investigating the feasibility of finding distinguishing features 
corresponding to \emph{good} and \emph{bad} schedules in \jsp. To summarise
\begin{enumerate*}[itemjoin={{? }}, itemjoin*={{? And }}, after={{? }}]
  \item How to define problem difficulty
  \item Is there a time of divergence
  \item What are ``similar'' schedules
  \item Do similar features yield contradictory outcomes
  \item Are extra features needed
  \item what can be learned from feature behaviour
\end{enumerate*}

Instead of searching through a large set of algorithms  and determining which 
algorithm is the most suitable for a given subset of the instance space, i.e., 
creating an algorithm portfolio, as is generally the focus in the current 
literature \citep{SmithMilesLion3,SmithMilesLion5,Corne10}, the focus of the 
experimental study in the subsequent \lcnamecref{sec:diff:easyhard}s
is rather on few simple algorithms, namely the SDRs described in 
\cref{sec:SDR}, i.e., we will limit the algorithm space to, 
\begin{equation}\label{eq:SDRset}
\mathcal{A}:=\{\text{SPT,~LPT,~LWR,~MWR}\}
\end{equation} 
and try to understand \emph{how} they work on the instance space, similar to 
\citet{Whitley}, who analysed the fitness landscape of several problem classes 
for a fixed algorithm. 

Depending on the data distribution, dispatching rules perform differently. 
A box-plot for \fullnamerho, using all problem spaces from \cref{tbl:data} are 
depicted in \cref{fig:SDR:boxplot}. 
As one can see, there is a staggering difference between the interaction of 
SDRs and their problem space. MWR is by far the best out of the four SDRs 
inspected for \JSP\ -- not only does it reach the known optimum most often but 
it also has the lowest worst-case factor from optimality. Similarly LWR for 
\FSP.

\clearpage
Although the same processing time distribution is used, there are some inherent structure in which MWR and LWR can exploit for \JSP\ and  \FSP, respectively, whereas the other SDRs cannot. However, \emph{all} of these dispatching rules are considered good and commonly used in practice and no one is better than the rest \citep{Haupt89}, it simply depends on the data distribution at hand. This indicates that some distributions are harder than others, and these \JSP\ problem generators simply favours MWR, whereas the \FSP\ problem generators favours LWR. 

\begin{figure}[p]
  \subcaptionbox{$6\times5$\label{fig:SDR:boxplot:6x5}}{
    \includegraphics[width=\textwidth]{{boxplotRho.SDR.6x5}.pdf}}
  \caption[Box-plots of \namerho, when applying SDRs]{Box-plots of \namerho, 
    when applying SDRs for all problem spaces in \cref{ch:genprobleminstances}.}
  \label{fig:SDR:boxplot}
\end{figure}
\begin{figure}[h]
  \ContinuedFloat 
  \subcaptionbox{$10\times10$\label{fig:SDR:boxplot:10x10}}{
    \includegraphics[width=\textwidth]{{boxplotRho.SDR.10x10}.pdf}}
  \caption{Continued.}
\end{figure}

\section{Distribution difficulty}
In \cref{InRu12}, a single problem generator was used to create  $N=1,500$ 
synthetic $6\times6$ \jsp\ problem instances, where 
$\vec{p}\sim\mathcal{U}(1,200)$ and $\vsigma$ was a random permutation. The 
experimental study showed that MWR works either well or poorly on a subset of 
the instances, in fact 18\% and 16\% of the instances were classified as 
\emph{easy} and \emph{hard} for MWR, respectively. 
Since the problem instances were na\"{i}vely generated, not to mention given 
the high variance of the data distribution, it is intuitive that there are some 
inherent structural qualities that could explain this difference in 
performance. The experimental study investigated the feature behaviours for 
these two subsets, namely, the easy and hard problem instances. For some 
features, the trend was more or less the same, which are explained by the 
common denominating factor, that all instances were sampled from the same 
problem generator. Whereas, those features that were highly correlated with the 
end-result, i.e., the final makespan, which determined if an instance was 
labelled easy or hard, then the significant features varied greatly between the 
two difficulties, which imply the inherent difference in data structure. 
Moreover, the study in gives support to that random problem instance generators 
are \emph{too} general and might not suit real-world applications. 
\citet{Whitley} argue that problem instance generator should be more 
structured, since real-world manufacturing environment is not completely 
random, but rather structured, e.g.,  job's tasks can be correlated or machines 
in the shop. \citeauthor{Whitley} propose a problem instance generator that 
relates to real-world \fsp\ attributes, albeit not directly modelled after 
real-world \fsp\ due to the fact that deterministic $Fm||C_{\max}$ is seldom 
directly applicable in practice \citep{Dudek92}. This is why \fjc{n}{m}, 
\fmc{n}{m} and \fmxc{n}{m} are also taken into consideration in 
\cref{sec:data:FSP}.

\section*{Experimental settings}
% Katie orðar: A correlation analysis between the feature space and the performance space was conducted across all 1,500 problem instances revealed that instance features that appear to correlate (linearly) with heuristic performance are $phi(?)$ (correlation $=-0.59$) and $phi(?)$ (correlation $=0.44$). None of the other instance features appear to have a linear relationship with algorithmic performance.

The main focus is on knowing \emph{when} during the scheduling process easy and 
hard problems diverge and explore in further detail \emph{why} they diverged. 
Rather than visualising high-dimensional data projected onto two dimensional 
space (as was the focus in \cite{SmithMilesLion5} with self-organising maps), 
instead appropriate statistical tests with a significance level $\alpha=0.05$ 
is applied to determine if there is any difference between different data 
distributions. 

For this the two-sample Kolmogorov–Smirnov test (K-S test) is used to determine 
whether two underlying one-dimensional probability distributions differ. 
Furthermore, in order to find defining characteristics for easy or hard problems, a (linear) correlation is computed between features to the resulting \namerho.

Note, when inspecting any statistical difference between data distribution of the features on a step by step basis, the features at step $k+1$ are of course dependant on all previous $k$ steps. This results in repetitive statistical testing, therefore a Bonferroni adjustment is used to counteract the multiple comparisons, i.e., each stepwise comparison has the significant level $\alpha_k=\frac{\alpha}{\ell}$, and thus maintaining the $\sum_{k=1}^{\ell}\alpha_k=\alpha$ significance level.

\section{Defining \emph{easy} versus \emph{hard} schedules} 
\label{sec:diff:easyhard}
It's relatively ad-hoc how to define what makes a schedule difficult. Intuitively, it's logical to use the schedule's objective to define it directly, i.e., inspecting \fullnamerho. Moreover, since the SDRs from \cref{sec:SDR} will be used throughout as a benchmark for subsequent models, the quantiles for \namerho, using the SDRs on their training set will be used to differentiate between easy and hard scheduling problems. In particular, the classification is defined as follows, 
\begin{description}
  \item[Easy] schedules belong to the first quantile, i.e., \hfill \\
  \begin{equation}\label{eq:easy}
    \mathcal{E}(a):=\condset{\vec{x}}{
      \rho=\Upsilon(a,\vec{x})<\rho^{\text{1st. Qu.}}}
  \end{equation} 
  \item[Hard] schedules belong to the third quantile, i.e., \hfill \\
  \begin{equation}\label{eq:hard}
    \mathcal{H}(a):=\condset{\vec{x}}{
      \rho=\Upsilon(a,\vec{x})>\rho^{\text{3rd. Qu.}}}
  \end{equation} 
\end{description}
where $\vec{x}\in\mathcal{P}_{\text{train}}$ for a given $a\in\mathcal{A}$ from 
\cref{eq:SDRset}, 

\input{tables/quartiles.EasyHard} 

\Cref{tbl:easyhard:quantile} reports the first and third quantiles for each problem space, i.e., the cut-off values that determine the SDRs difficulty, whose division, defined as percentage of problem instances, i.e., 
\begin{equation}\label{eq:easyhard:cnt}
  \frac{\abs{\mathcal{E}(a)}}{N_{\text{train}}} \cdot 100\%
  \quad \text{and} \quad 
  \frac{\abs{\mathcal{H}(a)}}{N_{\text{train}}}\cdot 100\%
\end{equation}
for each $a\in\mathcal{A}$, are given in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10}, respectively. 

{\setlength{\tabcolsep}{3pt} 
  \input{tables/quartiles.6x5.EasyHard}
  \input{tables/quartiles.10x10.EasyHard}
}

\section{Consistency of problem instances}
The intersection of pairwise SDRs being simultaneously easy or hard are given in \cref{tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}, i.e., 
\begin{equation}\label{eq:easyorhard:cnt}
  \frac{\abs{\mathcal{E}(a_i)\cap \mathcal{E}(a_j) }}{N_{\text{train}}} \cdot 
  100\%
  \quad \text{or} \quad 
  \frac{\abs{\mathcal{H}(a_i)\cap \mathcal{H}(a_j)}}{N_{\text{train}}}\cdot 
  100\%
\end{equation}
where $a_i,a_j\in\mathcal{A}$. Note, when $a_i=a_j$ then \cref{eq:easyorhard:cnt} is equivalent to \cref{eq:easyhard:cnt}.

{\setlength{\tabcolsep}{3pt} 
  \input{tables/quartiles.6x5.Easy}
  \input{tables/quartiles.6x5.Hard}
  
  \input{tables/quartiles.10x10.Easy}
  \input{tables/quartiles.10x10.Hard}
}

Even though this is a na\"ive way to inspect difference between varying SDRs, it's does give some initial insight of the potential of improving dispatching rules; a sanity check before going into extensive experiments, as will be done in \cref{sec:diff:stepwise}.

For the corresponding $10\times10$ training set (cf. \cref{tbl:easy:cnt:10x10,tbl:hard:cnt:10x10}), the intersections between SDRs from $6\times5$ (cf. \cref{tbl:easy:cnt:6x5,tbl:hard:cnt:6x5}) seem to hold. 
However, by going to a higher dimension, the performance edge between SDRs becomes more apparent, e.g., in \JSP\ when there was a slight possibility of LWR being simultaneously easy as other SDRs ($5\%<$ chance), it becomes almost impossible for $10\times10$. 
Making LWR a clear underdog. 
Despite that, for \FSP\ the tables turn; now LWR has the performance edge. 
For instance, for \frndn{6}{5} the second  best option is to apply LPT 
(13.22\%), however, there is a quite high overlap with LWR (11.74\%), and since 
LWR is easier significantly more often (85.18\%), the choice of SDR is quite 
obvious. 
Although, it goes to show that there is the possibility of improving LWR by sometimes applying LPT-based insight; by seeing what sets apart the intersection of their easy training sets. 

Similarly for every $10\times10$ \JSP\ (cf. \cref{tbl:easy:cnt:10x10}), almost all easy LPT schedules are also easy  for MWR ($<1\%$ difference), as is to be expected as MWR is the more sophisticated counterpart for LPT (like LWR is for SPT). 
However, the greedy approach here is  not gaining any new information on how to improve MWR. 
In fact, MWR is never considered hard for any of the \JSP\ (cf. \cref{tbl:hard:cnt:10x10}), therefore no intersection with any hard schedules. 
But the LPT counterpart has a relatively high occurrence rate (3-14\%), so due to the similarity of the dispatching rules, the denominating factor between LPT and MWR can be an indicator for explaining some of MWR's pitfalls.
That is to say, why aren't all of the \jsp\ schedules easy when applying MWR? 

These have up until now all been speculations about how SDRs differ. One thing is for certain, the underlying problem space plays a great role on a SDR's success. Even slight variations to one job or machine, i.e., \jrndJ{10}{10} and \jrndM{10}{10}, shows significant change in performance. Due to the presence of bottleneck, MWR is able to detect it and thus becomes the clear winner. Even outperforming the original \jrnd{10}{10} which they're based on, despite having processing times doubled for a single job or machine, with approximately 10\% lower first quantile (cf. \cref{tbl:easyhard:quantile:10x10}) in both cases. 

As the objective of this dissertation is not to choose which DR is best to use for each problem instance. 
The focus is set on finding what characterises of \jsp\ overall, are of value (i.e. feature selection), and create a new model that works well for the problem space to a great degree.
Namely, by exploiting feature behaviour that is considered more favourable. The hypothesis being that features evolutions of easy schedules greatly differ from features evolutions corresponding to hard schedules, and \cref{sec:diff:stepwise} will attempt to explain the evidence show in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10,tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}.

Note, this \lcnamecref{sec:diff:easyhard} gave the definition of what 
constitutes an `easy' and `hard' schedule. Since these are based on four SDRs 
(cf. \cref{eq:SDRset}) the training data for the experiments done in this 
\lcnamecref{ch:problemstructure} is based on $4N_{\text{train}}$ problem 
instances, per problem space, therefore,
\begin{equation}\label{eq:easyhard:all}
  \sum_{a\in\mathcal{A}}\abs{\mathcal{E}(a)} \approx N_{\text{train}}
  \quad\text{and}\quad
  \sum_{a\in\mathcal{A}}\abs{\mathcal{H}(a)} \approx N_{\text{train}}
\end{equation} 
due to the fact \cref{eq:easy,eq:hard} are based on the first and third quantiles of the entire training set.
Now, as the SDRs vary greatly in performance, the contribution of a SDR to \cref{eq:easyhard:all} varies, resulting in an unbalanced sample size when restricted to a single SDR. 



\section{Discussion and Conclusion}
Despite problem instances being created by the same problem generator, they vary among one another enough. As a result, all instances are not created equal; some are always hard to solve, others always easy. 
Since the description of the problem space isn't enough to predict its performance, we need a measure to understand what's going on. Why are some instances easier to find their optimum (or close enough)? That is to say, what's their secret? This is where their feature evolution comes into play.
By using schedules obtained by applying SDRs we have the ability to get some insight into the matter. 

From the experimental study it is apparent that features have different %impact 
correlation with the resulting schedule depending in what stage it is in the scheduling process, implying that their influence varies over the dispatching sequencing. Moreover, features constant throughout the scheduling process are not correlated with the end-result.
There are some common features for both difficulties considered which define 
\jsp\ on a whole. However, the significant features are quite different across 
the two difficulties, implying there is a clear difference in their data 
structure. The amount of significant features were considerably more for easy 
problems, indicating their key elements had been found. However, the features 
distinguishing hard problems were scarce. Most likely due to their more complex 
data structure their key features are of a more composite nature. As a result, 
new `global' features were introduced. 

It is possible for a \JSP\ schedule to have more than one sequential 
dispatching representation. It is especially w.r.t. the initial dispatches. 
Revisiting \cref{fig:example:midway}, if we were to dispatch $J_2$ 
first and then $J_4$, then that would be the same equivalent
temporal schedule if we did it the other way around. 
This is because they don't create a conflict for one another 
(as is the case for jobs $J_2$ and $J_3$). This drawback of non-uniqueness of 
sequential dispatching representation explains why there is hardly any 
significant feature for the initial steps of the scheduling process (cf. 
\cref{tbl:JSP:feat:easy} and \cref{tbl:JSP:feat:hard}). 

Since feature selection is of paramount importance in order for algorithms to become successful, one needs to give great thought to how features are selected. What kind of features yield \emph{bad} schedules? And can they be steered onto the path of more promising feature characteristics. This sort of investigation can be an indicator how to create meaningful problem generators. On the account that real-world problem instances are scarce, their hidden properties need be drawn forth in order to generate artificial problem instances from the same data distribution. 

The feature attributes need to be based on statistical or theoretical grounds. 
Scrutiny in understanding the nature of problem instances therefore becomes of paramount importance in feature engineering for learning, as it yields feedback into what features are important to devote more attention to, i.e., features that result in a failing algorithm. 
For instance, in \cref{tbl:JSP:feat:same} the slack features have the same 
distribution in the initial stages of the scheduling process, however, there is 
a clear point of divergence which needs to be investigate why the sudden 
change? 
In general, this sort of investigation can undoubtedly be used in better 
algorithm design which is more equipped to deal with varying problem instances 
and tailor to individual problem instance's needs, i.e., a footprint-oriented 
algorithm.
 
Although this methodology was only implemented on a set of simple single-priority dispatching rules, the methodology is easily adaptable for more complex algorithms, such as the learned preference models in \cref{sec:pref:problemstructure}. The main objective of this work is to illustrate the interaction of a specific algorithm on a given problem structure and its properties. 