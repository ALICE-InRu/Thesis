\HeaderQuote{Sentence first -- verdict afterwards.}{The Queen}

\chapter{Problem Structure}\label{ch:problemstructure} 

%\begin{abstract} 
%Many heuristic methods have been proposed for the \jsp\ scheduling problem. Different solution methodologies outperform other depending on the particular problem instance under consideration. Therefore, one is interested in knowing how the instances differ in structure and determine when a particular heuristic solution is likely to fail and explore in further detail the causes. In order to achieve this, we seek to characterise features for different difficulties. Preliminary experiments show there are different significant features that distinguish between easy and hard \jsp\ problem, and that they vary throughout the scheduling process. 
%The insight attained by investigating the relationship between problem structure and heuristic performance can undoubtedly lead to better heuristic design that is tailored to the data distribution under consideration.
%\end{abstract}

\FirstSentence{P}{roblem structure and heuristic effectiveness} are closely 
intertwined. When investigating the relation between the two, one can research 
what \citet{Corne10} call \emph{footprints}, which is an indicator how an 
algorithm generalises over a given instance space. This sort of investigation 
has also been conducted by \citet{Pfahringer00} under the alias 
\emph{landmarking}. 
% quote Corne10: ``such a footprint indicates how an algorithm's comparative performance generalises in instance space''
% quote Katie2009: ``Landmarking tries to directly characterise a domain by relating the performance to some learners -- the landmarkers -- to the performance of some other algorithm.''
From experiments performed by \citeauthor{Corne10}, it is evident that 
one-algorithm-for-all problem instances is not ideal, in accordance with no 
free lunch theorem \citep{Wolpert97nofree}. An algorithm may be favoured for 
its best overall performance, however, it is rarely the best algorithm 
available over various subspaces of the instance space.
Therefore, when comparing different algorithms one needs to explore how they perform w.r.t. the instance space, i.e., their footprint. That is to say, one can look at it as finding which footprints correspond to a subset of the instance space that works \emph{well} for a given algorithm, and similarly finding which footprints correspond to a subset of the instance space that works \emph{poorly} for a given algorithm. 

\clearpage

In the context of \jsp\ this corresponds to finding \emph{good} (makespan close to its optimum)  and \emph{bad} (makespan far off its optimum) schedules. Note, good and bad schedules are interchangeably referred to as \emph{easy} and \emph{hard} schedules (pertaining to the manner they are achieved), respectively. 

\citet{SmithMilesLion5} also investigate algorithm performance in instance space using footprints. The main difference between \citeauthor{Corne10} and \citeauthor{SmithMilesLion5} is how they discretise the instance space. In the case of \citeauthor{Corne10} they use \jsp\ and discretise manually between different problem instances; on one hand w.r.t. processing times, e.g.,  $\vec{p}\sim \mathcal{U}(10,20)$ versus $\vec{p}\sim \mathcal{U}(20,30)$ etc., and on the other hand w.r.t. number of jobs, $n$. 
They warn that footprinting can be uneven, so great care needs to be taken in 
how to discretise the instance space into subspaces. 
This is why we consider the random vs. random-narrow problem spaces in 
\cref{sec:data:JSP,sec:data:FSP}.

On the other hand, \citeauthor{SmithMilesLion5} use a completely automated 
approach. Using timetabling instances, they implement a self-organizing map 
(SOM) on the feature space to group similar problem instances together, that 
were both real world instances and synthetic ones using different problem 
generators. That way it was possible to plot visually the footprints for 
several algorithms.

Going back to the \jsp\ paradigm, then the interaction between processing time 
distribution and its permutation is extremely important, because it introduces 
hidden properties in the data structure making it \emph{easy} or \emph{hard} to 
schedule for the given algorithm. These underlying characteristics (i.e. 
features), define its data structure. A more sophisticated way of discretising 
the instance space is grouping together problem instances that show the same 
kind of feature behaviour, especially given the fact the learning models in 
\cref{ch:prefmodels} will be heavily based on feature pairs. Thereby making it 
possible to infer what sort of feature behaviour distinguishes  between 
\emph{good} and \emph{bad} schedules. 

It is interesting to know if the difference in the structure of the schedule is 
time dependent, e.g.,  is there a clear time of divergence within the 
scheduling process? 
Moreover, investigation of how sensitive is the difference between two sets of 
features, e.g., can two schedules with similar feature values yield 
\begin{enumerate*}[label={{}}, 
  itemjoin={{? }}, itemjoin*={{? Or }}, after={{? }}]
  \item completely contradictory outcomes (i.e. one poor and one good schedule)
  \item will they more or less follow the their predicted trend
\end{enumerate*}
If the latter is the prevalent case, then instances need to be segregated 
w.r.t. their difficulty, where each has their own learning algorithm 
implemented, for a meaningful outcome overall.  

This also, essentially, answers the question of whether  it is in fact feasible 
to discriminate between \emph{good} and \emph{bad} schedules using the 
currently selected features as a measure of the quality of a solution. 
If results are contradictory, it is an indicator the features selected are not 
robust enough to capture the essence of the data structure and some key 
features are missing from the feature set that could be able to discriminate 
between \emph{good} and \emph{bad} schedules. 
Additionally, there is also the question of how can one define ``similar'' 
schedules, and what measures should be used? This 
\lcnamecref{ch:problemstructure} describes some preliminary experiments with 
the aim of investigating the feasibility of finding distinguishing features 
corresponding to \emph{good} and \emph{bad} schedules in \jsp. To summarise
\begin{enumerate*}[itemjoin={{? }}, itemjoin*={{? And }}, after={{? }}]
  \item How to define problem difficulty
  \item Is there a time of divergence
  \item What are ``similar'' schedules
  \item Do similar features yield contradictory outcomes
  \item Are extra features needed
  \item what can be learned from feature behaviour
\end{enumerate*}

Instead of searching through a large set of algorithms  and determining which 
algorithm is the most suitable for a given subset of the instance space, i.e., 
creating an algorithm portfolio, as is generally the focus in the current 
literature \citep{SmithMilesLion3,SmithMilesLion5,Corne10}, the focus of the 
experimental study in the subsequent \lcnamecref{sec:diff:easyhard}s
is rather on few simple algorithms, namely the SDRs described in 
\cref{sec:SDR}, i.e., we will limit the algorithm space to, 
\begin{equation}\label{eq:SDRset}
\mathcal{A}:=\{\text{SPT,~LPT,~LWR,~MWR}\}
\end{equation} 
and try to understand \emph{how} they work on the instance space, similar to 
\citet{Whitley}, who analysed the fitness landscape of several problem classes 
for a fixed algorithm. 

Depending on the data distribution, dispatching rules perform differently. 
A box-plot for \fullnamerho, using all problem spaces from \cref{tbl:data} are 
depicted in \cref{fig:SDR:boxplot}. 
As one can see, there is a staggering difference between the interaction of 
SDRs and their problem space. MWR is by far the best out of the four SDRs 
inspected for \JSP\ -- not only does it reach the known optimum most often but 
it also has the lowest worst-case factor from optimality. Similarly LWR for 
\FSP.

\clearpage
Although the same processing time distribution is used, there are some inherent structure in which MWR and LWR can exploit for \JSP\ and  \FSP, respectively, whereas the other SDRs cannot. However, \emph{all} of these dispatching rules are considered good and commonly used in practice and no one is better than the rest \citep{Haupt89}, it simply depends on the data distribution at hand. This indicates that some distributions are harder than others, and these \JSP\ problem generators simply favours MWR, whereas the \FSP\ problem generators favours LWR. 

\begin{figure}[p]
  \subcaptionbox{$6\times5$\label{fig:SDR:boxplot:6x5}}{
    \includegraphics[width=\textwidth]{{boxplotRho.SDR.6x5}.pdf}}
  \caption[Box-plots of \namerho, when applying SDRs]{Box-plots of \namerho, 
    when applying SDRs for all problem spaces in \cref{ch:genprobleminstances}.}
  \label{fig:SDR:boxplot}
\end{figure}
\begin{figure}[h]
  \ContinuedFloat 
  \subcaptionbox{$10\times10$\label{fig:SDR:boxplot:10x10}}{
    \includegraphics[width=\textwidth]{{boxplotRho.SDR.10x10}.pdf}}
  \caption{Continued.}
\end{figure}

\section{Distribution difficulty}
In \cref{InRu12}, a single problem generator was used to create  $N=1,500$ 
synthetic $6\times6$ \jsp\ problem instances, where 
$\vec{p}\sim\mathcal{U}(1,200)$ and $\vsigma$ was a random permutation. The 
experimental study showed that MWR works either well or poorly on a subset of 
the instances, in fact 18\% and 16\% of the instances were classified as 
\emph{easy} and \emph{hard} for MWR, respectively. 
Since the problem instances were na\"{i}vely generated, not to mention given 
the high variance of the data distribution, it is intuitive that there are some 
inherent structural qualities that could explain this difference in 
performance. The experimental study investigated the feature behaviours for 
these two subsets, namely, the easy and hard problem instances. For some 
features, the trend was more or less the same, which are explained by the 
common denominating factor, that all instances were sampled from the same 
problem generator. Whereas, those features that were highly correlated with the 
end-result, i.e., the final makespan, which determined if an instance was 
labelled easy or hard, then the significant features varied greatly between the 
two difficulties, which imply the inherent difference in data structure. 
Moreover, the study in gives support to that random problem instance generators 
are \emph{too} general and might not suit real-world applications. 
\citet{Whitley} argue that problem instance generator should be more 
structured, since real-world manufacturing environment is not completely 
random, but rather structured, e.g.,  job's tasks can be correlated or machines 
in the shop. \citeauthor{Whitley} propose a problem instance generator that 
relates to real-world \fsp\ attributes, albeit not directly modelled after 
real-world \fsp\ due to the fact that deterministic $Fm||C_{\max}$ is seldom 
directly applicable in practice \citep{Dudek92}. This is why \fjc{n}{m}, 
\fmc{n}{m} and \fmxc{n}{m} are also taken into consideration in 
\cref{sec:data:FSP}.

\section{Defining \emph{easy} versus \emph{hard} schedules} 
\label{sec:diff:easyhard}

It's relatively ad-hoc how to define what makes a schedule difficult. 
For instance, it could be sensible to define it in terms of how many Simplex 
iterations are needed to find an optimal schedule, using \emph{Branch and 
Bound}. However, it showed that an increased amount of Simplex iterations 
didn't necessarily transcend to high $\rho$. 
If anything, it means there are many optimal (or near-optimal) solutions 
available, which causes the slow process of eliminating branches of the tree, 
before reaching to a final incumbent solution. If that's the case, than that's 
promising for our instance, as it's likelier for an arbitrary algorithm to find 
a good solution. 
Intuitively, it's logical to use the schedule's objective to define the 
difficulty directly, i.e., inspecting \namerho. Moreover, since the SDRs from 
\cref{eq:SDRset} will be used throughout as a benchmark for subsequent models, 
the quantiles for $\rho$, using the SDRs on their training set will be used 
to differentiate between easy and hard instances. In particular, the 
classification is defined as follows, 
\begin{description}
  \item[Easy] schedules belong to the first quantile, i.e., \hfill \\
  \begin{equation}\label{eq:easy}
    \mathcal{E}(a):=\condset{\vec{x}}{
      \rho=\Upsilon(a,\vec{x})<\rho^{\text{1st. Qu.}}}
  \end{equation} 
  \item[Hard] schedules belong to the third quantile, i.e., \hfill \\
  \begin{equation}\label{eq:hard}
    \mathcal{H}(a):=\condset{\vec{x}}{
      \rho=\Upsilon(a,\vec{x})>\rho^{\text{3rd. Qu.}}}
  \end{equation} 
\end{description}
where $\vec{x}\in\mathcal{P}_{\text{train}}$ for a given $a\in\mathcal{A}$ from 
\cref{eq:SDRset}.

\input{tables/quartiles.EasyHard} 

\Cref{tbl:easyhard:quantile} reports the first and third quantiles for each 
problem space, i.e., the cut-off values that determine the SDRs difficulty, 
whose division, defined as percentage of problem instances, i.e., 
\begin{equation}\label{eq:easyhard:cnt}
  \frac{\abs{\mathcal{E}(a)}}{N_{\text{train}}} \cdot 100\%
  \quad \text{and} \quad 
  \frac{\abs{\mathcal{H}(a)}}{N_{\text{train}}}\cdot 100\%
\end{equation}
for each $a\in\mathcal{A}$, are given in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10}, respectively. 

{\setlength{\tabcolsep}{3pt} 
  \input{tables/quartiles.6x5.EasyHard}
  \input{tables/quartiles.10x10.EasyHard}
}

\section{Consistency of problem instances}
The intersection of pairwise SDRs being simultaneously easy or hard are given in \cref{tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}, i.e., 
\begin{equation}\label{eq:easyorhard:cnt}
  \frac{\abs{\mathcal{E}(a_i)\cap \mathcal{E}(a_j) }}{N_{\text{train}}} \cdot 
  100\%
  \quad \text{or} \quad 
  \frac{\abs{\mathcal{H}(a_i)\cap \mathcal{H}(a_j)}}{N_{\text{train}}}\cdot 
  100\%
\end{equation}
where $a_i,a_j\in\mathcal{A}$. Note, when $a_i=a_j$ then \cref{eq:easyorhard:cnt} is equivalent to \cref{eq:easyhard:cnt}.

{\setlength{\tabcolsep}{3pt} 
  \input{tables/quartiles.6x5.Easy}
  \input{tables/quartiles.6x5.Hard}
  
  \input{tables/quartiles.10x10.Easy}
  \input{tables/quartiles.10x10.Hard}
}

Even though this is a na\"ive way to inspect difference between varying SDRs, it's does give some initial insight of the potential of improving dispatching rules; a sanity check before going into extensive experiments, as will be done in \cref{sec:diff:stepwise}.

For the corresponding $10\times10$ training set (cf. \cref{tbl:easy:cnt:10x10,tbl:hard:cnt:10x10}), the intersections between SDRs from $6\times5$ (cf. \cref{tbl:easy:cnt:6x5,tbl:hard:cnt:6x5}) seem to hold. 
However, by going to a higher dimension, the performance edge between SDRs becomes more apparent, e.g., in \JSP\ when there was a slight possibility of LWR being simultaneously easy as other SDRs ($5\%<$ chance), it becomes almost impossible for $10\times10$. 
Making LWR a clear underdog. 
Despite that, for \FSP\ the tables turn; now LWR has the performance edge. 
For instance, for \frndn{6}{5} the second  best option is to apply LPT 
(13.22\%), however, there is a quite high overlap with LWR (11.74\%), and since 
LWR is easier significantly more often (85.18\%), the choice of SDR is quite 
obvious. 
Although, it goes to show that there is the possibility of improving LWR by sometimes applying LPT-based insight; by seeing what sets apart the intersection of their easy training sets. 

Similarly for every $10\times10$ \JSP\ (cf. \cref{tbl:easy:cnt:10x10}), almost all easy LPT schedules are also easy  for MWR ($<1\%$ difference), as is to be expected as MWR is the more sophisticated counterpart for LPT (like LWR is for SPT). 
However, the greedy approach here is  not gaining any new information on how to improve MWR. 
In fact, MWR is never considered hard for any of the \JSP\ (cf. \cref{tbl:hard:cnt:10x10}), therefore no intersection with any hard schedules. 
But the LPT counterpart has a relatively high occurrence rate (3-14\%), so due to the similarity of the dispatching rules, the denominating factor between LPT and MWR can be an indicator for explaining some of MWR's pitfalls.
That is to say, why aren't all of the \jsp\ schedules easy when applying MWR? 

\clearpage
These have up until now all been speculations about how SDRs differ. One thing is for certain, the underlying problem space plays a great role on a SDR's success. Even slight variations to one job or machine, i.e., \jrndJ{10}{10} and \jrndM{10}{10}, shows significant change in performance. Due to the presence of bottleneck, MWR is able to detect it and thus becomes the clear winner. Even outperforming the original \jrnd{10}{10} which they're based on, despite having processing times doubled for a single job or machine, with approximately 10\% lower first quantile (cf. \cref{tbl:easyhard:quantile:10x10}) in both cases. 

As the objective of this dissertation is not to choose which DR is best to use for each problem instance. 
The focus is set on finding what characterises of \jsp\ overall, are of value (i.e. feature selection), and create a new model that works well for the problem space to a great degree.
Namely, by exploiting feature behaviour that is considered more favourable. The hypothesis being that features evolutions of easy schedules greatly differ from features evolutions corresponding to hard schedules, and \cref{sec:diff:stepwise} will attempt to explain the evidence show in \cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10,tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}.
\todoFind{Make sure \cref{sec:diff:stepwise} reference 
\cref{tbl:easyhard:cnt:6x5,tbl:easyhard:cnt:10x10,tbl:easy:cnt:6x5,tbl:easy:cnt:10x10,tbl:hard:cnt:6x5,tbl:hard:cnt:10x10}.}

Note, this \lcnamecref{sec:diff:easyhard} gave the definition of what 
constitutes an `easy' and `hard' schedule. Since these are based on four SDRs 
(cf. \cref{eq:SDRset}) the training data for the experiments done in this 
\lcnamecref{ch:problemstructure} is based on $4N_{\text{train}}$ problem 
instances, per problem space, therefore,
\begin{equation}\label{eq:easyhard:all}
  \sum_{a\in\mathcal{A}}\abs{\mathcal{E}(a)} \approx N_{\text{train}}
  \quad\text{and}\quad
  \sum_{a\in\mathcal{A}}\abs{\mathcal{H}(a)} \approx N_{\text{train}}
\end{equation} 
due to the fact \cref{eq:easy,eq:hard} are based on the first and third quantiles of the entire training set.
Now, as the SDRs vary greatly in performance, the contribution of a SDR to \cref{eq:easyhard:all} varies, resulting in an unbalanced sample size when restricted to a single SDR. 



\section{Discussion and Conclusion}
Despite problem instances being created by the same problem generator, they vary among one another enough. As a result, all instances are not created equal; some are always hard to solve, others always easy. 
Since the description of the problem space isn't enough to predict its performance, we need a measure to understand what's going on. Why are some instances easier to find their optimum (or close enough)? That is to say, what's their secret? This is where their feature evolution comes into play.
By using schedules obtained by applying SDRs we have the ability to get some insight into the matter. 

