\HeaderQuote{I don't believe there's an atom of meaning in it.}{Alice}

\chapter{Analysing Optimal Solutions}\label{ch:analysingopt} 

It is interesting to know if the difference in the structure of the schedule is 
time dependent, e.g.,  is there a clear time of divergence within the 
scheduling process? 
Moreover, investigation of how sensitive is the difference between two sets of 
features, e.g., can two schedules with similar feature values yield 
\begin{enumerate*}[label={{}}, 
    itemjoin={{? }}, itemjoin*={{? Or }}, after={{? }}]
    \item completely contradictory outcomes (i.e. one poor and one good 
    schedule)
    \item will they more or less follow the their predicted trend
\end{enumerate*}
If the latter is the prevalent case, then instances need to be segregated 
w.r.t. their difficulty, where each has their own learning algorithm 
implemented, for a meaningful outcome overall.  

This also, essentially, answers the question of whether  it is in fact feasible 
to discriminate between \emph{good} and \emph{bad} schedules using the 
currently selected features as a measure of the quality of a solution. 
If results are contradictory, it is an indicator the features selected are not 
robust enough to capture the essence of the data structure and some key 
features are missing from the feature set that could be able to discriminate 
between \emph{good} and \emph{bad} schedules. 
Additionally, there is also the question of how can one define ``similar'' 
schedules, and what measures should be used? This 
\lcnamecref{ch:defdifficulty} describes some preliminary experiments with 
the aim of investigating the feasibility of finding distinguishing features 
corresponding to \emph{good} and \emph{bad} schedules in \jsp. To summarise
\begin{enumerate*}[itemjoin={{? }}, itemjoin*={{? And }}, after={{? }}]
    \item Is there a time of divergence
    \item What are ``similar'' schedules
    \item Do similar features yield contradictory outcomes
    \item Are extra features needed
    \item what can be learned from feature behaviour
\end{enumerate*}


%\section{Probability of choosing optimal decision}\label{sec:diff:opt:rnd}
\FirstSentence{I}{n order to create successful \dr s}, a good starting point is 
to investigate the properties of optimal solutions and hopefully be able to 
learn how to mimic such `good' behaviour. 
For this, we follow an optimal solution,\footnote{Optimal solutions can be 
  obtained by using a commercial software package by \citet{gurobi}, which has 
  a free academic licence. However, GLPK by \citet{glpk} has a free licence. 
  Alas, GLPK has a lacklustre performance w.r.t. speed for solving 
  $10\times10$ \JSP.}
and inspect the evolution of its features  (defined in \cref{tbl:features}) 
throughout the dispatching process, which is detailed in \cref{ch:gentrdat}. 
Moreover, it is noted, that there are several optimal solutions available for 
each problem instance. However, it is deemed sufficient to inspect only one 
optimal trajectory per problem instance as there are $N_{\text{train}}$ 
independent instances which gives the training data variety. 

\Cref{fig:diff:opt:unique,fig:diff:opt:rnd} 
depict the mean over all the training data, which are quite noisy 
functions. \Cref{InRu15b} depicts the mean as is, albeit only for 
$10\times10$ problem spaces. 
Thus, for clarity purposes, they are fitted with local polynomial regression, 
making the boundary points sometimes biased. 
However, 
\cref{fig:diff:extr:SDR,fig:diff:case:OPT,fig:diff:case:SDR,fig:diff:extr:features,fig:diff:opt:evol}
depict the mean as is.

\section{Making optimal decisions}
Firstly, we can observe that on a step-by-step basis there are several optimal 
dispatches to choose from. \Cref{fig:diff:opt:unique} depicts how the number of 
optimal dispatches evolve at each dispatch iteration. Note, that only one 
optimal trajectory is pursued (chosen at random), hence this is only a lower 
bound of uniqueness of optimal solutions.
As the number of possible dispatches decrease over time, 
\cref{fig:diff:opt:rnd} 
depicts the probability of choosing an optimal dispatch at each iteration. 

\begin{figure}
  \centering
  \subcaptionbox{$6\times5$\label{fig:diff:opt:unique:6x5}}{
    \includegraphics[width=\textwidth]{figures/{stepwise.6x5.OPT.unique}.pdf}}
  \\
  \subcaptionbox{$10\times10$\label{fig:diff:opt:unique:10x10}}{
    \includegraphics[width=\textwidth]{figures/{stepwise.10x10.OPT.unique}.pdf}}
  \caption[Number of unique optimal dispatches]{Number of unique optimal 
    dispatches (lower bound).}
  \label{fig:diff:opt:unique}
\end{figure}

\begin{figure}\centering
  \subcaptionbox{$6\times5$\label{fig:diff:opt:rnd:6x5}}{
    \includegraphics[width=\textwidth]{figures/{stepwise.6x5.OPT}.pdf}}
  \\
  \subcaptionbox{$10\times10$\label{fig:diff:opt:rnd:10x10}}{
    \includegraphics[width=\textwidth]{figures/{stepwise.10x10.OPT}.pdf}}
  \caption{Probability of choosing optimal move (at random)}
  \label{fig:diff:opt:rnd}
\end{figure}

\section{Making suboptimal decisions}\label{sec:diff:opt:sub}
Looking at \cref{fig:diff:opt:rnd}, \jrnd{10}{10} has a relatively high 
probability 
($70\%$ and above) of choosing an optimal job. However, it is imperative to 
keep making optimal decisions, because once off the optimal track the 
consequences can be dire. To demonstrate this interaction, 
\cref{fig:diff:case:OPT} 
depicts the worst and best case scenario of \namerho, once you've fallen off 
the optimal track. Note, that this is given that you make \emph{one} wrong 
turn. Generally, there will be many mistakes made, and then the compound 
effects of making suboptimal decisions really start adding up. 
In fact, \cref{fig:diff:extr:SDR} shows the probability of optimality when 
following a fixed SDR.

It is interesting that for \JSP, that over time making suboptimal decisions 
make more of an impact on the resulting makespan. This is most likely due to 
the fact that if a suboptimal decision is made in the early stages, then there 
is space to rectify the situation with the subsequent dispatches. However, if 
done at a later point in time, little is to be done as the damage is already 
been inflicted upon the schedule. 
However, for \FSP, the case is the exact opposite. Under those circumstances 
it's imperative to make good decisions right from the get-go. This is due to 
the major structural differences between \jsp\ and \fsp, namely the latter 
having a homogeneous machine ordering, constricting the solution immensely. 
Luckily, this does have the added benefit of making \fsp\ less vulnerable for 
suboptimal decisions later in the decision process. 

\begin{figure}\centering
  \subcaptionbox{$6\times5$\label{fig:diff:case:OPT:6x5}}{
    \includegraphics[width=\textwidth]{figures/ALL/{stepwise.6x5.OPT.casescenario}.pdf}}
  \\
  \subcaptionbox{$10\times10$\label{fig:diff:case:OPT:10x10}}{
    \includegraphics[width=\textwidth]{figures/ALL/{stepwise.10x10.OPT.casescenario}.pdf}}
  \caption{Mean \namerho, for best and worst case scenario of when making
    \emph{one} sub-optimal dispatch, depicted as lower and upper bound, 
    respectively. Moreover, mean suboptimal move is given as dashed line.}
  \label{fig:diff:case:OPT}
\end{figure}

\section{Optimality of extremal features}\label{sec:diff:opt:ext}
The probability of optimality of the aforementioned features from 
\cref{tbl:features}, yet still maintaining our optimal trajectory, i.e., the 
probability of a job chosen by an extremal value for a feature being able to 
yield an optimal makespan on a step-by-step basis, is depicted  in   
\cref{fig:diff:extr:features}, for both \jrnd{6}{5} and \jrnd{10}{10}.
Moreover, the dashed line represents the benchmark of randomly guessing the 
optimum (cf. \cref{fig:diff:opt:rnd}).
Furthermore, the \lcnamecref{fig:diff:extr:features}s are annotated with the 
corresponding mean \namerho, for the training set if it were scheduled solely 
w.r.t. that extremal feature.

Generally, a high stepwise optimality means a low $\rho$, e.g., 
\phiGlobalRelated, save for \phiRNDstd.\footnote{Note, \phiRNDstd\ is 
non-informative on its own, as a tight standard deviation implies either 
consistently high \emph{or} low $C_{\max}$ from the roll-outs.} 
Unfortunately, it's not always so predictable. Take for instance \phiproc, then 
the minimum value gives a better $\rho$, even though it's unlikelier to be 
optimal than it's maximum counterpart.

\todoFind{Check if \cref{fig:diff:extr:features} are part of $d$ choose 1 
    pref-models feature selection}
Before inspecting the local based features further. Notice that the staggering 
performance edge for \phiRNDmin\ is lost when going to a higher dimension (cf. 
\phiRNDmin\ in \cref{fig:diff:extr:jrnd:6x5} has $\rho=1.3\%$ and increases to 
8.8\% in  \cref{fig:diff:extr:jrnd:10x10}), implying that 100 random roll-outs 
for are not sufficient for fully exploring $10\times10$ state-space, yet highly 
competitive for $6\times5$.

\begin{figure}\centering
    \subcaptionbox{\jrnd{6}{5}\label{fig:diff:extr:jrnd:6x5}}{
        \includegraphics[width=\textwidth]{figures/{j.rnd}/{stepwise.6x5.OPT.extremal}.pdf}}
    \caption{Probability of extremal feature being optimal. }
    \label{fig:diff:extr:features}
\end{figure}
\begin{figure}\centering
    \ContinuedFloat
    \subcaptionbox{\jrnd{10}{10}\label{fig:diff:extr:jrnd:10x10}}{
        \includegraphics[width=\textwidth]{figures/{j.rnd}/{stepwise.10x10.OPT.extremal}.pdf}}
    \caption{(continued)}
\end{figure}

\subsection{Optimality of SDRs}\label{sec:diff:opt:sdr}
Let's limit ourselves to only features that correspond to SDRs from 
\cref{sec:SDR}. Namely, \cref{pref:SDR} yield
\begin{enumerate*}
    \item \phiproc\ for SPT and LPT
    \item \phijobWrm\ for LWR and MWR 
\end{enumerate*}
By choosing the lowest value for the first SDR, and highest value for the 
latter SDR, i.e., the extremal values for those given features. 
\Cref{fig:diff:extr:SDR} depicts the corresponding probabilities from 
\cref{fig:diff:extr:features} in one graph.

Now, let's bare in mind \namerho, of applying SDRs throughout the dispatching 
process (cf. box-plots of which in \cref{fig:SDR:boxplot}), then there is a 
some correspondence between high probability of stepwise optimality and low 
$\rho$. Alas, this isn't always the case, for \jrnd{10}{10} SPT always 
outperforms LPT w.r.t. stepwise optimality, however, this does not transcend to 
SPT having a lower $\rho$ value than LPT. Hence, it's not enough to just learn 
optimal behaviour, one needs to investigate what happens once we encounter 
suboptimal state spaces.

\section{Simple blended dispatching rule}\label{sec:diff:opt:bdr}
As stated before, the goal of this \lcnamecref{ch:defdifficulty} is to 
utilise feature behaviour to motivate new, and \emph{hopefully} better, 
\dr s. 
A na\"ive approach would be creating a simple blended \dr\ (BDR) which 
would be for instance switch between two SDRs at a predetermined time point. 
Hence, going back to \cref{fig:diff:extr:SDR} a presumably good BDR for 
\jrnd{10}{10}  would be starting with SPT and then switching over to MWR at 
around time step $k=40$, where the SDRs change places in outperforming one 
another. 
Note, MWR$\cap$SPT hardly ever coincide for easy or hard schedules (cf. 
\cref{tbl:easy:cnt:10x10,tbl:hard:cnt:10x10}, so its reasonable to believe they 
could complement one another.
A box-plot for \namerho, for all $10\times10$ problem spaces is 
depicted in \cref{fig:diff:boxplot:BDR}. This little manipulation between SDRs 
does outperform SPT immensely, yet doesn't manage to gain the performance edge 
of MWR, save for \frnd{10}{10}. This gives us insight that for \jsp, the 
attribute based on MWR is quite fruitful for good dispatches, whereas the same 
cannot be said about SPT -- a more sophisticated DR is needed to improve upon 
MWR. 

A reason for this lack of performance of our proposed BDR is perhaps that by 
starting out with SPT in the beginning, it sets up the schedules in such a way 
that it's quite greedy and only takes into consideration jobs with shortest 
immediate processing times. Now, even though it is possible to find optimal 
schedules from this scenario, as \cref{fig:diff:extr:SDR} shows, the inherent 
structure is already taking place, and might make it hard to come across 
optimal moves by simple methods. Therefore it's by no means guaranteed that by 
simply swapping over to MWR will handle the situation that applying SPT has 
already created. \Cref{fig:diff:boxplot:BDR} does however show, that by 
applying MWR instead of SPT in the latter stages, does help the schedule to be 
more compact w.r.t. SPT. However, in the case of \jrnd{10}{10} and 
\jrndn{10}{10} the fact remains that the schedules have diverged too far from 
what MWR would have been able to achieve on its own. Preferably the blended 
dispatching rule should use  best of both worlds, and outperform all of its 
inherited DRs, otherwise it goes without saying, one would simply keep on still 
using the original DR that achieved the best results.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/{boxplotRho.BDR.10x10}.pdf}
  \caption{Box plot for \namerho, for BDR where SPT is applied for the first 
  40\% of the dispatches, followed by MWR.}
  \label{fig:diff:boxplot:BDR}
\end{figure}


\section{Feature evolution}\label{sec:diff:opt:evol}
In order to put the extremal features from \cref{fig:diff:extr:features} into 
perspective, it's worth comparing them with how the evolution of the features 
are over time, depicted in \cref{fig:diff:opt:evol}. 
Note that the optimal trajectory describes  how `good' features should aspire 
to be.
We can also notice that the relative ranking in \phiGlobalRelated\ is 
proportional their expected mean \namerho. Although $(K-k)$-step lookahead give 
are consistently the best indicators for finding good solutions. However, they 
are not practical features for high dimensional data due to computational cost.
However, bearing \cref{fig:diff:case:OPT} in mind, then 

\todoExtend{Very short section. Elaborate.}

\begin{figure}\centering
  \subcaptionbox{\jrnd{6}{5}\label{diff:evol:jrnd:6x5}}{
    \includegraphics[width=\textwidth]{figures/{j.rnd}/{stepwise.6x5.evolution}.pdf}}
  \caption[Mean stepwise evolution of $\tilde{\vphi}$]{Mean stepwise evolution 
    of $\tilde{\vphi}$, which is scaled according to \cref{eq:scale}.}
  \label{fig:diff:opt:evol}
\end{figure}
\begin{figure}\centering
  %  \ContinuedFloat
  \subcaptionbox{\jrnd{10}{10}\label{diff:evol:jrnd:10x10}}{
    \includegraphics[width=\textwidth]{figures/{j.rnd}/{stepwise.10x10.evolution}.pdf}}
  \caption{(continued)}
\end{figure}

\section*{Experimental settings}
% Katie orðar: A correlation analysis between the feature space and the 
%performance space was conducted across all 1,500 problem instances revealed 
%that instance features that appear to correlate (linearly) with heuristic 
%performance are $phi(?)$ (correlation $=-0.59$) and $phi(?)$ (correlation 
%$=0.44$). None of the other instance features appear to have a linear 
%relationship with algorithmic performance.

The main focus is on knowing \emph{when} during the scheduling process easy and 
hard problems diverge and explore in further detail \emph{why} they diverged. 
Rather than visualising high-dimensional data projected onto two dimensional 
space (as was the focus in \cite{SmithMilesLion5} with self-organising maps), 
instead appropriate statistical tests with a significance level $\alpha=0.05$ 
is applied to determine if there is any difference between different data 
distributions. 

For this the two-sample Kolmogorov–Smirnov test (K-S test) is used to determine 
whether two underlying one-dimensional probability distributions differ. 
Furthermore, in order to find defining characteristics for easy or hard 
problems, a (linear) correlation is computed between features to the resulting 
\namerho. 

Note, when inspecting any statistical difference between data distribution of 
the features on a step by step basis, the features at step $k+1$ are of course 
dependant on all previous $k$ steps. This results in repetitive statistical 
testing, therefore a Bonferroni adjustment is used to counteract the multiple 
comparisons, i.e., each stepwise comparison has the significant level 
$\alpha_k=\frac{\alpha}{K}$, and thus maintaining the 
$\sum_{k=1}^K\alpha_k=\alpha$ significance level.

\section{Emergence of problem difficulty}\label{sec:diff:stepwise}

From the experimental study it is apparent that features have different %impact 
correlation with the resulting schedule depending in what stage it is in the 
scheduling process, implying that their influence varies over the dispatching 
sequencing. Moreover, features constant throughout the scheduling process are 
not correlated with the end-result.
There are some common features for both difficulties considered which define 
\jsp\ on a whole. However, the significant features are quite different across 
the two difficulties, implying there is a clear difference in their data 
structure. The amount of significant features were considerably more for easy 
problems, indicating their key elements had been found. However, the features 
distinguishing hard problems were scarce. Most likely due to their more complex 
data structure their key features are of a more composite nature. As a result, 
new `global' features were introduced. 

It is possible for a \JSP\ schedule to have more than one sequential 
dispatching representation. It is especially w.r.t. the initial dispatches. 
Revisiting \cref{fig:example:midway}, if we were to dispatch $J_2$ 
first and then $J_4$, then that would be the same equivalent
temporal schedule if we did it the other way around. 
This is because they don't create a conflict for one another 
(as is the case for jobs $J_2$ and $J_3$). This drawback of non-uniqueness of 
sequential dispatching representation explains why there is hardly any 
significant feature for the initial steps of the scheduling process (cf. 
\cref{tbl:JSP:feat:easy} and \cref{tbl:JSP:feat:hard}). 

Since feature selection is of paramount importance in order for algorithms to 
become successful, one needs to give great thought to how features are 
selected. What kind of features yield \emph{bad} schedules? And can they be 
steered onto the path of more promising feature characteristics? This sort of 
investigation can be an indicator how to create meaningful problem generators. 
On the account that real-world problem instances are scarce, their hidden 
properties need be drawn forth in order to generate artificial problem 
instances from the same data distribution. 

The feature attributes need to be based on statistical or theoretical grounds. 
Scrutiny in understanding the nature of problem instances therefore becomes of 
paramount importance in feature engineering for learning, as it yields feedback 
into what features are important to devote more attention to, i.e., features 
that result in a failing algorithm. 
For instance, in \cref{tbl:JSP:feat:same} the slack features have the same 
distribution in the initial stages of the scheduling process. However, there is 
a clear point of divergence which needs to be investigate why the sudden 
change? \todoFind{Check best/worst case scenario}
In general, this sort of analysis can undoubtedly be used in better 
algorithm design which is more equipped to deal with varying problem instances 
and tailor to individual problem instance's needs, i.e., a footprint-oriented 
algorithm.

Although this methodology was only implemented on a set of simple 
single-priority \dr s, the methodology is easily adaptable for more 
complex algorithms, such as the learned preference models in 
\cref{sec:pref:defdifficulty}. The main objective of this work is to 
illustrate the interaction of a specific algorithm on a given problem structure 
and its properties. 