\begin{algorithm}[t]
    \caption{DAgger: Dataset Aggregation for \JSP}
    \label{pseudo:DAgger}
    \begin{algorithmic}[1]
        \Require $T\geq1$
        \Procedure{DAgger}{$\pi_\star,\Phi^\OPT,T$}
        \State $\Phi^{\IL{0}} \gets \Phi^{\OPT}$ \Comment{initialize 
        dataset}
        \State $\hat{\pi}_0 \gets$ \Call{Train}{$\Phi^{\IL{0}}$}
        \Comment{initial model, equivalent to \cref{sec:expertPolicy}}
        \For{i}{1}{T} \Comment{at each imitation learning iteration}
        \State Let $\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}$
        \Comment{\cref{eq:il}}
        \State Sample a $K$-solution using $\pi_i$
        \Comment{cf. \cref{pseudo:imitationLearning}: 
        \Call{ImitationLearning}{$i,\hat{\pi}_{i-1},\pi_\star$}}
        \State $\Phi^{\IL{i}} = \{(s,\pi_\star(s))\}$ 
        \Comment{visited states by $\pi_i$ and actions given by expert}
        \State $\Phi^{\DA{i}} \gets \Phi^{\DA{i-1}} \cup 
        \Phi^{\IL{i}}$ 
        \Comment{aggregate datasets, cf. \cref{eq:DAgger}}
        \State $\hat{\pi}_{i+1} \gets$ \Call{Train}{$\Phi^{\DA{i}}$}
        \Comment{preference model from \cref{eq:jssp:linweights}}
        \EndFor
        \State \Return best $\hat{\pi}_i$ on validation \Comment{best 
        preference model}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}