This dissertation is based on the following publications, listed in 
chronological order:
{\raggedright
\begin{description} 
    \item[Paper \labelcref{InRu11a}] \nameref{InRu11a}
    \item[Paper \labelcref{InRu11b}] \nameref{InRu11b}
    \item[Paper \labelcref{InRu12} ] \nameref{InRu12}
    \item[Paper \labelcref{InRu14} ] \nameref{InRu14}
    \item[Paper \labelcref{InRu15a}] \nameref{InRu15a}
    \item[Paper \labelcref{InRu15b}] \nameref{InRu15b}
\end{description}}

\noindent These publications will be referenced throughout using their Roman 
numeral. 
The thesis is divided into two parts
\begin{enumerate*}[label={{}}]
    \item \emph{\nameref{Prologue}}
    \item \emph{\nameref{Papers}}
\end{enumerate*}
\makeatletter 
\ifthenelse{\equal{\@monograph}{true}}{
    \nameref{Prologue} gives a coherent connection for the publications, and 
    elaborates on chosen aspects,
    whereas, \nameref{Papers} cites the publications that are referenced.    
}{
    \nameref{Prologue} gives a coherent connection for the publications, and 
    elaborates on chosen aspects, written as a monograph, 
    whereas, \nameref{Papers} contains copies of the publications, reprinted 
    with permission from the publishers.}
\makeatother

\clearpage
\subsection*{Mapping between \cref{Prologue} and \cref{Papers}}
The prologue will address the \jsp\ scheduling problem, detailed in 
\cref{ch:scheduling} and correspond to the application in 
\cref{InRu11a,InRu12,InRu14,InRu15a,InRu15b}. The problem generators 
used are subsequently described in \cref{ch:genprobleminstances}.
From there, we try do define problem difficulty in \cref{ch:defdifficulty}, 
improving upon the ad hoc definition from \cref{InRu12}. 
There will be two algorithms considered
\begin{enumerate*}
    \item preference learning in \cref{ch:prefmodels}, which is a tailored 
    algorithm
    \item evolutionary search in \cref{ch:esmodels}, which is a general 
    algorithm
\end{enumerate*}

The latter was implemented in \cref{InRu14}, which could be improved by 
incorporating the methodology from \cref{InRu11b}.
Preference models on  the other hand, are highly dependent on training data, 
whose collection is addressed in \cref{ch:gentrdat,InRu15a} using passive 
imitation learning, whereas \cref{ch:imitation,InRu15b} included active 
imitation learning with greatly improved results. 
Moreover, the training data contains an abundance of information that can be 
used to determine the algorithm's footprint in instance space, which was done 
for optimal solutions in \cref{InRu15b}, and in addition to that SDR based 
trajectories were inspected in \cref{ch:analysingsol} along with tying together 
the preliminary work in \cref{InRu12}. 
Furthermore, \cref{ch:orlibrobust} compares two methodologies, as the 
preference models had been significantly improved since \cref{InRu14}.
An overview of experimental settings in \cref{Papers} is given in 
\cref{papers:summary}. 
\Cref{ch:conclusions} concludes the dissertation with discussion and addresses 
future work.
    
\input{tables/papers-summary.tex}
