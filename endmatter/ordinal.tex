\HeaderQuote{What is the use of repeating all that stuff, if you don't explain it as you go on? It's by far the most confusing thing I ever heard!}{The Mock Turtle} 

\chapter{Ordinal Regression}\label{ch:ordinal} 
\FirstSentence{O}{rdinal regression has been} previously presented in 
\cite{Ru06:PPSN}, but given here for completeness. The preference learning task 
of linear classification presented there is based on the work proposed in 
\citep{main:LIBLINEAR,Lin08:newtontrustregion}. 
The modification relates to how the point pairs are selected and the fact that 
a $L2$-regularized logistic regression is used. 

\section{Preference set}
The ranking problem is specified by a \emph{preference set}, 
\begin{equation}
\Psi := \{(\vec{x}_i,y_i)\}_{i=1}^N \subset X \times Y
\end{equation}
consisting of $N$ (solution, rank)-pairs, where $Y=\{r_1,\ldots,r_N\}$ 
is the outcome space with ordered ranks $r_1> r_2,> \ldots > r_N$.  

Now consider the model space $\mathcal{H} = \{h(\cdot) : X \mapsto Y\}$ of 
mappings from solutions to ranks. Each such function $h$ induces an ordering 
$\succ$ on the solutions  by the following rule,
\begin{equation}\label{eq:linear}
	\vec{x}_i \succ \vec{x}_j \quad \Leftrightarrow \quad h(\vec{x}_i) > h(\vec{x}_j)
\end{equation}
where the symbol $\succ$ denotes `is preferred to.' 

In ordinal regression the task is to obtain function $h$ that can for a given pair $(\vec{x}_i,y_i)$ and $(\vec{x}_j,y_j)$ distinguish between two different outcomes: $y_i > y_j$ and $y_j > y_i$. The task is, therefore, transformed into the problem of predicting the relative ordering of all possible pairs of examples \citep{Herbrich00,Joachims02}.  However, it is sufficient to consider only all possible pairs of adjacent ranks (see also \cite{ShaweTaylor04:book} for yet an alternative formulation).  The preference set, composed of pairs, is then as follows,
\begin{equation}\label{eq:PrefSet:problem}
	\Psi = \left\{(\vec{x}_k^{(1)}, \vec{x}_k^{(2)}),t_k=\text{sign}(y_k^{(1)} 
	- y_k^{(2)})\right\}_{k=1}^{N'} \subset X\times Y  
\end{equation}
where $(y_k^{(1)} = r_i) \wedge (y_k^{(2)} = r_{i+1})$, and vice versa 
$(y_k^{(1)} = r_{i+1}) \wedge (y_k^{(2)} = r_{i})$, resulting in $N'=2(N-1)$ 
possible adjacently ranked preference pairs. The rank difference is denoted by 
$t_k\in\{-1,1\}$.

In order to generalize the technique to different solution data types and model 
spaces an implicit kernel-defined feature space $\Phi\subset\mathbb{R}^d$ of 
dimension $d$, with corresponding feature mapping $\vphi:X\mapsto\Phi$ is 
applied, i.e., the feature vector 
$\vphi(\vec{x})=[\phi_1(\vec{x}),\ldots,\phi_d(\vec{x})]^T\in\Phi$. Thus the 
preference set defined by \cref{eq:PrefSet:problem} is redefined as follows,
\begin{equation}\label{eq:PrefSet:feat}
	\Psi = \left\{\left(\vphi(\vec{x}_k^{(1)}), 
	\vphi(\vec{x}_k^{(2)})\right),t_k=\text{sign}(y_k^{(1)} - 
	y_k^{(2)})\right\}_{k=1}^{N'} \subset \Phi \times Y.
\end{equation}


\section{Ordinal Regression}\label{sec:ord:linpref}
The function used to induce the preference is defined by a linear function in the kernel-defined feature space,
\begin{equation}\label{eq:pref:kernel} 
  h(\vec{x}) = \sum_{i=1}^d w_i\phi_i(\vec{x})=\inner{\vec{w}}{\vphi(\vec{x})} 
\end{equation}
where $\vec{w}=[w_1,\ldots,w_d]\in\mathbb{R}^d$ has weight $w_i$ for feature $\phi_i$.

The aim now is to find a function $h$ that encounters as few training errors as 
possible on $\Psi$. Applying the method of large margin rank boundaries of 
ordinal regression described in \cite{Herbrich00}, the optimal $\vec{w}^*$ is 
determined by solving the following task, 
\begin{equation}\label{eq:margin:linear}
	\min_{\vec{w}}\quad \tfrac{1}{2}\inner{\vec{w}}{\vec{w}} + \tfrac{C}{2}\sum_{k=1}^{N'}\xi_k^2
\end{equation}
subject to $t_k\inner{\vec{w}}{(\vphi(\vec{x}_k^{(1)})-\vphi(\vec{x}_k^{(2)})}\ge 1 - \xi_k$ and $\xi_k \ge 0$, $k = 1,\ldots, N'$. The degree of constraint violation is given by the margin slack variable $\xi_k$ and when greater than $1$ the corresponding pair are incorrectly ranked. 
Note that,
\begin{equation}\label{eq:hfun}
	h(\vec{x}_i)-h(\vec{x}_j)=\inner{\vec{w}}{\vphi(\vec{x}_i)-\vphi(\vec{x}_j)}
\end{equation}
and minimising $\inner{\vec{w}}{\vec{w}}$ in \cref{eq:margin:linear} maximises 
the margin between rank boundaries, i.e., the distance between adjacently 
ranked pair $h(\vec{x}^{(1)})$ and $h(\vec{x}^{(2)})$.

\section{Logistic Regression}
Let $\vec{z}$ denote either $\vphi(\vec{x}_k^{(1)})-\vphi(\vec{x}_k^{(2)})$ 
with \mbox{$t_k=+1$} or 
$\vphi(\vec{x}_k^{(2)})-\vphi(\vec{x}_k^{(1)})$ with \mbox{$t_k=-1$}, positive 
or negative example respectively.

Logistic regression learns the optimal parameters $\vec{w}\in\mathbb{R}^d$ 
determined by solving the following task,
\begin{equation}\label{eq:margin:logit}
\min_{\vec{w}}\quad \tfrac{1}{2}\inner{\vec{w}}{\vec{w}} + C \sum_{i=1}^{N'} 
\log\left(1 + e^{-y_i \inner{\vec{w}}{\vec{z}_i}}\right) 
\end{equation}
where $C > 0$ is a penalty parameter, and the negative log-likelihood is due to 
the fact the given data point $\vec{z}_i$ and weights $\vec{w}$ are assumed to 
follow the probability model,
\begin{equation}\label{eq:prob}
\mathcal{P}\big(y=\pm1|\vec{z},\vec{w}\big)=\frac{1}{1+e^{-y\inner{\vec{w}}{\vec{z}_i}}}.
\end{equation}
The logistic regression defined in \cref{eq:margin:logit} is solved 
iteratively, in particular using Trust Region Newton method 
\citep[cf.][]{Lin08:newtontrustregion}, which generates a sequence 
$\{\vec{w}^{(k)}\}_{k=1}^\infty$ converging to the optimal solution $\vec{w}^*$ 
of \cref{eq:margin:logit}.


\section{Non-Linear Preference}\label{sec:ord:nonlinpref}
In the case that the preference set $\Psi$ defined by \cref{eq:PrefSet:feat} is 
not linearly separable, a common way of coping with non-linearity is to apply 
the `kernel-trick' to transform $\Psi$ onto a higher dimension. In which case, 
the dot product in \cref{eq:pref:kernel} is replaced by a kernel function 
$\kappa$.

In terms of training data, the optimal $\vec{w}^*$ can be expressed as, 
\begin{equation}
	\vec{w}^*=\sum_{k=1}^{N'} \alpha^* t_k \left( \vphi(\vec{x}_k^{(1)})-\vphi(\vec{x}_k^{(2)}) \right)
\end{equation}
and the function $h(\cdot)$ from \cref{eq:hfun} may be reconstructed as follows,
\begin{eqnarray}\label{eq:nonlinear}
	h(\vec{x})=\inner{\vec{w}^*}{\vphi(\vec{x})} &=& 
	\sum_{k=1}^{N'} \alpha^* t_k \left( \inner{\vphi(\vec{x}_k^{(1)})}{\vphi(\vec{x})}-\inner{\vphi(\vec{x}_k^{(2)})}{\vphi(\vec{x})} ) \right) \nonumber \\ 
	&=&\sum_{k=1}^{N'} \alpha^* t_k \left( \kappa(\vec{x}_k^{(1)},\vec{x})-\kappa(\vec{x}_k^{(2)},\vec{x}) \right)
\end{eqnarray}
where $\kappa(\vec{x},\vec{z})=\inner{\vphi(\vec{x})}{\vphi(\vec{z})}$ is the chosen kernel and $\alpha_k^*$ are the Lagrangian multi\-pliers for the constraints that can be determined by solving the dual quadratic programming problem,
\begin{equation}\label{eq:margin:nonlinear}
	\max_{\alpha} \sum_{k=1}^{N'} \alpha_k -\frac{1}{2} 
	\sum_{i=1}^{N'}\sum_{j=1}^{N'} \alpha_i\alpha_jt_it_j\left(
  K\left(\vec{x}_i^{(1)},\vec{x}_i^{(2)},\vec{x}_j^{(1)},\vec{x}_j^{(2)}\right) 
  +\frac{1}{C}\delta_{ij}\right)
\end{equation} 
subject to $\sum_{k=1}^{N'} \alpha_kt_k=0$ and $\alpha_k\geq0$ for all $k\in\{1,\ldots,N'\}$, and where,
\begin{eqnarray}
K\left(\vec{x}_i^{(1)},\vec{x}_i^{(2)},\vec{x}_j^{(1)},\vec{x}_j^{(2)}\right)  
&=& 
  \kappa\left(\vec{x}_i^{(1)},\vec{x}_j^{(1)}\right)
  -\kappa\left(\vec{x}_i^{(1)},\vec{x}_j^{(2)}\right) \\ && \nonumber
  -\kappa\left(\vec{x}_i^{(2)},\vec{x}_j^{(1)}\right)
  +\kappa\left(\vec{x}_i^{(2)},\vec{x}_j^{(2)}\right)
\end{eqnarray}
and $\delta_{ij}$ is the Kronecker $\delta$ defined to be 1 iff $i=j$ and 0 otherwise.

\subsection*{Kernel functions}
There are several choices for a kernel $\kappa$, e.g., \emph{polynomial kernel},
\begin{eqnarray}
	\kappa_{\text{poly}}(\vec{x}_i,\vec{x}_j)&=& \left(1+\inner{\vec{x}_i}{\vec{x}_j} \right)^p
\end{eqnarray}
of order $p$, or the most commonly used kernel in the literature which implements a Gauss\-ian radial basis function, the \emph{rbf kernel},
\begin{eqnarray}
	\kappa_{\text{rbf}}(\vec{x}_i,\vec{x}_j)&=& e^{-\gamma \norm{\vec{x}_i-\vec{x}_j}^2}
\end{eqnarray}
for $\gamma>0$.

\section{Parameter setting and tuning}
The regulation parameter $C$ in 
\cref{eq:margin:linear,eq:margin:logit,eq:margin:nonlinear}, 
controls the balance between model complexity and training errors, and must be 
chosen appropriately. A high value for $C$ gives greater emphasis on correctly 
distinguishing between different ranks, whereas a low $C$ value results in 
maximising the margin between classes.

\section{Scaling}\label{app:feat:scaling}
It some cases it becomes necessary to scale the features $\vphi$ from 
\cref{sec:features} first, especially if implementing a kernel method in 
\cref{eq:pref:kernel}.
In the case of \JSP, scaling makes the features less sensitive to varying 
problem instances.
Moreover, for surrogate modelling (cf. \cref{InRu11b}), it is important to 
scale the features $\vphi$ as the evolutionary search zooms in on a particular 
region of the search space. 

A standard method of doing so is by scaling the preference set such that all 
points are in some range, typically $[-1,1]$. That is, scaled $\tilde{\vphi}$ 
is,
\begin{equation}\label{eq:scale}
\tilde{\phi}_i = 2 (\phi_i - \underline{\phi}_i) / (\overline{\phi}_i - 
\underline{\phi}_i) - 1 
\quad\quad \forall i\in\{1,\ldots,d\}
\end{equation}
where $\underline{\phi}_i$, $\overline{\phi}_i$ are the maximum and minimum 
$i$-th component of all the feature variables in $\Phi$, namely,
\begin{equation}
\underline{\phi}_i=\min\{\phi_i\;|\;\forall\vphi\in \Phi\} 
\quad\textrm{and}\quad \overline{\phi}_i=\max\{\phi_i\;|\;\forall\vphi\in 
\Phi\}
\end{equation}
where $i\in\{1\ldots d\}$. 

\section{Implementation}
To use linear ordinal regression, then it's best to use LIBLINEAR: A 
Library for Large Linear Classification by \citet{main:LIBLINEAR}, which 
contains implementations in several programming languages. The preferred choice 
of the author was the R-package 
\href{https://cran.r-project.org/web/packages/LiblineaR/}{\texttt{LiblineaR}} 
by \citet{cran:LiblineaR}.
However, if more sophisticated kernel methods are sought after, then {LIBSVM}: 
A Library for Support Vector Machines by 
\citet{main:LIBSVM} is an obvious substitute.

